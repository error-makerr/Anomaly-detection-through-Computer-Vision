{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7ffdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n",
      "NumPy version: 1.26.4\n",
      "Num GPUs Available: 1\n",
      "GPU Devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "All available devices:\n",
      "  /device:CPU:0: CPU\n",
      "  /device:GPU:0: GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Devices:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Additional GPU check\n",
    "from tensorflow.python.client import device_lib\n",
    "print(\"\\nAll available devices:\")\n",
    "for device in device_lib.list_local_devices():\n",
    "    print(f\"  {device.name}: {device.device_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d09a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SIMPLERNN DEFECT DETECTION PIPELINE\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "DATA PREPARATION\n",
      "============================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images across 63 classes\n",
      "‚úì Classes: ['broken_large', 'broken_small', 'contamination', 'good', 'bent_wire', 'cable_swap', 'combined', 'cut_inner_insulation', 'cut_outer_insulation', 'missing_cable']...\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1962\n",
      "‚úì Test samples: 981\n",
      "\n",
      "============================================================\n",
      "BUILDING SIMPLERNN MODEL\n",
      "============================================================\n",
      "\n",
      "‚úì Model architecture:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 16, 128)           409728    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 128)           0         \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 64)                12352     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               16640     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 63)                8127      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 479,743\n",
      "Trainable params: 479,743\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "============================================================\n",
      "TRAINING SIMPLERNN MODEL\n",
      "============================================================\n",
      "\n",
      "üöÄ Starting training for 50 epochs...\n",
      "‚è±Ô∏è  Start time: 2025-11-13 22:04:13\n",
      "\n",
      "Epoch 1/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 2.5465 - accuracy: 0.4006 - top_5_accuracy: 0.6536\n",
      "Epoch 1: val_accuracy improved from -inf to 0.43852, saving model to rnn_output\\models\\best_model.h5\n",
      "214/214 [==============================] - 270s 1s/step - loss: 2.5465 - accuracy: 0.4006 - top_5_accuracy: 0.6536 - val_loss: 2.1003 - val_accuracy: 0.4385 - val_top_5_accuracy: 0.7275 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 2.1732 - accuracy: 0.4303 - top_5_accuracy: 0.7237\n",
      "Epoch 2: val_accuracy did not improve from 0.43852\n",
      "214/214 [==============================] - 200s 937ms/step - loss: 2.1732 - accuracy: 0.4303 - top_5_accuracy: 0.7237 - val_loss: 2.6849 - val_accuracy: 0.3735 - val_top_5_accuracy: 0.6117 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 2.0996 - accuracy: 0.4325 - top_5_accuracy: 0.7494\n",
      "Epoch 3: val_accuracy improved from 0.43852 to 0.44621, saving model to rnn_output\\models\\best_model.h5\n",
      "214/214 [==============================] - 196s 921ms/step - loss: 2.0996 - accuracy: 0.4325 - top_5_accuracy: 0.7494 - val_loss: 1.9337 - val_accuracy: 0.4462 - val_top_5_accuracy: 0.7787 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 2.0286 - accuracy: 0.4404 - top_5_accuracy: 0.7602\n",
      "Epoch 4: val_accuracy improved from 0.44621 to 0.45748, saving model to rnn_output\\models\\best_model.h5\n",
      "214/214 [==============================] - 196s 921ms/step - loss: 2.0286 - accuracy: 0.4404 - top_5_accuracy: 0.7602 - val_loss: 1.8655 - val_accuracy: 0.4575 - val_top_5_accuracy: 0.7715 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.9948 - accuracy: 0.4451 - top_5_accuracy: 0.7639\n",
      "Epoch 5: val_accuracy did not improve from 0.45748\n",
      "214/214 [==============================] - 187s 876ms/step - loss: 1.9948 - accuracy: 0.4451 - top_5_accuracy: 0.7639 - val_loss: 1.8922 - val_accuracy: 0.4488 - val_top_5_accuracy: 0.7818 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.9637 - accuracy: 0.4395 - top_5_accuracy: 0.7672\n",
      "Epoch 6: val_accuracy did not improve from 0.45748\n",
      "214/214 [==============================] - 182s 853ms/step - loss: 1.9637 - accuracy: 0.4395 - top_5_accuracy: 0.7672 - val_loss: 1.8407 - val_accuracy: 0.4447 - val_top_5_accuracy: 0.7833 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.9108 - accuracy: 0.4486 - top_5_accuracy: 0.7734\n",
      "Epoch 7: val_accuracy improved from 0.45748 to 0.48156, saving model to rnn_output\\models\\best_model.h5\n",
      "214/214 [==============================] - 142s 667ms/step - loss: 1.9108 - accuracy: 0.4486 - top_5_accuracy: 0.7734 - val_loss: 1.7427 - val_accuracy: 0.4816 - val_top_5_accuracy: 0.7915 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.9322 - accuracy: 0.4533 - top_5_accuracy: 0.7672\n",
      "Epoch 8: val_accuracy did not improve from 0.48156\n",
      "214/214 [==============================] - 119s 558ms/step - loss: 1.9322 - accuracy: 0.4533 - top_5_accuracy: 0.7672 - val_loss: 1.7861 - val_accuracy: 0.4580 - val_top_5_accuracy: 0.7997 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.8842 - accuracy: 0.4596 - top_5_accuracy: 0.7739\n",
      "Epoch 9: val_accuracy did not improve from 0.48156\n",
      "214/214 [==============================] - 116s 546ms/step - loss: 1.8842 - accuracy: 0.4596 - top_5_accuracy: 0.7739 - val_loss: 1.8908 - val_accuracy: 0.4406 - val_top_5_accuracy: 0.7894 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.8859 - accuracy: 0.4559 - top_5_accuracy: 0.7802\n",
      "Epoch 10: val_accuracy did not improve from 0.48156\n",
      "214/214 [==============================] - 116s 542ms/step - loss: 1.8859 - accuracy: 0.4559 - top_5_accuracy: 0.7802 - val_loss: 1.7328 - val_accuracy: 0.4693 - val_top_5_accuracy: 0.7905 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.8068 - accuracy: 0.4797 - top_5_accuracy: 0.7859\n",
      "Epoch 11: val_accuracy did not improve from 0.48156\n",
      "214/214 [==============================] - 119s 559ms/step - loss: 1.8068 - accuracy: 0.4797 - top_5_accuracy: 0.7859 - val_loss: 1.6529 - val_accuracy: 0.4600 - val_top_5_accuracy: 0.8105 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.8362 - accuracy: 0.4546 - top_5_accuracy: 0.7782\n",
      "Epoch 12: val_accuracy improved from 0.48156 to 0.50717, saving model to rnn_output\\models\\best_model.h5\n",
      "214/214 [==============================] - 112s 524ms/step - loss: 1.8362 - accuracy: 0.4546 - top_5_accuracy: 0.7782 - val_loss: 1.6438 - val_accuracy: 0.5072 - val_top_5_accuracy: 0.8099 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.8346 - accuracy: 0.4612 - top_5_accuracy: 0.7811\n",
      "Epoch 13: val_accuracy did not improve from 0.50717\n",
      "214/214 [==============================] - 112s 525ms/step - loss: 1.8346 - accuracy: 0.4612 - top_5_accuracy: 0.7811 - val_loss: 1.6611 - val_accuracy: 0.4800 - val_top_5_accuracy: 0.8074 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.7864 - accuracy: 0.4717 - top_5_accuracy: 0.7878\n",
      "Epoch 14: val_accuracy did not improve from 0.50717\n",
      "214/214 [==============================] - 113s 529ms/step - loss: 1.7864 - accuracy: 0.4717 - top_5_accuracy: 0.7878 - val_loss: 1.6238 - val_accuracy: 0.4969 - val_top_5_accuracy: 0.8202 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.8373 - accuracy: 0.4698 - top_5_accuracy: 0.7770\n",
      "Epoch 15: val_accuracy did not improve from 0.50717\n",
      "214/214 [==============================] - 111s 519ms/step - loss: 1.8373 - accuracy: 0.4698 - top_5_accuracy: 0.7770 - val_loss: 1.6376 - val_accuracy: 0.5046 - val_top_5_accuracy: 0.8079 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.8391 - accuracy: 0.4550 - top_5_accuracy: 0.7814\n",
      "Epoch 16: val_accuracy did not improve from 0.50717\n",
      "214/214 [==============================] - 111s 521ms/step - loss: 1.8391 - accuracy: 0.4550 - top_5_accuracy: 0.7814 - val_loss: 1.6892 - val_accuracy: 0.4908 - val_top_5_accuracy: 0.8048 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.7826 - accuracy: 0.4664 - top_5_accuracy: 0.7915\n",
      "Epoch 17: val_accuracy did not improve from 0.50717\n",
      "214/214 [==============================] - 110s 518ms/step - loss: 1.7826 - accuracy: 0.4664 - top_5_accuracy: 0.7915 - val_loss: 1.7781 - val_accuracy: 0.4559 - val_top_5_accuracy: 0.7951 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.7846 - accuracy: 0.4582 - top_5_accuracy: 0.7932\n",
      "Epoch 18: val_accuracy did not improve from 0.50717\n",
      "214/214 [==============================] - 111s 521ms/step - loss: 1.7846 - accuracy: 0.4582 - top_5_accuracy: 0.7932 - val_loss: 1.6233 - val_accuracy: 0.4985 - val_top_5_accuracy: 0.8058 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.7743 - accuracy: 0.4750 - top_5_accuracy: 0.7925\n",
      "Epoch 19: val_accuracy did not improve from 0.50717\n",
      "214/214 [==============================] - 116s 545ms/step - loss: 1.7743 - accuracy: 0.4750 - top_5_accuracy: 0.7925 - val_loss: 1.6048 - val_accuracy: 0.5020 - val_top_5_accuracy: 0.8233 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.7513 - accuracy: 0.4775 - top_5_accuracy: 0.7941\n",
      "Epoch 20: val_accuracy improved from 0.50717 to 0.51383, saving model to rnn_output\\models\\best_model.h5\n",
      "214/214 [==============================] - 113s 531ms/step - loss: 1.7513 - accuracy: 0.4775 - top_5_accuracy: 0.7941 - val_loss: 1.5834 - val_accuracy: 0.5138 - val_top_5_accuracy: 0.8161 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.7189 - accuracy: 0.4869 - top_5_accuracy: 0.8023\n",
      "Epoch 21: val_accuracy did not improve from 0.51383\n",
      "214/214 [==============================] - 114s 533ms/step - loss: 1.7189 - accuracy: 0.4869 - top_5_accuracy: 0.8023 - val_loss: 1.6267 - val_accuracy: 0.4913 - val_top_5_accuracy: 0.8110 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.6996 - accuracy: 0.4874 - top_5_accuracy: 0.8042\n",
      "Epoch 22: val_accuracy did not improve from 0.51383\n",
      "214/214 [==============================] - 116s 543ms/step - loss: 1.6996 - accuracy: 0.4874 - top_5_accuracy: 0.8042 - val_loss: 1.5743 - val_accuracy: 0.5133 - val_top_5_accuracy: 0.8161 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.7242 - accuracy: 0.4796 - top_5_accuracy: 0.7986\n",
      "Epoch 23: val_accuracy did not improve from 0.51383\n",
      "214/214 [==============================] - 116s 544ms/step - loss: 1.7242 - accuracy: 0.4796 - top_5_accuracy: 0.7986 - val_loss: 1.6717 - val_accuracy: 0.4985 - val_top_5_accuracy: 0.8038 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.7219 - accuracy: 0.4761 - top_5_accuracy: 0.8053\n",
      "Epoch 24: val_accuracy did not improve from 0.51383\n",
      "214/214 [==============================] - 116s 544ms/step - loss: 1.7219 - accuracy: 0.4761 - top_5_accuracy: 0.8053 - val_loss: 1.5471 - val_accuracy: 0.5123 - val_top_5_accuracy: 0.8227 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.6951 - accuracy: 0.4842 - top_5_accuracy: 0.8051\n",
      "Epoch 25: val_accuracy did not improve from 0.51383\n",
      "214/214 [==============================] - 115s 540ms/step - loss: 1.6951 - accuracy: 0.4842 - top_5_accuracy: 0.8051 - val_loss: 1.5618 - val_accuracy: 0.5000 - val_top_5_accuracy: 0.8166 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.6704 - accuracy: 0.4863 - top_5_accuracy: 0.8091\n",
      "Epoch 26: val_accuracy did not improve from 0.51383\n",
      "214/214 [==============================] - 115s 541ms/step - loss: 1.6704 - accuracy: 0.4863 - top_5_accuracy: 0.8091 - val_loss: 1.5467 - val_accuracy: 0.4698 - val_top_5_accuracy: 0.8335 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.6983 - accuracy: 0.4800 - top_5_accuracy: 0.8043\n",
      "Epoch 27: val_accuracy did not improve from 0.51383\n",
      "214/214 [==============================] - 114s 534ms/step - loss: 1.6983 - accuracy: 0.4800 - top_5_accuracy: 0.8043 - val_loss: 1.6215 - val_accuracy: 0.4682 - val_top_5_accuracy: 0.8099 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.7055 - accuracy: 0.4895 - top_5_accuracy: 0.8023\n",
      "Epoch 28: val_accuracy did not improve from 0.51383\n",
      "214/214 [==============================] - 128s 600ms/step - loss: 1.7055 - accuracy: 0.4895 - top_5_accuracy: 0.8023 - val_loss: 1.5842 - val_accuracy: 0.5036 - val_top_5_accuracy: 0.8335 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.6955 - accuracy: 0.4793 - top_5_accuracy: 0.8112\n",
      "Epoch 29: val_accuracy did not improve from 0.51383\n",
      "214/214 [==============================] - 121s 567ms/step - loss: 1.6955 - accuracy: 0.4793 - top_5_accuracy: 0.8112 - val_loss: 1.5575 - val_accuracy: 0.5041 - val_top_5_accuracy: 0.8330 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.6606 - accuracy: 0.4928 - top_5_accuracy: 0.8126\n",
      "Epoch 30: val_accuracy did not improve from 0.51383\n",
      "214/214 [==============================] - 118s 551ms/step - loss: 1.6606 - accuracy: 0.4928 - top_5_accuracy: 0.8126 - val_loss: 1.5923 - val_accuracy: 0.5138 - val_top_5_accuracy: 0.8274 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.6761 - accuracy: 0.4895 - top_5_accuracy: 0.8096\n",
      "Epoch 31: val_accuracy improved from 0.51383 to 0.51895, saving model to rnn_output\\models\\best_model.h5\n",
      "214/214 [==============================] - 116s 542ms/step - loss: 1.6761 - accuracy: 0.4895 - top_5_accuracy: 0.8096 - val_loss: 1.5117 - val_accuracy: 0.5190 - val_top_5_accuracy: 0.8366 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.6568 - accuracy: 0.4918 - top_5_accuracy: 0.8140\n",
      "Epoch 32: val_accuracy did not improve from 0.51895\n",
      "214/214 [==============================] - 115s 538ms/step - loss: 1.6568 - accuracy: 0.4918 - top_5_accuracy: 0.8140 - val_loss: 1.6504 - val_accuracy: 0.4826 - val_top_5_accuracy: 0.8125 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.6598 - accuracy: 0.4890 - top_5_accuracy: 0.8137\n",
      "Epoch 33: val_accuracy did not improve from 0.51895\n",
      "214/214 [==============================] - 115s 540ms/step - loss: 1.6598 - accuracy: 0.4890 - top_5_accuracy: 0.8137 - val_loss: 1.5288 - val_accuracy: 0.5097 - val_top_5_accuracy: 0.8304 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.6422 - accuracy: 0.4940 - top_5_accuracy: 0.8124\n",
      "Epoch 34: val_accuracy did not improve from 0.51895\n",
      "214/214 [==============================] - 115s 538ms/step - loss: 1.6422 - accuracy: 0.4940 - top_5_accuracy: 0.8124 - val_loss: 1.5509 - val_accuracy: 0.5123 - val_top_5_accuracy: 0.8386 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.6360 - accuracy: 0.4877 - top_5_accuracy: 0.8097\n",
      "Epoch 35: val_accuracy did not improve from 0.51895\n",
      "214/214 [==============================] - 115s 539ms/step - loss: 1.6360 - accuracy: 0.4877 - top_5_accuracy: 0.8097 - val_loss: 1.5505 - val_accuracy: 0.5087 - val_top_5_accuracy: 0.8253 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.6794 - accuracy: 0.4984 - top_5_accuracy: 0.8070\n",
      "Epoch 36: val_accuracy did not improve from 0.51895\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "214/214 [==============================] - 115s 537ms/step - loss: 1.6794 - accuracy: 0.4984 - top_5_accuracy: 0.8070 - val_loss: 2.3482 - val_accuracy: 0.4524 - val_top_5_accuracy: 0.6593 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.6736 - accuracy: 0.4816 - top_5_accuracy: 0.8145\n",
      "Epoch 37: val_accuracy did not improve from 0.51895\n",
      "214/214 [==============================] - 113s 532ms/step - loss: 1.6736 - accuracy: 0.4816 - top_5_accuracy: 0.8145 - val_loss: 1.5775 - val_accuracy: 0.5020 - val_top_5_accuracy: 0.8320 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.6093 - accuracy: 0.4994 - top_5_accuracy: 0.8246\n",
      "Epoch 38: val_accuracy did not improve from 0.51895\n",
      "214/214 [==============================] - 118s 552ms/step - loss: 1.6093 - accuracy: 0.4994 - top_5_accuracy: 0.8246 - val_loss: 1.5238 - val_accuracy: 0.5067 - val_top_5_accuracy: 0.8463 - lr: 5.0000e-04\n",
      "Epoch 39/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.5999 - accuracy: 0.4965 - top_5_accuracy: 0.8251\n",
      "Epoch 39: val_accuracy did not improve from 0.51895\n",
      "214/214 [==============================] - 115s 538ms/step - loss: 1.5999 - accuracy: 0.4965 - top_5_accuracy: 0.8251 - val_loss: 1.5028 - val_accuracy: 0.5133 - val_top_5_accuracy: 0.8371 - lr: 5.0000e-04\n",
      "Epoch 40/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.5967 - accuracy: 0.4971 - top_5_accuracy: 0.8240\n",
      "Epoch 40: val_accuracy did not improve from 0.51895\n",
      "214/214 [==============================] - 116s 543ms/step - loss: 1.5967 - accuracy: 0.4971 - top_5_accuracy: 0.8240 - val_loss: 1.5401 - val_accuracy: 0.4969 - val_top_5_accuracy: 0.8279 - lr: 5.0000e-04\n",
      "Epoch 41/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.5872 - accuracy: 0.4961 - top_5_accuracy: 0.8248\n",
      "Epoch 41: val_accuracy did not improve from 0.51895\n",
      "214/214 [==============================] - 117s 551ms/step - loss: 1.5872 - accuracy: 0.4961 - top_5_accuracy: 0.8248 - val_loss: 1.5410 - val_accuracy: 0.5077 - val_top_5_accuracy: 0.8207 - lr: 5.0000e-04\n",
      "Epoch 42/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.5794 - accuracy: 0.4958 - top_5_accuracy: 0.8259\n",
      "Epoch 42: val_accuracy did not improve from 0.51895\n",
      "214/214 [==============================] - 116s 544ms/step - loss: 1.5794 - accuracy: 0.4958 - top_5_accuracy: 0.8259 - val_loss: 1.5104 - val_accuracy: 0.4985 - val_top_5_accuracy: 0.8335 - lr: 5.0000e-04\n",
      "Epoch 43/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.5722 - accuracy: 0.5003 - top_5_accuracy: 0.8290\n",
      "Epoch 43: val_accuracy did not improve from 0.51895\n",
      "214/214 [==============================] - 111s 522ms/step - loss: 1.5722 - accuracy: 0.5003 - top_5_accuracy: 0.8290 - val_loss: 1.4989 - val_accuracy: 0.5072 - val_top_5_accuracy: 0.8432 - lr: 5.0000e-04\n",
      "Epoch 44/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.5859 - accuracy: 0.5016 - top_5_accuracy: 0.8246\n",
      "Epoch 44: val_accuracy did not improve from 0.51895\n",
      "214/214 [==============================] - 115s 541ms/step - loss: 1.5859 - accuracy: 0.5016 - top_5_accuracy: 0.8246 - val_loss: 1.5365 - val_accuracy: 0.4944 - val_top_5_accuracy: 0.8258 - lr: 5.0000e-04\n",
      "Epoch 45/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.5720 - accuracy: 0.5031 - top_5_accuracy: 0.8264\n",
      "Epoch 45: val_accuracy improved from 0.51895 to 0.52152, saving model to rnn_output\\models\\best_model.h5\n",
      "214/214 [==============================] - 121s 570ms/step - loss: 1.5720 - accuracy: 0.5031 - top_5_accuracy: 0.8264 - val_loss: 1.4676 - val_accuracy: 0.5215 - val_top_5_accuracy: 0.8391 - lr: 5.0000e-04\n",
      "Epoch 46/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.5986 - accuracy: 0.4924 - top_5_accuracy: 0.8254\n",
      "Epoch 46: val_accuracy did not improve from 0.52152\n",
      "214/214 [==============================] - 120s 565ms/step - loss: 1.5986 - accuracy: 0.4924 - top_5_accuracy: 0.8254 - val_loss: 1.4979 - val_accuracy: 0.5026 - val_top_5_accuracy: 0.8489 - lr: 5.0000e-04\n",
      "Epoch 47/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.5827 - accuracy: 0.5063 - top_5_accuracy: 0.8271\n",
      "Epoch 47: val_accuracy improved from 0.52152 to 0.52408, saving model to rnn_output\\models\\best_model.h5\n",
      "214/214 [==============================] - 121s 566ms/step - loss: 1.5827 - accuracy: 0.5063 - top_5_accuracy: 0.8271 - val_loss: 1.4872 - val_accuracy: 0.5241 - val_top_5_accuracy: 0.8268 - lr: 5.0000e-04\n",
      "Epoch 48/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.5719 - accuracy: 0.5012 - top_5_accuracy: 0.8281\n",
      "Epoch 48: val_accuracy did not improve from 0.52408\n",
      "214/214 [==============================] - 149s 699ms/step - loss: 1.5719 - accuracy: 0.5012 - top_5_accuracy: 0.8281 - val_loss: 1.5199 - val_accuracy: 0.4846 - val_top_5_accuracy: 0.8356 - lr: 5.0000e-04\n",
      "Epoch 49/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.5733 - accuracy: 0.4985 - top_5_accuracy: 0.8274\n",
      "Epoch 49: val_accuracy did not improve from 0.52408\n",
      "214/214 [==============================] - 176s 827ms/step - loss: 1.5733 - accuracy: 0.4985 - top_5_accuracy: 0.8274 - val_loss: 1.4886 - val_accuracy: 0.5149 - val_top_5_accuracy: 0.8438 - lr: 5.0000e-04\n",
      "Epoch 50/50\n",
      "214/214 [==============================] - ETA: 0s - loss: 1.5636 - accuracy: 0.5060 - top_5_accuracy: 0.8290\n",
      "Epoch 50: val_accuracy did not improve from 0.52408\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "214/214 [==============================] - 126s 590ms/step - loss: 1.5636 - accuracy: 0.5060 - top_5_accuracy: 0.8290 - val_loss: 1.5185 - val_accuracy: 0.5051 - val_top_5_accuracy: 0.8366 - lr: 5.0000e-04\n",
      "\n",
      "‚úì Training completed!\n",
      "‚è±Ô∏è  End time: 2025-11-13 23:51:41\n",
      "‚úì Model saved to: rnn_output\\models\n",
      "\n",
      "============================================================\n",
      "EVALUATING MODEL\n",
      "============================================================\n",
      "\n",
      "üìä Evaluating on test set...\n",
      "30/30 [==============================] - 29s 1s/step - loss: 1.5277 - accuracy: 0.5000 - top_5_accuracy: 0.8292\n",
      "\n",
      "‚úì Test Results:\n",
      "  Loss: 1.5277\n",
      "  Accuracy: 0.5000\n",
      "  Top-5 Accuracy: 0.8292\n",
      "\n",
      "üìà Generating training plots...\n",
      "‚úì Training plots saved to: rnn_output\\plots\\training_history.png\n",
      "\n",
      "üìù Generating training summary...\n",
      "‚úì Training summary saved to: rnn_output\\logs\\training_summary.txt\n",
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "üìÅ Output Directory: rnn_output/\n",
      "üìä Models saved in: rnn_output/models/\n",
      "üìà Plots saved in: rnn_output/plots/\n",
      "üìù Logs saved in: rnn_output/logs/\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "class SimpleRNNDefectDetector:\n",
    "    def __init__(self, dataset_path, img_height=128, img_width=128, sequence_length=16):\n",
    "        \"\"\"\n",
    "        Initialize SimpleRNN Defect Detection Model\n",
    "        \n",
    "        Args:\n",
    "            dataset_path: Path to final_merged_dataset\n",
    "            img_height: Height to resize images\n",
    "            img_width: Width to resize images\n",
    "            sequence_length: Number of feature vectors to treat as sequence\n",
    "        \"\"\"\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.class_names = []\n",
    "        self.num_classes = 0\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir = Path(\"rnn_output\")\n",
    "        self.model_dir = self.output_dir / \"models\"\n",
    "        self.plots_dir = self.output_dir / \"plots\"\n",
    "        self.logs_dir = self.output_dir / \"logs\"\n",
    "        \n",
    "        for dir_path in [self.output_dir, self.model_dir, self.plots_dir, self.logs_dir]:\n",
    "            dir_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def prepare_data(self, validation_split=0.2, test_split=0.1, batch_size=32):\n",
    "        \"\"\"\n",
    "        Prepare dataset with train/validation/test splits\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATA PREPARATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Get all image paths and labels\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        print(\"\\nüìÅ Scanning dataset...\")\n",
    "        for class_dir in self.dataset_path.rglob(\"*\"):\n",
    "            if class_dir.is_dir():\n",
    "                images = list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\")) + \\\n",
    "                         list(class_dir.glob(\"*.jpeg\")) + list(class_dir.glob(\"*.bmp\"))\n",
    "                \n",
    "                if images:\n",
    "                    class_name = class_dir.name\n",
    "                    if class_name not in self.class_names:\n",
    "                        self.class_names.append(class_name)\n",
    "                    \n",
    "                    for img_path in images:\n",
    "                        image_paths.append(str(img_path))\n",
    "                        labels.append(class_name)\n",
    "        \n",
    "        self.num_classes = len(self.class_names)\n",
    "        print(f\"‚úì Found {len(image_paths)} images across {self.num_classes} classes\")\n",
    "        print(f\"‚úì Classes: {self.class_names[:10]}{'...' if len(self.class_names) > 10 else ''}\")\n",
    "        \n",
    "        # Convert labels to integers\n",
    "        label_to_index = {label: idx for idx, label in enumerate(self.class_names)}\n",
    "        labels_encoded = [label_to_index[label] for label in labels]\n",
    "        \n",
    "        # Split data\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            image_paths, labels_encoded, test_size=test_split, random_state=42, stratify=labels_encoded\n",
    "        )\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=validation_split/(1-test_split), random_state=42, stratify=y_temp\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úì Train samples: {len(X_train)}\")\n",
    "        print(f\"‚úì Validation samples: {len(X_val)}\")\n",
    "        print(f\"‚úì Test samples: {len(X_test)}\")\n",
    "        \n",
    "        # Create data generators\n",
    "        self.train_generator = self._create_data_generator(X_train, y_train, batch_size, augment=True)\n",
    "        self.val_generator = self._create_data_generator(X_val, y_val, batch_size, augment=False)\n",
    "        self.test_generator = self._create_data_generator(X_test, y_test, batch_size, augment=False)\n",
    "        \n",
    "        self.steps_per_epoch = len(X_train) // batch_size\n",
    "        self.validation_steps = len(X_val) // batch_size\n",
    "        self.test_steps = len(X_test) // batch_size\n",
    "        \n",
    "        return len(X_train), len(X_val), len(X_test)\n",
    "    \n",
    "    def _create_data_generator(self, image_paths, labels, batch_size, augment=False):\n",
    "        \"\"\"Create a generator that yields batches of sequences\"\"\"\n",
    "        def generator():\n",
    "            indices = np.arange(len(image_paths))\n",
    "            while True:\n",
    "                np.random.shuffle(indices)\n",
    "                \n",
    "                for start_idx in range(0, len(indices) - batch_size + 1, batch_size):\n",
    "                    batch_indices = indices[start_idx:start_idx + batch_size]\n",
    "                    \n",
    "                    batch_images = []\n",
    "                    batch_labels = []\n",
    "                    \n",
    "                    for idx in batch_indices:\n",
    "                        # Load and preprocess image\n",
    "                        img = tf.keras.preprocessing.image.load_img(\n",
    "                            image_paths[idx],\n",
    "                            target_size=(self.img_height, self.img_width)\n",
    "                        )\n",
    "                        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                        img_array = img_array / 255.0  # Normalize\n",
    "                        \n",
    "                        # Data augmentation\n",
    "                        if augment:\n",
    "                            img_array = self._augment_image(img_array)\n",
    "                        \n",
    "                        batch_images.append(img_array)\n",
    "                        batch_labels.append(labels[idx])\n",
    "                    \n",
    "                    # Convert to sequences for RNN\n",
    "                    batch_images = np.array(batch_images)\n",
    "                    batch_labels = np.array(batch_labels)\n",
    "                    \n",
    "                    # Reshape images to sequences: flatten spatial dimensions and create sequences\n",
    "                    # Shape: (batch, height, width, channels) -> (batch, sequence_length, features)\n",
    "                    sequences = self._image_to_sequence(batch_images)\n",
    "                    \n",
    "                    yield sequences, tf.keras.utils.to_categorical(batch_labels, self.num_classes)\n",
    "        \n",
    "        return generator()\n",
    "    \n",
    "    def _augment_image(self, img_array):\n",
    "        \"\"\"Apply random augmentations to image\"\"\"\n",
    "        # Random flip\n",
    "        if np.random.random() > 0.5:\n",
    "            img_array = tf.image.flip_left_right(img_array)\n",
    "        \n",
    "        # Random brightness\n",
    "        img_array = tf.image.random_brightness(img_array, max_delta=0.2)\n",
    "        \n",
    "        # Random contrast\n",
    "        img_array = tf.image.random_contrast(img_array, lower=0.8, upper=1.2)\n",
    "        \n",
    "        return tf.clip_by_value(img_array, 0.0, 1.0)\n",
    "    \n",
    "    def _image_to_sequence(self, images):\n",
    "        \"\"\"\n",
    "        Convert images to sequences for RNN processing\n",
    "        Split image into horizontal strips to create a sequence\n",
    "        \"\"\"\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Split image into sequence_length horizontal strips\n",
    "        strip_height = self.img_height // self.sequence_length\n",
    "        sequences = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            img = images[i]\n",
    "            strips = []\n",
    "            \n",
    "            for j in range(self.sequence_length):\n",
    "                start_h = j * strip_height\n",
    "                end_h = start_h + strip_height\n",
    "                strip = img[start_h:end_h, :, :]\n",
    "                # Flatten strip to create feature vector\n",
    "                strip_flat = strip.reshape(-1)\n",
    "                strips.append(strip_flat)\n",
    "            \n",
    "            sequences.append(strips)\n",
    "        \n",
    "        return np.array(sequences)\n",
    "    \n",
    "    def build_model(self, rnn_units=128, dropout_rate=0.3, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Build SimpleRNN model architecture\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BUILDING SIMPLERNN MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Calculate input feature size\n",
    "        strip_height = self.img_height // self.sequence_length\n",
    "        feature_size = strip_height * self.img_width * 3  # 3 for RGB channels\n",
    "        \n",
    "        model = keras.Sequential([\n",
    "            # Input layer\n",
    "            layers.Input(shape=(self.sequence_length, feature_size)),\n",
    "            \n",
    "            # First SimpleRNN layer with return sequences\n",
    "            layers.SimpleRNN(rnn_units, return_sequences=True, activation='tanh'),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            \n",
    "            # Second SimpleRNN layer\n",
    "            layers.SimpleRNN(rnn_units // 2, activation='tanh'),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            \n",
    "            # Dense layers\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            \n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(dropout_rate / 2),\n",
    "            \n",
    "            # Output layer\n",
    "            layers.Dense(self.num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(\"\\n‚úì Model architecture:\")\n",
    "        model.summary()\n",
    "        \n",
    "        # Save model architecture\n",
    "        with open(self.model_dir / 'model_architecture.txt', 'w') as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, epochs=50, early_stopping_patience=10):\n",
    "        \"\"\"\n",
    "        Train the SimpleRNN model\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING SIMPLERNN MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built. Call build_model() first.\")\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=early_stopping_patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath=str(self.model_dir / 'best_model.h5'),\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.CSVLogger(\n",
    "                str(self.logs_dir / 'training_log.csv')\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting training for {epochs} epochs...\")\n",
    "        print(f\"‚è±Ô∏è  Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            self.train_generator,\n",
    "            steps_per_epoch=self.steps_per_epoch,\n",
    "            epochs=epochs,\n",
    "            validation_data=self.val_generator,\n",
    "            validation_steps=self.validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úì Training completed!\")\n",
    "        print(f\"‚è±Ô∏è  End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Save final model\n",
    "        self.model.save(self.model_dir / 'final_model.h5')\n",
    "        print(f\"‚úì Model saved to: {self.model_dir}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model on test set\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EVALUATING MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        print(\"\\nüìä Evaluating on test set...\")\n",
    "        test_results = self.model.evaluate(\n",
    "            self.test_generator,\n",
    "            steps=self.test_steps,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úì Test Results:\")\n",
    "        print(f\"  Loss: {test_results[0]:.4f}\")\n",
    "        print(f\"  Accuracy: {test_results[1]:.4f}\")\n",
    "        print(f\"  Top-5 Accuracy: {test_results[2]:.4f}\")\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'test_loss': float(test_results[0]),\n",
    "            'test_accuracy': float(test_results[1]),\n",
    "            'test_top5_accuracy': float(test_results[2]),\n",
    "            'num_classes': self.num_classes,\n",
    "            'evaluation_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(self.logs_dir / 'test_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"\n",
    "        Plot training history\n",
    "        \"\"\"\n",
    "        print(\"\\nüìà Generating training plots...\")\n",
    "        \n",
    "        if self.history is None:\n",
    "            print(\"‚ö†Ô∏è  No training history available\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        axes[0, 0].plot(self.history.history['accuracy'], label='Train Accuracy')\n",
    "        axes[0, 0].plot(self.history.history['val_accuracy'], label='Val Accuracy')\n",
    "        axes[0, 0].set_title('Model Accuracy')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Accuracy')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[0, 1].plot(self.history.history['loss'], label='Train Loss')\n",
    "        axes[0, 1].plot(self.history.history['val_loss'], label='Val Loss')\n",
    "        axes[0, 1].set_title('Model Loss')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Plot top-5 accuracy\n",
    "        axes[1, 0].plot(self.history.history['top_5_accuracy'], label='Train Top-5 Acc')\n",
    "        axes[1, 0].plot(self.history.history['val_top_5_accuracy'], label='Val Top-5 Acc')\n",
    "        axes[1, 0].set_title('Top-5 Accuracy')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Accuracy')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Plot learning rate (if available)\n",
    "        if 'lr' in self.history.history:\n",
    "            axes[1, 1].plot(self.history.history['lr'])\n",
    "            axes[1, 1].set_title('Learning Rate')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Learning Rate')\n",
    "            axes[1, 1].set_yscale('log')\n",
    "            axes[1, 1].grid(True)\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'Learning Rate\\nNot Tracked', \n",
    "                           ha='center', va='center', fontsize=14)\n",
    "            axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úì Training plots saved to: {self.plots_dir / 'training_history.png'}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def save_training_summary(self):\n",
    "        \"\"\"\n",
    "        Save comprehensive training summary\n",
    "        \"\"\"\n",
    "        print(\"\\nüìù Generating training summary...\")\n",
    "        \n",
    "        summary_file = self.logs_dir / 'training_summary.txt'\n",
    "        \n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "            f.write(\"SIMPLERNN DEFECT DETECTION - TRAINING SUMMARY\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            f.write(\"DATASET INFORMATION\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            f.write(f\"Dataset Path: {self.dataset_path}\\n\")\n",
    "            f.write(f\"Number of Classes: {self.num_classes}\\n\")\n",
    "            f.write(f\"Image Size: {self.img_height}x{self.img_width}\\n\")\n",
    "            f.write(f\"Sequence Length: {self.sequence_length}\\n\\n\")\n",
    "            \n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            f.write(\"MODEL CONFIGURATION\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            f.write(\"Architecture: SimpleRNN with Dense layers\\n\")\n",
    "            f.write(f\"Total Parameters: {self.model.count_params():,}\\n\\n\")\n",
    "            \n",
    "            if self.history:\n",
    "                f.write(\"-\"*70 + \"\\n\")\n",
    "                f.write(\"TRAINING RESULTS\\n\")\n",
    "                f.write(\"-\"*70 + \"\\n\")\n",
    "                f.write(f\"Epochs Trained: {len(self.history.history['loss'])}\\n\")\n",
    "                f.write(f\"Best Train Accuracy: {max(self.history.history['accuracy']):.4f}\\n\")\n",
    "                f.write(f\"Best Val Accuracy: {max(self.history.history['val_accuracy']):.4f}\\n\")\n",
    "                f.write(f\"Final Train Loss: {self.history.history['loss'][-1]:.4f}\\n\")\n",
    "                f.write(f\"Final Val Loss: {self.history.history['val_loss'][-1]:.4f}\\n\\n\")\n",
    "            \n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        print(f\"‚úì Training summary saved to: {summary_file}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SIMPLERNN DEFECT DETECTION PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Configuration\n",
    "    DATASET_PATH = \"final_merged_dataset\"\n",
    "    IMG_HEIGHT = 128\n",
    "    IMG_WIDTH = 128\n",
    "    SEQUENCE_LENGTH = 16\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 50\n",
    "    RNN_UNITS = 128\n",
    "    DROPOUT_RATE = 0.3\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # Initialize detector\n",
    "    detector = SimpleRNNDefectDetector(\n",
    "        dataset_path=DATASET_PATH,\n",
    "        img_height=IMG_HEIGHT,\n",
    "        img_width=IMG_WIDTH,\n",
    "        sequence_length=SEQUENCE_LENGTH\n",
    "    )\n",
    "    \n",
    "    # Prepare data\n",
    "    train_size, val_size, test_size = detector.prepare_data(\n",
    "        validation_split=0.2,\n",
    "        test_split=0.1,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # Build model\n",
    "    detector.build_model(\n",
    "        rnn_units=RNN_UNITS,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        learning_rate=LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    detector.train(epochs=EPOCHS, early_stopping_patience=10)\n",
    "    \n",
    "    # Evaluate model\n",
    "    test_results = detector.evaluate()\n",
    "    \n",
    "    # Plot training history\n",
    "    detector.plot_training_history()\n",
    "    \n",
    "    # Save summary\n",
    "    detector.save_training_summary()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nüìÅ Output Directory: rnn_output/\")\n",
    "    print(f\"üìä Models saved in: rnn_output/models/\")\n",
    "    print(f\"üìà Plots saved in: rnn_output/plots/\")\n",
    "    print(f\"üìù Logs saved in: rnn_output/logs/\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c38eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "569e23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "class DefectDetectionModel:\n",
    "    \"\"\"Base class for defect detection models\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, model_name, img_height=224, img_width=224):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.model_name = model_name\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.class_names = []\n",
    "        self.num_classes = 0\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir = Path(f\"{model_name}_output\")\n",
    "        self.model_dir = self.output_dir / \"models\"\n",
    "        self.plots_dir = self.output_dir / \"plots\"\n",
    "        self.logs_dir = self.output_dir / \"logs\"\n",
    "        \n",
    "        for dir_path in [self.output_dir, self.model_dir, self.plots_dir, self.logs_dir]:\n",
    "            dir_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def prepare_data(self, validation_split=0.2, test_split=0.1, batch_size=32):\n",
    "        \"\"\"Prepare dataset with efficient TensorFlow pipeline\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"DATA PREPARATION - {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Get all image paths and labels\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        print(\"\\nüìÅ Scanning dataset...\")\n",
    "        for class_dir in self.dataset_path.rglob(\"*\"):\n",
    "            if class_dir.is_dir():\n",
    "                images = (list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\")) + \n",
    "                         list(class_dir.glob(\"*.jpeg\")) + list(class_dir.glob(\"*.bmp\")))\n",
    "                \n",
    "                if images:\n",
    "                    class_name = class_dir.name\n",
    "                    if class_name not in self.class_names:\n",
    "                        self.class_names.append(class_name)\n",
    "                    \n",
    "                    for img_path in images:\n",
    "                        image_paths.append(str(img_path))\n",
    "                        labels.append(class_name)\n",
    "        \n",
    "        self.class_names.sort()  # Sort for consistency\n",
    "        self.num_classes = len(self.class_names)\n",
    "        print(f\"‚úì Found {len(image_paths)} images\")\n",
    "        print(f\"‚úì Number of classes: {self.num_classes}\")\n",
    "        print(f\"‚úì Classes: {self.class_names[:10]}{'...' if len(self.class_names) > 10 else ''}\")\n",
    "        \n",
    "        # Create label mapping\n",
    "        label_to_index = {label: idx for idx, label in enumerate(self.class_names)}\n",
    "        labels_encoded = [label_to_index[label] for label in labels]\n",
    "        \n",
    "        # Create tf.data.Dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels_encoded))\n",
    "        dataset = dataset.shuffle(len(image_paths), seed=42)\n",
    "        \n",
    "        # Calculate splits\n",
    "        train_size = int(len(image_paths) * (1 - validation_split - test_split))\n",
    "        val_size = int(len(image_paths) * validation_split)\n",
    "        test_size = len(image_paths) - train_size - val_size\n",
    "        \n",
    "        # Split datasets\n",
    "        train_ds = dataset.take(train_size)\n",
    "        val_ds = dataset.skip(train_size).take(val_size)\n",
    "        test_ds = dataset.skip(train_size + val_size)\n",
    "        \n",
    "        # Process datasets\n",
    "        self.train_dataset = train_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=True),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.val_dataset = val_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=False),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.test_dataset = test_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=False),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        print(f\"\\n‚úì Train samples: {train_size}\")\n",
    "        print(f\"‚úì Validation samples: {val_size}\")\n",
    "        print(f\"‚úì Test samples: {test_size}\")\n",
    "        \n",
    "        return train_size, val_size, test_size\n",
    "    \n",
    "    def _load_and_preprocess(self, image_path, label, augment=False):\n",
    "        \"\"\"Load and preprocess image\"\"\"\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, [self.img_height, self.img_width])\n",
    "        \n",
    "        # Data augmentation\n",
    "        if augment:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_flip_up_down(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "            \n",
    "            # Random rotation\n",
    "            img = tf.image.rot90(img, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "        \n",
    "        img = tf.clip_by_value(img, 0.0, 255.0)\n",
    "        \n",
    "        # Don't normalize here - let the model handle it\n",
    "        # This avoids serialization issues\n",
    "        \n",
    "        label_onehot = tf.one_hot(label, self.num_classes)\n",
    "        return img, label_onehot\n",
    "    \n",
    "    def _normalize_image(self, img):\n",
    "        \"\"\"Normalize image - override in subclasses if needed\"\"\"\n",
    "        return img  # Keep raw [0, 255] values\n",
    "    \n",
    "    def train(self, epochs=50, early_stopping_patience=10):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"TRAINING {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built. Call build_model() first.\")\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=early_stopping_patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath=str(self.model_dir / 'best_model.h5'),\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.CSVLogger(\n",
    "                str(self.logs_dir / 'training_log.csv')\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting training for {epochs} epochs...\")\n",
    "        print(f\"‚è±Ô∏è  Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            self.train_dataset,\n",
    "            epochs=epochs,\n",
    "            validation_data=self.val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úì Training completed!\")\n",
    "        print(f\"‚è±Ô∏è  End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Save final model using SavedModel format\n",
    "        try:\n",
    "            self.model.save(self.model_dir / 'final_model.h5')\n",
    "            print(f\"‚úì Model saved to: {self.model_dir / 'final_model.h5'}\")\n",
    "        except:\n",
    "            # If H5 format fails, use SavedModel format\n",
    "            self.model.save(str(self.model_dir / 'final_model'))\n",
    "            print(f\"‚úì Model saved to: {self.model_dir / 'final_model'}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"EVALUATING {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìä Evaluating on test set...\")\n",
    "        test_results = self.model.evaluate(self.test_dataset, verbose=1)\n",
    "        \n",
    "        print(\"\\n‚úì Test Results:\")\n",
    "        print(f\"  Loss: {test_results[0]:.4f}\")\n",
    "        print(f\"  Accuracy: {test_results[1]:.4f}\")\n",
    "        if len(test_results) > 2:\n",
    "            print(f\"  Top-5 Accuracy: {test_results[2]:.4f}\")\n",
    "        \n",
    "        # Get predictions for confusion matrix\n",
    "        print(\"\\nüìä Generating detailed metrics...\")\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for images, labels in self.test_dataset:\n",
    "            predictions = self.model.predict(images, verbose=0)\n",
    "            y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'test_loss': float(test_results[0]),\n",
    "            'test_accuracy': float(test_results[1]),\n",
    "            'num_classes': self.num_classes,\n",
    "            'evaluation_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        if len(test_results) > 2:\n",
    "            results['test_top5_accuracy'] = float(test_results[2])\n",
    "        \n",
    "        with open(self.logs_dir / 'test_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        self._plot_confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Generate classification report\n",
    "        self._save_classification_report(y_true, y_pred)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _plot_confusion_matrix(self, y_true, y_pred):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(max(10, self.num_classes), max(8, self.num_classes * 0.8)))\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        sns.heatmap(cm_normalized, annot=False, cmap='Blues', \n",
    "                    xticklabels=self.class_names if self.num_classes <= 20 else False,\n",
    "                    yticklabels=self.class_names if self.num_classes <= 20 else False)\n",
    "        \n",
    "        plt.title(f'{self.model_name} - Confusion Matrix (Normalized)')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Confusion matrix saved to: {self.plots_dir / 'confusion_matrix.png'}\")\n",
    "    \n",
    "    def _save_classification_report(self, y_true, y_pred):\n",
    "        \"\"\"Save classification report\"\"\"\n",
    "        report = classification_report(y_true, y_pred, target_names=self.class_names, \n",
    "                                      zero_division=0)\n",
    "        \n",
    "        with open(self.logs_dir / 'classification_report.txt', 'w') as f:\n",
    "            f.write(f\"{self.model_name} - Classification Report\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"‚úì Classification report saved to: {self.logs_dir / 'classification_report.txt'}\")\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"‚ö†Ô∏è  No training history available\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüìà Generating training plots...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        axes[0].plot(self.history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "        axes[0].plot(self.history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "        axes[0].set_title(f'{self.model_name} - Model Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[1].plot(self.history.history['loss'], label='Train Loss', linewidth=2)\n",
    "        axes[1].plot(self.history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "        axes[1].set_title(f'{self.model_name} - Model Loss', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Training plots saved to: {self.plots_dir / 'training_history.png'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c441770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorSafeCSVLogger(keras.callbacks.Callback):\n",
    "    \"\"\"Ensures metrics are converted to float before CSVLogger writes them\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        super().__init__()\n",
    "        self.csv_logger = keras.callbacks.CSVLogger(filename)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        safe_logs = {}\n",
    "        for k, v in logs.items():\n",
    "            if hasattr(v, \"numpy\"):\n",
    "                safe_logs[k] = float(v.numpy())\n",
    "            elif isinstance(v, (np.ndarray, list)):\n",
    "                safe_logs[k] = float(np.mean(v))\n",
    "            else:\n",
    "                safe_logs[k] = float(v)\n",
    "        self.csv_logger.on_epoch_end(epoch, safe_logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.csv_logger.on_train_end(logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead9efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetDefectDetector(DefectDetectionModel):\n",
    "    \"\"\"EfficientNet-B0 for defect detection\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, img_height=224, img_width=224):\n",
    "        super().__init__(dataset_path, \"EfficientNetB0\", img_height, img_width)\n",
    "    \n",
    "    def build_model(self, learning_rate=0.001, dropout_rate=0.3, fine_tune_layers=50):\n",
    "        \"\"\"Build EfficientNet model with transfer learning\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"BUILDING EFFICIENTNET MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load pre-trained EfficientNet\n",
    "        base_model = EfficientNetB0(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(self.img_height, self.img_width, 3)\n",
    "        )\n",
    "        \n",
    "        # Freeze base model initially\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Build model (EfficientNet expects [0, 255] range)\n",
    "        inputs = keras.Input(shape=(self.img_height, self.img_width, 3))\n",
    "        x = base_model(inputs, training=False)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate / 2)(x)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(\"\\n‚úì Model built successfully!\")\n",
    "        print(f\"‚úì Total parameters: {model.count_params():,}\")\n",
    "        print(f\"‚úì Base model frozen: {not base_model.trainable}\")\n",
    "        \n",
    "        # Save architecture\n",
    "        with open(self.model_dir / 'model_architecture.txt', 'w') as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def unfreeze_and_fine_tune(self, fine_tune_layers=50, learning_rate=0.0001):\n",
    "        \"\"\"Unfreeze top layers for fine-tuning\"\"\"\n",
    "        print(f\"\\nüîì Unfreezing top {fine_tune_layers} layers for fine-tuning...\")\n",
    "        \n",
    "        base_model = self.model.layers[1]  # Get EfficientNet base\n",
    "        base_model.trainable = True\n",
    "        \n",
    "        # Freeze all layers except the last fine_tune_layers\n",
    "        for layer in base_model.layers[:-fine_tune_layers]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Recompile with lower learning rate\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Fine-tuning enabled with learning rate: {learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0984decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50DefectDetector(DefectDetectionModel):\n",
    "    \"\"\"ResNet-50 for defect detection\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, img_height=224, img_width=224):\n",
    "        super().__init__(dataset_path, \"ResNet50\", img_height, img_width)\n",
    "    \n",
    "    def _normalize_image(self, img):\n",
    "        \"\"\"ResNet uses standard normalization\"\"\"\n",
    "        return img / 255.0\n",
    "    \n",
    "    def build_model(self, learning_rate=0.001, dropout_rate=0.3):\n",
    "        \"\"\"Build ResNet-50 model with transfer learning\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"BUILDING RESNET-50 MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load pre-trained ResNet50\n",
    "        base_model = ResNet50(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(self.img_height, self.img_width, 3)\n",
    "        )\n",
    "        \n",
    "        # Freeze base model initially\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Build model with preprocessing\n",
    "        inputs = keras.Input(shape=(self.img_height, self.img_width, 3))\n",
    "        \n",
    "        # Add preprocessing for ResNet (ImageNet normalization)\n",
    "        x = layers.Rescaling(scale=1./255)(inputs)\n",
    "        x = layers.Normalization(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            variance=[0.229**2, 0.224**2, 0.225**2]\n",
    "        )(x)\n",
    "        \n",
    "        x = base_model(x, training=False)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(1024, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate / 2)(x)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(\"\\n‚úì Model built successfully!\")\n",
    "        print(f\"‚úì Total parameters: {model.count_params():,}\")\n",
    "        print(f\"‚úì Base model frozen: {not base_model.trainable}\")\n",
    "        \n",
    "        # Save architecture\n",
    "        with open(self.model_dir / 'model_architecture.txt', 'w') as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def unfreeze_and_fine_tune(self, fine_tune_from_layer=140, learning_rate=0.0001):\n",
    "        \"\"\"Unfreeze layers for fine-tuning\"\"\"\n",
    "        print(f\"\\nüîì Unfreezing layers from layer {fine_tune_from_layer}...\")\n",
    "        \n",
    "        base_model = self.model.layers[1]  # Get ResNet base\n",
    "        base_model.trainable = True\n",
    "        \n",
    "        # Freeze layers before fine_tune_from_layer\n",
    "        for layer in base_model.layers[:fine_tune_from_layer]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Recompile\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Fine-tuning enabled with learning rate: {learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c8038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerDefectDetector(DefectDetectionModel):\n",
    "    \"\"\"Vision Transformer (ViT) for defect detection\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, img_height=224, img_width=224, patch_size=16):\n",
    "        super().__init__(dataset_path, \"VisionTransformer\", img_height, img_width)\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_height // patch_size) * (img_width // patch_size)\n",
    "    \n",
    "    def build_model(self, transformer_layers=8, num_heads=8, mlp_dim=2048, \n",
    "                    dropout_rate=0.1, learning_rate=0.001):\n",
    "        \"\"\"Build Vision Transformer model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"BUILDING VISION TRANSFORMER MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        inputs = keras.Input(shape=(self.img_height, self.img_width, 3))\n",
    "        \n",
    "        # Normalize images to [0, 1]\n",
    "        x = layers.Rescaling(scale=1./255)(inputs)\n",
    "        \n",
    "        # Create patches\n",
    "        patches = self._create_patches(x)\n",
    "        \n",
    "        # Patch embedding\n",
    "        projection_dim = 768\n",
    "        patch_embeddings = layers.Dense(projection_dim)(patches)\n",
    "        \n",
    "        # Position embedding\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        position_embeddings = layers.Embedding(\n",
    "            input_dim=self.num_patches, output_dim=projection_dim\n",
    "        )(positions)\n",
    "        \n",
    "        # Add position embeddings to patch embeddings\n",
    "        encoded = patch_embeddings + position_embeddings\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for _ in range(transformer_layers):\n",
    "            # Layer normalization 1\n",
    "            x1 = layers.LayerNormalization(epsilon=1e-6)(encoded)\n",
    "            \n",
    "            # Multi-head attention\n",
    "            attention_output = layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, key_dim=projection_dim // num_heads, dropout=dropout_rate\n",
    "            )(x1, x1)\n",
    "            \n",
    "            # Skip connection 1\n",
    "            x2 = layers.Add()([attention_output, encoded])\n",
    "            \n",
    "            # Layer normalization 2\n",
    "            x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "            \n",
    "            # MLP\n",
    "            x3 = layers.Dense(mlp_dim, activation='gelu')(x3)\n",
    "            x3 = layers.Dropout(dropout_rate)(x3)\n",
    "            x3 = layers.Dense(projection_dim)(x3)\n",
    "            x3 = layers.Dropout(dropout_rate)(x3)\n",
    "            \n",
    "            # Skip connection 2\n",
    "            encoded = layers.Add()([x3, x2])\n",
    "        \n",
    "        # Final layer normalization\n",
    "        representation = layers.LayerNormalization(epsilon=1e-6)(encoded)\n",
    "        \n",
    "        # Global average pooling\n",
    "        representation = layers.GlobalAveragePooling1D()(representation)\n",
    "        \n",
    "        # Classification head\n",
    "        representation = layers.Dropout(dropout_rate)(representation)\n",
    "        features = layers.Dense(mlp_dim, activation='gelu')(representation)\n",
    "        features = layers.Dropout(dropout_rate)(features)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(features)\n",
    "        \n",
    "        # Create model\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=0.0001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(\"\\n‚úì Vision Transformer built successfully!\")\n",
    "        print(f\"‚úì Total parameters: {model.count_params():,}\")\n",
    "        print(f\"‚úì Number of patches: {self.num_patches}\")\n",
    "        print(f\"‚úì Transformer layers: {transformer_layers}\")\n",
    "        print(f\"‚úì Attention heads: {num_heads}\")\n",
    "        \n",
    "        # Save architecture\n",
    "        with open(self.model_dir / 'model_architecture.txt', 'w') as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _create_patches(self, images):\n",
    "        \"\"\"Extract patches from images\"\"\"\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d13f892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ADVANCED DEFECT DETECTION PIPELINE\n",
      "======================================================================\n",
      "\n",
      "\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "TRAINING EFFICIENTNET\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "\n",
      "======================================================================\n",
      "DATA PREPARATION - EfficientNetB0\n",
      "======================================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images\n",
      "‚úì Number of classes: 63\n",
      "‚úì Classes: ['Missing_hole', 'Missing_hole_rotation', 'Mouse_bite', 'Mouse_bite_rotation', 'Open_circuit', 'Open_circuit_rotation', 'PCB_USED', 'Short', 'Short_rotation', 'Spur']...\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1961\n",
      "‚úì Test samples: 982\n",
      "\n",
      "======================================================================\n",
      "BUILDING EFFICIENTNET MODEL\n",
      "======================================================================\n",
      "\n",
      "‚úì Model built successfully!\n",
      "‚úì Total parameters: 4,858,082\n",
      "‚úì Base model frozen: True\n",
      "\n",
      "======================================================================\n",
      "TRAINING EfficientNetB0\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting training for 30 epochs...\n",
      "‚è±Ô∏è  Start time: 2025-11-14 11:28:35\n",
      "\n",
      "Epoch 1/30\n",
      "215/215 [==============================] - ETA: 0s - loss: 1.5335 - accuracy: 0.6287 - top_5_accuracy: 0.8012\n",
      "Epoch 1: val_accuracy improved from -inf to 0.67466, saving model to EfficientNetB0_output\\models\\best_model.h5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7284\\613697484.py\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7284\\613697484.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mefficientnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mefficientnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mefficientnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EfficientNet'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mefficientnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mefficientnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_training_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7284\\356607983.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, early_stopping_patience)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         self.history = self.model.fit(\n\u001b[0m\u001b[0;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m     return cls(\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[0mskipkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_ascii\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>."
     ]
    }
   ],
   "source": [
    "def compare_models(results_dict):\n",
    "    \"\"\"Compare results from different models\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    comparison_data = []\n",
    "    for model_name, results in results_dict.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Test Accuracy': f\"{results['test_accuracy']*100:.2f}%\",\n",
    "            'Test Loss': f\"{results['test_loss']:.4f}\",\n",
    "            'Top-5 Accuracy': f\"{results.get('test_top5_accuracy', 0)*100:.2f}%\" if 'test_top5_accuracy' in results else 'N/A'\n",
    "        })\n",
    "    \n",
    "    print(\"\\nüìä Performance Comparison:\")\n",
    "    print(\"-\" * 70)\n",
    "    for data in comparison_data:\n",
    "        print(f\"\\n{data['Model']}:\")\n",
    "        print(f\"  Accuracy: {data['Test Accuracy']}\")\n",
    "        print(f\"  Loss: {data['Test Loss']}\")\n",
    "        print(f\"  Top-5 Acc: {data['Top-5 Accuracy']}\")\n",
    "    \n",
    "    # Save comparison\n",
    "    with open('model_comparison.json', 'w') as f:\n",
    "        json.dump(comparison_data, f, indent=2)\n",
    "    \n",
    "    print(\"\\n‚úì Comparison saved to: model_comparison.json\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ADVANCED DEFECT DETECTION PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    DATASET_PATH = \"final_merged_dataset\"\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. EfficientNet\n",
    "    print(\"\\n\\n\" + \"üî∑\"*35)\n",
    "    print(\"TRAINING EFFICIENTNET\")\n",
    "    print(\"üî∑\"*35)\n",
    "    \n",
    "    efficientnet = EfficientNetDefectDetector(DATASET_PATH)\n",
    "    efficientnet.prepare_data(batch_size=BATCH_SIZE)\n",
    "    efficientnet.build_model(learning_rate=0.001)\n",
    "    efficientnet.train(epochs=EPOCHS)\n",
    "    results['EfficientNet'] = efficientnet.evaluate()\n",
    "    efficientnet.plot_training_history()\n",
    "    \n",
    "    # 2. ResNet-50\n",
    "    print(\"\\n\\n\" + \"üî∂\"*35)\n",
    "    print(\"TRAINING RESNET-50\")\n",
    "    print(\"üî∂\"*35)\n",
    "    \n",
    "    resnet = ResNet50DefectDetector(DATASET_PATH)\n",
    "    resnet.prepare_data(batch_size=BATCH_SIZE)\n",
    "    resnet.build_model(learning_rate=0.001)\n",
    "    resnet.train(epochs=EPOCHS)\n",
    "    results['ResNet50'] = resnet.evaluate()\n",
    "    resnet.plot_training_history()\n",
    "    \n",
    "    # 3. Vision Transformer\n",
    "    print(\"\\n\\n\" + \"üîπ\"*35)\n",
    "    print(\"TRAINING VISION TRANSFORMER\")\n",
    "    print(\"üîπ\"*35)\n",
    "    \n",
    "    vit = VisionTransformerDefectDetector(DATASET_PATH)\n",
    "    vit.prepare_data(batch_size=BATCH_SIZE)\n",
    "    vit.build_model(transformer_layers=8, num_heads=8, learning_rate=0.001)\n",
    "    vit.train(epochs=EPOCHS)\n",
    "    results['VisionTransformer'] = vit.evaluate()\n",
    "    vit.plot_training_history()\n",
    "    \n",
    "    # Compare all models\n",
    "    compare_models(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ ALL MODELS TRAINED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nüìÅ Output directories:\")\n",
    "    print(\"  - EfficientNetB0_output/\")\n",
    "    print(\"  - ResNet50_output/\")\n",
    "    print(\"  - VisionTransformer_output/\")\n",
    "    print(\"\\n‚úì Check model_comparison.json for detailed comparison\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e21943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ADVANCED DEFECT DETECTION PIPELINE\n",
      "======================================================================\n",
      "\n",
      "\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "TRAINING EFFICIENTNET\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "\n",
      "======================================================================\n",
      "DATA PREPARATION - EfficientNetB0\n",
      "======================================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images\n",
      "‚úì Number of classes: 63\n",
      "‚úì Classes: ['Missing_hole', 'Missing_hole_rotation', 'Mouse_bite', 'Mouse_bite_rotation', 'Open_circuit', 'Open_circuit_rotation', 'PCB_USED', 'Short', 'Short_rotation', 'Spur']...\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1961\n",
      "‚úì Test samples: 982\n",
      "\n",
      "======================================================================\n",
      "BUILDING EFFICIENTNET MODEL\n",
      "======================================================================\n",
      "\n",
      "‚úì Model built successfully!\n",
      "‚úì Total parameters: 4,858,082\n",
      "‚úì Base model frozen: True\n",
      "\n",
      "======================================================================\n",
      "TRAINING EfficientNetB0\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting training for 50 epochs...\n",
      "‚è±Ô∏è  Start time: 2025-12-07 11:12:35\n",
      "\n",
      "Epoch 1/50\n",
      "215/215 [==============================] - 102s 441ms/step - loss: 1.5455 - accuracy: 0.6248 - top_5_accuracy: 0.7945 - val_loss: 1.2169 - val_accuracy: 0.6787 - val_top_5_accuracy: 0.8695 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "215/215 [==============================] - 68s 316ms/step - loss: 1.1926 - accuracy: 0.6606 - top_5_accuracy: 0.8685 - val_loss: 0.9894 - val_accuracy: 0.7088 - val_top_5_accuracy: 0.9123 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "215/215 [==============================] - 62s 288ms/step - loss: 1.0712 - accuracy: 0.6829 - top_5_accuracy: 0.8905 - val_loss: 0.9287 - val_accuracy: 0.7292 - val_top_5_accuracy: 0.9291 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "215/215 [==============================] - 53s 243ms/step - loss: 0.9923 - accuracy: 0.6940 - top_5_accuracy: 0.9101 - val_loss: 0.8809 - val_accuracy: 0.7032 - val_top_5_accuracy: 0.9281 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "215/215 [==============================] - 55s 254ms/step - loss: 0.9633 - accuracy: 0.6967 - top_5_accuracy: 0.9145 - val_loss: 0.7750 - val_accuracy: 0.7425 - val_top_5_accuracy: 0.9439 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "215/215 [==============================] - 52s 240ms/step - loss: 0.9347 - accuracy: 0.7053 - top_5_accuracy: 0.9189 - val_loss: 0.8469 - val_accuracy: 0.7221 - val_top_5_accuracy: 0.9327 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "215/215 [==============================] - 37s 172ms/step - loss: 0.8865 - accuracy: 0.7205 - top_5_accuracy: 0.9282 - val_loss: 0.7990 - val_accuracy: 0.7374 - val_top_5_accuracy: 0.9429 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "215/215 [==============================] - 39s 181ms/step - loss: 0.8919 - accuracy: 0.7113 - top_5_accuracy: 0.9267 - val_loss: 0.8211 - val_accuracy: 0.7241 - val_top_5_accuracy: 0.9408 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "215/215 [==============================] - 34s 158ms/step - loss: 0.8796 - accuracy: 0.7154 - top_5_accuracy: 0.9342 - val_loss: 0.7726 - val_accuracy: 0.7297 - val_top_5_accuracy: 0.9378 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "215/215 [==============================] - 33s 153ms/step - loss: 0.8438 - accuracy: 0.7224 - top_5_accuracy: 0.9320 - val_loss: 0.7092 - val_accuracy: 0.7598 - val_top_5_accuracy: 0.9556 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "215/215 [==============================] - 33s 153ms/step - loss: 0.8220 - accuracy: 0.7232 - top_5_accuracy: 0.9361 - val_loss: 0.6643 - val_accuracy: 0.7649 - val_top_5_accuracy: 0.9587 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "215/215 [==============================] - 32s 147ms/step - loss: 0.8116 - accuracy: 0.7292 - top_5_accuracy: 0.9400 - val_loss: 0.7361 - val_accuracy: 0.7486 - val_top_5_accuracy: 0.9449 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "215/215 [==============================] - 33s 155ms/step - loss: 0.8107 - accuracy: 0.7266 - top_5_accuracy: 0.9406 - val_loss: 0.6892 - val_accuracy: 0.7568 - val_top_5_accuracy: 0.9500 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "215/215 [==============================] - 32s 147ms/step - loss: 0.7788 - accuracy: 0.7423 - top_5_accuracy: 0.9419 - val_loss: 0.6636 - val_accuracy: 0.7751 - val_top_5_accuracy: 0.9567 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "215/215 [==============================] - 35s 164ms/step - loss: 0.7834 - accuracy: 0.7378 - top_5_accuracy: 0.9388 - val_loss: 0.6866 - val_accuracy: 0.7578 - val_top_5_accuracy: 0.9505 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "215/215 [==============================] - 41s 189ms/step - loss: 0.7749 - accuracy: 0.7359 - top_5_accuracy: 0.9400 - val_loss: 0.6275 - val_accuracy: 0.7715 - val_top_5_accuracy: 0.9602 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "215/215 [==============================] - 54s 250ms/step - loss: 0.7712 - accuracy: 0.7420 - top_5_accuracy: 0.9416 - val_loss: 0.6927 - val_accuracy: 0.7649 - val_top_5_accuracy: 0.9612 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "215/215 [==============================] - 56s 261ms/step - loss: 0.7628 - accuracy: 0.7404 - top_5_accuracy: 0.9455 - val_loss: 0.6692 - val_accuracy: 0.7552 - val_top_5_accuracy: 0.9577 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "215/215 [==============================] - 54s 249ms/step - loss: 0.7441 - accuracy: 0.7480 - top_5_accuracy: 0.9461 - val_loss: 0.6936 - val_accuracy: 0.7460 - val_top_5_accuracy: 0.9546 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "215/215 [==============================] - 53s 249ms/step - loss: 0.7342 - accuracy: 0.7503 - top_5_accuracy: 0.9457 - val_loss: 0.6053 - val_accuracy: 0.7858 - val_top_5_accuracy: 0.9623 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "215/215 [==============================] - 55s 257ms/step - loss: 0.7392 - accuracy: 0.7455 - top_5_accuracy: 0.9468 - val_loss: 0.5829 - val_accuracy: 0.7879 - val_top_5_accuracy: 0.9592 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "215/215 [==============================] - 57s 264ms/step - loss: 0.7350 - accuracy: 0.7449 - top_5_accuracy: 0.9442 - val_loss: 0.6642 - val_accuracy: 0.7537 - val_top_5_accuracy: 0.9551 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "215/215 [==============================] - 57s 266ms/step - loss: 0.7322 - accuracy: 0.7499 - top_5_accuracy: 0.9458 - val_loss: 0.5874 - val_accuracy: 0.7863 - val_top_5_accuracy: 0.9663 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "215/215 [==============================] - 58s 268ms/step - loss: 0.7305 - accuracy: 0.7490 - top_5_accuracy: 0.9458 - val_loss: 0.6269 - val_accuracy: 0.7721 - val_top_5_accuracy: 0.9551 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "215/215 [==============================] - 56s 259ms/step - loss: 0.7122 - accuracy: 0.7521 - top_5_accuracy: 0.9516 - val_loss: 0.5777 - val_accuracy: 0.7925 - val_top_5_accuracy: 0.9618 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "215/215 [==============================] - 56s 262ms/step - loss: 0.6986 - accuracy: 0.7576 - top_5_accuracy: 0.9476 - val_loss: 0.6115 - val_accuracy: 0.7726 - val_top_5_accuracy: 0.9587 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "215/215 [==============================] - 41s 189ms/step - loss: 0.6977 - accuracy: 0.7547 - top_5_accuracy: 0.9511 - val_loss: 0.5921 - val_accuracy: 0.7792 - val_top_5_accuracy: 0.9618 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "215/215 [==============================] - 42s 195ms/step - loss: 0.6909 - accuracy: 0.7586 - top_5_accuracy: 0.9508 - val_loss: 0.5864 - val_accuracy: 0.7970 - val_top_5_accuracy: 0.9567 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "215/215 [==============================] - 38s 178ms/step - loss: 0.6888 - accuracy: 0.7617 - top_5_accuracy: 0.9532 - val_loss: 0.6340 - val_accuracy: 0.7787 - val_top_5_accuracy: 0.9633 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.6899 - accuracy: 0.7585 - top_5_accuracy: 0.9555\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 38s 174ms/step - loss: 0.6905 - accuracy: 0.7580 - top_5_accuracy: 0.9554 - val_loss: 0.6129 - val_accuracy: 0.7700 - val_top_5_accuracy: 0.9567 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 0.6461 - accuracy: 0.7650 - top_5_accuracy: 0.9534 - val_loss: 0.5093 - val_accuracy: 0.8129 - val_top_5_accuracy: 0.9669 - lr: 5.0000e-04\n",
      "Epoch 32/50\n",
      "215/215 [==============================] - 39s 183ms/step - loss: 0.6189 - accuracy: 0.7738 - top_5_accuracy: 0.9573 - val_loss: 0.5345 - val_accuracy: 0.8011 - val_top_5_accuracy: 0.9628 - lr: 5.0000e-04\n",
      "Epoch 33/50\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 0.6186 - accuracy: 0.7738 - top_5_accuracy: 0.9585 - val_loss: 0.5331 - val_accuracy: 0.7889 - val_top_5_accuracy: 0.9679 - lr: 5.0000e-04\n",
      "Epoch 34/50\n",
      "215/215 [==============================] - 42s 194ms/step - loss: 0.5862 - accuracy: 0.7854 - top_5_accuracy: 0.9598 - val_loss: 0.5342 - val_accuracy: 0.7940 - val_top_5_accuracy: 0.9638 - lr: 5.0000e-04\n",
      "Epoch 35/50\n",
      "215/215 [==============================] - 39s 181ms/step - loss: 0.6150 - accuracy: 0.7739 - top_5_accuracy: 0.9608 - val_loss: 0.5191 - val_accuracy: 0.7965 - val_top_5_accuracy: 0.9669 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.5927 - accuracy: 0.7824 - top_5_accuracy: 0.9594\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 41s 190ms/step - loss: 0.5936 - accuracy: 0.7824 - top_5_accuracy: 0.9592 - val_loss: 0.5243 - val_accuracy: 0.7981 - val_top_5_accuracy: 0.9663 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "215/215 [==============================] - 36s 167ms/step - loss: 0.5767 - accuracy: 0.7800 - top_5_accuracy: 0.9611 - val_loss: 0.4917 - val_accuracy: 0.8103 - val_top_5_accuracy: 0.9720 - lr: 2.5000e-04\n",
      "Epoch 38/50\n",
      "215/215 [==============================] - 37s 171ms/step - loss: 0.5730 - accuracy: 0.7870 - top_5_accuracy: 0.9630 - val_loss: 0.5028 - val_accuracy: 0.8052 - val_top_5_accuracy: 0.9612 - lr: 2.5000e-04\n",
      "Epoch 39/50\n",
      "215/215 [==============================] - 39s 182ms/step - loss: 0.5782 - accuracy: 0.7851 - top_5_accuracy: 0.9611 - val_loss: 0.4719 - val_accuracy: 0.8108 - val_top_5_accuracy: 0.9674 - lr: 2.5000e-04\n",
      "Epoch 40/50\n",
      "215/215 [==============================] - 39s 178ms/step - loss: 0.5652 - accuracy: 0.7873 - top_5_accuracy: 0.9621 - val_loss: 0.4972 - val_accuracy: 0.8072 - val_top_5_accuracy: 0.9714 - lr: 2.5000e-04\n",
      "Epoch 41/50\n",
      "215/215 [==============================] - 38s 176ms/step - loss: 0.5575 - accuracy: 0.7883 - top_5_accuracy: 0.9634 - val_loss: 0.4698 - val_accuracy: 0.8134 - val_top_5_accuracy: 0.9658 - lr: 2.5000e-04\n",
      "Epoch 42/50\n",
      "215/215 [==============================] - 40s 187ms/step - loss: 0.5597 - accuracy: 0.7914 - top_5_accuracy: 0.9642 - val_loss: 0.4983 - val_accuracy: 0.8149 - val_top_5_accuracy: 0.9709 - lr: 2.5000e-04\n",
      "Epoch 43/50\n",
      "215/215 [==============================] - 38s 175ms/step - loss: 0.5315 - accuracy: 0.7988 - top_5_accuracy: 0.9615 - val_loss: 0.4852 - val_accuracy: 0.8108 - val_top_5_accuracy: 0.9709 - lr: 2.5000e-04\n",
      "Epoch 44/50\n",
      "215/215 [==============================] - 37s 173ms/step - loss: 0.5607 - accuracy: 0.7895 - top_5_accuracy: 0.9608 - val_loss: 0.4989 - val_accuracy: 0.8057 - val_top_5_accuracy: 0.9669 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 0.5501 - accuracy: 0.7918 - top_5_accuracy: 0.9617 - val_loss: 0.4735 - val_accuracy: 0.8210 - val_top_5_accuracy: 0.9689 - lr: 2.5000e-04\n",
      "Epoch 46/50\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.5433 - accuracy: 0.7982 - top_5_accuracy: 0.9607\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "215/215 [==============================] - 37s 173ms/step - loss: 0.5427 - accuracy: 0.7984 - top_5_accuracy: 0.9607 - val_loss: 0.4866 - val_accuracy: 0.8134 - val_top_5_accuracy: 0.9643 - lr: 2.5000e-04\n",
      "Epoch 47/50\n",
      "215/215 [==============================] - 40s 185ms/step - loss: 0.5309 - accuracy: 0.7945 - top_5_accuracy: 0.9636 - val_loss: 0.4457 - val_accuracy: 0.8261 - val_top_5_accuracy: 0.9730 - lr: 1.2500e-04\n",
      "Epoch 48/50\n",
      "215/215 [==============================] - 40s 184ms/step - loss: 0.5209 - accuracy: 0.7990 - top_5_accuracy: 0.9661 - val_loss: 0.4572 - val_accuracy: 0.8271 - val_top_5_accuracy: 0.9694 - lr: 1.2500e-04\n",
      "Epoch 49/50\n",
      "215/215 [==============================] - 38s 178ms/step - loss: 0.5213 - accuracy: 0.8007 - top_5_accuracy: 0.9680 - val_loss: 0.4655 - val_accuracy: 0.8159 - val_top_5_accuracy: 0.9684 - lr: 1.2500e-04\n",
      "Epoch 50/50\n",
      "215/215 [==============================] - 38s 175ms/step - loss: 0.5286 - accuracy: 0.7966 - top_5_accuracy: 0.9624 - val_loss: 0.4474 - val_accuracy: 0.8317 - val_top_5_accuracy: 0.9704 - lr: 1.2500e-04\n",
      "\n",
      "‚úì Training completed!\n",
      "‚è±Ô∏è  End time: 2025-12-07 11:49:50\n",
      "‚úì Model weights saved to: EfficientNetB0_output\\models\\model_weights.h5\n",
      "\n",
      "======================================================================\n",
      "EVALUATING EfficientNetB0\n",
      "======================================================================\n",
      "\n",
      "üìä Evaluating on test set...\n",
      "31/31 [==============================] - 3s 81ms/step - loss: 0.4307 - accuracy: 0.8228 - top_5_accuracy: 0.9776\n",
      "\n",
      "‚úì Test Results:\n",
      "  Loss: 0.4307\n",
      "  Accuracy: 0.8228\n",
      "  Top-5 Accuracy: 0.9776\n",
      "\n",
      "üìä Generating detailed metrics...\n",
      "‚úì Confusion matrix saved to: EfficientNetB0_output\\plots\\confusion_matrix.png\n",
      "‚úì Classification report saved to: EfficientNetB0_output\\logs\\classification_report.txt\n",
      "\n",
      "üìà Generating training plots...\n",
      "‚úì Training plots saved to: EfficientNetB0_output\\plots\\training_history.png\n",
      "\n",
      "\n",
      "üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂\n",
      "TRAINING RESNET-50\n",
      "üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂\n",
      "\n",
      "======================================================================\n",
      "DATA PREPARATION - ResNet50\n",
      "======================================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images\n",
      "‚úì Number of classes: 63\n",
      "‚úì Classes: ['Missing_hole', 'Missing_hole_rotation', 'Mouse_bite', 'Mouse_bite_rotation', 'Open_circuit', 'Open_circuit_rotation', 'PCB_USED', 'Short', 'Short_rotation', 'Spur']...\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1961\n",
      "‚úì Test samples: 982\n",
      "\n",
      "======================================================================\n",
      "BUILDING RESNET-50 MODEL\n",
      "======================================================================\n",
      "\n",
      "‚úì Model built successfully!\n",
      "‚úì Total parameters: 26,251,199\n",
      "‚úì Base model frozen: True\n",
      "\n",
      "======================================================================\n",
      "TRAINING ResNet50\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting training for 50 epochs...\n",
      "‚è±Ô∏è  Start time: 2025-12-07 11:50:06\n",
      "\n",
      "Epoch 1/50\n",
      "215/215 [==============================] - 67s 299ms/step - loss: 1.6148 - accuracy: 0.6172 - top_5_accuracy: 0.8092 - val_loss: 1.1694 - val_accuracy: 0.6731 - val_top_5_accuracy: 0.8766 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "215/215 [==============================] - 45s 205ms/step - loss: 1.2510 - accuracy: 0.6498 - top_5_accuracy: 0.8749 - val_loss: 1.0581 - val_accuracy: 0.6741 - val_top_5_accuracy: 0.9082 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "215/215 [==============================] - 49s 226ms/step - loss: 1.1060 - accuracy: 0.6696 - top_5_accuracy: 0.8928 - val_loss: 1.0093 - val_accuracy: 0.7047 - val_top_5_accuracy: 0.9179 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "215/215 [==============================] - 47s 217ms/step - loss: 1.0509 - accuracy: 0.6820 - top_5_accuracy: 0.9025 - val_loss: 1.0221 - val_accuracy: 0.6772 - val_top_5_accuracy: 0.9097 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "215/215 [==============================] - 48s 222ms/step - loss: 1.0371 - accuracy: 0.6814 - top_5_accuracy: 0.9069 - val_loss: 0.9260 - val_accuracy: 0.7114 - val_top_5_accuracy: 0.9240 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "215/215 [==============================] - 48s 222ms/step - loss: 0.9904 - accuracy: 0.6910 - top_5_accuracy: 0.9113 - val_loss: 0.9516 - val_accuracy: 0.6920 - val_top_5_accuracy: 0.9281 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "215/215 [==============================] - 48s 221ms/step - loss: 0.9656 - accuracy: 0.6983 - top_5_accuracy: 0.9165 - val_loss: 0.8486 - val_accuracy: 0.7134 - val_top_5_accuracy: 0.9357 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "215/215 [==============================] - 50s 230ms/step - loss: 0.9519 - accuracy: 0.6945 - top_5_accuracy: 0.9170 - val_loss: 0.9146 - val_accuracy: 0.6986 - val_top_5_accuracy: 0.9301 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "215/215 [==============================] - 48s 225ms/step - loss: 0.9588 - accuracy: 0.6972 - top_5_accuracy: 0.9259 - val_loss: 0.8306 - val_accuracy: 0.7134 - val_top_5_accuracy: 0.9444 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "215/215 [==============================] - 47s 219ms/step - loss: 0.9323 - accuracy: 0.6969 - top_5_accuracy: 0.9261 - val_loss: 0.7790 - val_accuracy: 0.7287 - val_top_5_accuracy: 0.9424 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "215/215 [==============================] - 46s 215ms/step - loss: 0.9313 - accuracy: 0.6996 - top_5_accuracy: 0.9210 - val_loss: 0.7571 - val_accuracy: 0.7425 - val_top_5_accuracy: 0.9470 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "215/215 [==============================] - 52s 242ms/step - loss: 0.9173 - accuracy: 0.7046 - top_5_accuracy: 0.9288 - val_loss: 0.8266 - val_accuracy: 0.7246 - val_top_5_accuracy: 0.9403 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "215/215 [==============================] - 53s 245ms/step - loss: 0.8884 - accuracy: 0.7088 - top_5_accuracy: 0.9312 - val_loss: 0.8151 - val_accuracy: 0.7277 - val_top_5_accuracy: 0.9322 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "215/215 [==============================] - 51s 234ms/step - loss: 0.8700 - accuracy: 0.7203 - top_5_accuracy: 0.9328 - val_loss: 0.7069 - val_accuracy: 0.7496 - val_top_5_accuracy: 0.9546 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "215/215 [==============================] - 51s 238ms/step - loss: 0.8766 - accuracy: 0.7116 - top_5_accuracy: 0.9282 - val_loss: 0.7157 - val_accuracy: 0.7455 - val_top_5_accuracy: 0.9561 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "215/215 [==============================] - 51s 237ms/step - loss: 0.8457 - accuracy: 0.7227 - top_5_accuracy: 0.9333 - val_loss: 0.7301 - val_accuracy: 0.7445 - val_top_5_accuracy: 0.9572 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "215/215 [==============================] - 51s 236ms/step - loss: 0.8661 - accuracy: 0.7135 - top_5_accuracy: 0.9343 - val_loss: 0.7556 - val_accuracy: 0.7445 - val_top_5_accuracy: 0.9475 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "215/215 [==============================] - 46s 213ms/step - loss: 0.8665 - accuracy: 0.7193 - top_5_accuracy: 0.9349 - val_loss: 0.7324 - val_accuracy: 0.7409 - val_top_5_accuracy: 0.9449 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.8269 - accuracy: 0.7246 - top_5_accuracy: 0.9388\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 49s 226ms/step - loss: 0.8258 - accuracy: 0.7248 - top_5_accuracy: 0.9390 - val_loss: 0.7767 - val_accuracy: 0.7292 - val_top_5_accuracy: 0.9454 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "215/215 [==============================] - 47s 220ms/step - loss: 0.7682 - accuracy: 0.7391 - top_5_accuracy: 0.9457 - val_loss: 0.6322 - val_accuracy: 0.7588 - val_top_5_accuracy: 0.9633 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "215/215 [==============================] - 45s 210ms/step - loss: 0.7512 - accuracy: 0.7359 - top_5_accuracy: 0.9492 - val_loss: 0.6264 - val_accuracy: 0.7680 - val_top_5_accuracy: 0.9612 - lr: 5.0000e-04\n",
      "Epoch 22/50\n",
      "215/215 [==============================] - 45s 209ms/step - loss: 0.7556 - accuracy: 0.7397 - top_5_accuracy: 0.9439 - val_loss: 0.6904 - val_accuracy: 0.7501 - val_top_5_accuracy: 0.9475 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "215/215 [==============================] - 46s 215ms/step - loss: 0.7408 - accuracy: 0.7358 - top_5_accuracy: 0.9458 - val_loss: 0.6217 - val_accuracy: 0.7726 - val_top_5_accuracy: 0.9582 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "215/215 [==============================] - 46s 216ms/step - loss: 0.7312 - accuracy: 0.7430 - top_5_accuracy: 0.9448 - val_loss: 0.6399 - val_accuracy: 0.7664 - val_top_5_accuracy: 0.9546 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "215/215 [==============================] - 47s 216ms/step - loss: 0.7228 - accuracy: 0.7499 - top_5_accuracy: 0.9503 - val_loss: 0.6139 - val_accuracy: 0.7782 - val_top_5_accuracy: 0.9623 - lr: 5.0000e-04\n",
      "Epoch 26/50\n",
      "215/215 [==============================] - 44s 206ms/step - loss: 0.7126 - accuracy: 0.7457 - top_5_accuracy: 0.9497 - val_loss: 0.6426 - val_accuracy: 0.7537 - val_top_5_accuracy: 0.9572 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "215/215 [==============================] - 47s 217ms/step - loss: 0.7168 - accuracy: 0.7490 - top_5_accuracy: 0.9470 - val_loss: 0.6332 - val_accuracy: 0.7659 - val_top_5_accuracy: 0.9536 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "215/215 [==============================] - 47s 219ms/step - loss: 0.7109 - accuracy: 0.7509 - top_5_accuracy: 0.9509 - val_loss: 0.6063 - val_accuracy: 0.7772 - val_top_5_accuracy: 0.9643 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "215/215 [==============================] - 45s 210ms/step - loss: 0.7080 - accuracy: 0.7452 - top_5_accuracy: 0.9515 - val_loss: 0.6168 - val_accuracy: 0.7736 - val_top_5_accuracy: 0.9648 - lr: 5.0000e-04\n",
      "Epoch 30/50\n",
      "215/215 [==============================] - 45s 210ms/step - loss: 0.6942 - accuracy: 0.7487 - top_5_accuracy: 0.9537 - val_loss: 0.6550 - val_accuracy: 0.7613 - val_top_5_accuracy: 0.9551 - lr: 5.0000e-04\n",
      "Epoch 31/50\n",
      "215/215 [==============================] - 47s 218ms/step - loss: 0.7133 - accuracy: 0.7484 - top_5_accuracy: 0.9495 - val_loss: 0.5607 - val_accuracy: 0.7960 - val_top_5_accuracy: 0.9633 - lr: 5.0000e-04\n",
      "Epoch 32/50\n",
      "215/215 [==============================] - 50s 234ms/step - loss: 0.7042 - accuracy: 0.7522 - top_5_accuracy: 0.9515 - val_loss: 0.6228 - val_accuracy: 0.7797 - val_top_5_accuracy: 0.9638 - lr: 5.0000e-04\n",
      "Epoch 33/50\n",
      "215/215 [==============================] - 50s 234ms/step - loss: 0.7047 - accuracy: 0.7493 - top_5_accuracy: 0.9548 - val_loss: 0.6098 - val_accuracy: 0.7670 - val_top_5_accuracy: 0.9679 - lr: 5.0000e-04\n",
      "Epoch 34/50\n",
      "215/215 [==============================] - 48s 221ms/step - loss: 0.6784 - accuracy: 0.7575 - top_5_accuracy: 0.9532 - val_loss: 0.6103 - val_accuracy: 0.7731 - val_top_5_accuracy: 0.9612 - lr: 5.0000e-04\n",
      "Epoch 35/50\n",
      "215/215 [==============================] - 44s 204ms/step - loss: 0.6963 - accuracy: 0.7562 - top_5_accuracy: 0.9547 - val_loss: 0.6075 - val_accuracy: 0.7721 - val_top_5_accuracy: 0.9607 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.7006 - accuracy: 0.7493 - top_5_accuracy: 0.9550\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 50s 232ms/step - loss: 0.7023 - accuracy: 0.7490 - top_5_accuracy: 0.9548 - val_loss: 0.6107 - val_accuracy: 0.7731 - val_top_5_accuracy: 0.9663 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "215/215 [==============================] - 50s 232ms/step - loss: 0.6566 - accuracy: 0.7534 - top_5_accuracy: 0.9613 - val_loss: 0.5624 - val_accuracy: 0.7858 - val_top_5_accuracy: 0.9689 - lr: 2.5000e-04\n",
      "Epoch 38/50\n",
      "215/215 [==============================] - 48s 223ms/step - loss: 0.6483 - accuracy: 0.7604 - top_5_accuracy: 0.9610 - val_loss: 0.5770 - val_accuracy: 0.7802 - val_top_5_accuracy: 0.9648 - lr: 2.5000e-04\n",
      "Epoch 39/50\n",
      "215/215 [==============================] - 49s 226ms/step - loss: 0.6406 - accuracy: 0.7645 - top_5_accuracy: 0.9566 - val_loss: 0.5367 - val_accuracy: 0.7889 - val_top_5_accuracy: 0.9628 - lr: 2.5000e-04\n",
      "Epoch 40/50\n",
      "215/215 [==============================] - 46s 215ms/step - loss: 0.6415 - accuracy: 0.7617 - top_5_accuracy: 0.9589 - val_loss: 0.5632 - val_accuracy: 0.7838 - val_top_5_accuracy: 0.9699 - lr: 2.5000e-04\n",
      "Epoch 41/50\n",
      "215/215 [==============================] - 48s 222ms/step - loss: 0.6370 - accuracy: 0.7647 - top_5_accuracy: 0.9566 - val_loss: 0.5423 - val_accuracy: 0.7868 - val_top_5_accuracy: 0.9658 - lr: 2.5000e-04\n",
      "Epoch 42/50\n",
      "215/215 [==============================] - 48s 224ms/step - loss: 0.6207 - accuracy: 0.7677 - top_5_accuracy: 0.9640 - val_loss: 0.5604 - val_accuracy: 0.7894 - val_top_5_accuracy: 0.9643 - lr: 2.5000e-04\n",
      "Epoch 43/50\n",
      "215/215 [==============================] - 45s 210ms/step - loss: 0.6055 - accuracy: 0.7749 - top_5_accuracy: 0.9597 - val_loss: 0.5631 - val_accuracy: 0.7838 - val_top_5_accuracy: 0.9684 - lr: 2.5000e-04\n",
      "Epoch 44/50\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.6205 - accuracy: 0.7702 - top_5_accuracy: 0.9603\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "215/215 [==============================] - 46s 212ms/step - loss: 0.6197 - accuracy: 0.7706 - top_5_accuracy: 0.9604 - val_loss: 0.5629 - val_accuracy: 0.7874 - val_top_5_accuracy: 0.9720 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "215/215 [==============================] - 47s 218ms/step - loss: 0.6057 - accuracy: 0.7733 - top_5_accuracy: 0.9630 - val_loss: 0.5229 - val_accuracy: 0.7976 - val_top_5_accuracy: 0.9714 - lr: 1.2500e-04\n",
      "Epoch 46/50\n",
      "215/215 [==============================] - 47s 220ms/step - loss: 0.5955 - accuracy: 0.7758 - top_5_accuracy: 0.9647 - val_loss: 0.5556 - val_accuracy: 0.7894 - val_top_5_accuracy: 0.9674 - lr: 1.2500e-04\n",
      "Epoch 47/50\n",
      "215/215 [==============================] - 48s 222ms/step - loss: 0.5851 - accuracy: 0.7773 - top_5_accuracy: 0.9642 - val_loss: 0.5022 - val_accuracy: 0.8032 - val_top_5_accuracy: 0.9684 - lr: 1.2500e-04\n",
      "Epoch 48/50\n",
      "215/215 [==============================] - 45s 209ms/step - loss: 0.5805 - accuracy: 0.7771 - top_5_accuracy: 0.9662 - val_loss: 0.5088 - val_accuracy: 0.8067 - val_top_5_accuracy: 0.9689 - lr: 1.2500e-04\n",
      "Epoch 49/50\n",
      "215/215 [==============================] - 47s 217ms/step - loss: 0.5903 - accuracy: 0.7798 - top_5_accuracy: 0.9614 - val_loss: 0.5267 - val_accuracy: 0.8001 - val_top_5_accuracy: 0.9714 - lr: 1.2500e-04\n",
      "Epoch 50/50\n",
      "215/215 [==============================] - 47s 219ms/step - loss: 0.6036 - accuracy: 0.7783 - top_5_accuracy: 0.9598 - val_loss: 0.5066 - val_accuracy: 0.7986 - val_top_5_accuracy: 0.9689 - lr: 1.2500e-04\n",
      "\n",
      "‚úì Training completed!\n",
      "‚è±Ô∏è  End time: 2025-12-07 12:30:09\n",
      "‚úì Model weights saved to: ResNet50_output\\models\\model_weights.h5\n",
      "\n",
      "======================================================================\n",
      "EVALUATING ResNet50\n",
      "======================================================================\n",
      "\n",
      "üìä Evaluating on test set...\n",
      "31/31 [==============================] - 4s 98ms/step - loss: 0.4799 - accuracy: 0.8167 - top_5_accuracy: 0.9705\n",
      "\n",
      "‚úì Test Results:\n",
      "  Loss: 0.4799\n",
      "  Accuracy: 0.8167\n",
      "  Top-5 Accuracy: 0.9705\n",
      "\n",
      "üìä Generating detailed metrics...\n",
      "‚úì Confusion matrix saved to: ResNet50_output\\plots\\confusion_matrix.png\n",
      "‚úì Classification report saved to: ResNet50_output\\logs\\classification_report.txt\n",
      "\n",
      "üìà Generating training plots...\n",
      "‚úì Training plots saved to: ResNet50_output\\plots\\training_history.png\n",
      "\n",
      "\n",
      "üîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπ\n",
      "TRAINING VISION TRANSFORMER\n",
      "üîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπ\n",
      "\n",
      "======================================================================\n",
      "MODEL COMPARISON\n",
      "======================================================================\n",
      "\n",
      "üìä Performance Comparison:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "EfficientNet:\n",
      "  Accuracy: 82.28%\n",
      "  Loss: 0.4307\n",
      "  Top-5 Acc: 97.76%\n",
      "\n",
      "ResNet50:\n",
      "  Accuracy: 81.67%\n",
      "  Loss: 0.4799\n",
      "  Top-5 Acc: 97.05%\n",
      "\n",
      "‚úì Comparison saved to: model_comparison.json\n",
      "\n",
      "======================================================================\n",
      "üéâ ALL MODELS TRAINED SUCCESSFULLY!\n",
      "======================================================================\n",
      "\n",
      "üìÅ Output directories:\n",
      "  - EfficientNetB0_output/\n",
      "  - ResNet50_output/\n",
      "\n",
      "‚úì Check model_comparison.json for detailed comparison\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "class DefectDetectionModel:\n",
    "    \"\"\"Base class for defect detection models\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, model_name, img_height=224, img_width=224):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.model_name = model_name\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.class_names = []\n",
    "        self.num_classes = 0\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir = Path(f\"{model_name}_output\")\n",
    "        self.model_dir = self.output_dir / \"models\"\n",
    "        self.plots_dir = self.output_dir / \"plots\"\n",
    "        self.logs_dir = self.output_dir / \"logs\"\n",
    "        \n",
    "        for dir_path in [self.output_dir, self.model_dir, self.plots_dir, self.logs_dir]:\n",
    "            dir_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def prepare_data(self, validation_split=0.2, test_split=0.1, batch_size=32):\n",
    "        \"\"\"Prepare dataset with efficient TensorFlow pipeline\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"DATA PREPARATION - {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Get all image paths and labels\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        print(\"\\nüìÅ Scanning dataset...\")\n",
    "        for class_dir in self.dataset_path.rglob(\"*\"):\n",
    "            if class_dir.is_dir():\n",
    "                images = (list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\")) + \n",
    "                         list(class_dir.glob(\"*.jpeg\")) + list(class_dir.glob(\"*.bmp\")))\n",
    "                \n",
    "                if images:\n",
    "                    class_name = class_dir.name\n",
    "                    if class_name not in self.class_names:\n",
    "                        self.class_names.append(class_name)\n",
    "                    \n",
    "                    for img_path in images:\n",
    "                        image_paths.append(str(img_path))\n",
    "                        labels.append(class_name)\n",
    "        \n",
    "        self.class_names.sort()\n",
    "        self.num_classes = len(self.class_names)\n",
    "        print(f\"‚úì Found {len(image_paths)} images\")\n",
    "        print(f\"‚úì Number of classes: {self.num_classes}\")\n",
    "        print(f\"‚úì Classes: {self.class_names[:10]}{'...' if len(self.class_names) > 10 else ''}\")\n",
    "        \n",
    "        # Create label mapping\n",
    "        label_to_index = {label: idx for idx, label in enumerate(self.class_names)}\n",
    "        labels_encoded = [label_to_index[label] for label in labels]\n",
    "        \n",
    "        # Create tf.data.Dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels_encoded))\n",
    "        dataset = dataset.shuffle(len(image_paths), seed=42)\n",
    "        \n",
    "        # Calculate splits\n",
    "        train_size = int(len(image_paths) * (1 - validation_split - test_split))\n",
    "        val_size = int(len(image_paths) * validation_split)\n",
    "        test_size = len(image_paths) - train_size - val_size\n",
    "        \n",
    "        # Split datasets\n",
    "        train_ds = dataset.take(train_size)\n",
    "        val_ds = dataset.skip(train_size).take(val_size)\n",
    "        test_ds = dataset.skip(train_size + val_size)\n",
    "        \n",
    "        # Process datasets\n",
    "        self.train_dataset = train_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=True),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.val_dataset = val_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=False),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.test_dataset = test_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=False),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        print(f\"\\n‚úì Train samples: {train_size}\")\n",
    "        print(f\"‚úì Validation samples: {val_size}\")\n",
    "        print(f\"‚úì Test samples: {test_size}\")\n",
    "        \n",
    "        return train_size, val_size, test_size\n",
    "    \n",
    "    def _load_and_preprocess(self, image_path, label, augment=False):\n",
    "        \"\"\"Load and preprocess image\"\"\"\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, [self.img_height, self.img_width])\n",
    "        \n",
    "        # Data augmentation\n",
    "        if augment:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_flip_up_down(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.rot90(img, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "        \n",
    "        img = tf.clip_by_value(img, 0.0, 255.0)\n",
    "        \n",
    "        # Normalize to [0, 1] for all models\n",
    "        img = img / 255.0\n",
    "        \n",
    "        label_onehot = tf.one_hot(label, self.num_classes)\n",
    "        return img, label_onehot\n",
    "    \n",
    "    def train(self, epochs=50, early_stopping_patience=10):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"TRAINING {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built. Call build_model() first.\")\n",
    "        \n",
    "        # Callbacks - SIMPLIFIED to avoid serialization issues\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=early_stopping_patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.CSVLogger(\n",
    "                str(self.logs_dir / 'training_log.csv')\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting training for {epochs} epochs...\")\n",
    "        print(f\"‚è±Ô∏è  Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            self.train_dataset,\n",
    "            epochs=epochs,\n",
    "            validation_data=self.val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úì Training completed!\")\n",
    "        print(f\"‚è±Ô∏è  End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Save model weights only (avoids serialization issues)\n",
    "        self.model.save_weights(str(self.model_dir / 'model_weights.h5'))\n",
    "        print(f\"‚úì Model weights saved to: {self.model_dir / 'model_weights.h5'}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"EVALUATING {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìä Evaluating on test set...\")\n",
    "        test_results = self.model.evaluate(self.test_dataset, verbose=1)\n",
    "        \n",
    "        print(\"\\n‚úì Test Results:\")\n",
    "        print(f\"  Loss: {test_results[0]:.4f}\")\n",
    "        print(f\"  Accuracy: {test_results[1]:.4f}\")\n",
    "        if len(test_results) > 2:\n",
    "            print(f\"  Top-5 Accuracy: {test_results[2]:.4f}\")\n",
    "        \n",
    "        # Get predictions for confusion matrix\n",
    "        print(\"\\nüìä Generating detailed metrics...\")\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for images, labels in self.test_dataset:\n",
    "            predictions = self.model.predict(images, verbose=0)\n",
    "            y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'test_loss': float(test_results[0]),\n",
    "            'test_accuracy': float(test_results[1]),\n",
    "            'num_classes': self.num_classes,\n",
    "            'evaluation_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        if len(test_results) > 2:\n",
    "            results['test_top5_accuracy'] = float(test_results[2])\n",
    "        \n",
    "        with open(self.logs_dir / 'test_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        self._plot_confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Generate classification report\n",
    "        self._save_classification_report(y_true, y_pred)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _plot_confusion_matrix(self, y_true, y_pred):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(max(10, self.num_classes), max(8, self.num_classes * 0.8)))\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        sns.heatmap(cm_normalized, annot=False, cmap='Blues', \n",
    "                    xticklabels=self.class_names if self.num_classes <= 20 else False,\n",
    "                    yticklabels=self.class_names if self.num_classes <= 20 else False)\n",
    "        \n",
    "        plt.title(f'{self.model_name} - Confusion Matrix (Normalized)')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Confusion matrix saved to: {self.plots_dir / 'confusion_matrix.png'}\")\n",
    "    \n",
    "    def _save_classification_report(self, y_true, y_pred):\n",
    "        \"\"\"Save classification report\"\"\"\n",
    "        report = classification_report(y_true, y_pred, target_names=self.class_names, \n",
    "                                      zero_division=0)\n",
    "        \n",
    "        with open(self.logs_dir / 'classification_report.txt', 'w') as f:\n",
    "            f.write(f\"{self.model_name} - Classification Report\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"‚úì Classification report saved to: {self.logs_dir / 'classification_report.txt'}\")\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"‚ö†Ô∏è  No training history available\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüìà Generating training plots...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        axes[0].plot(self.history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "        axes[0].plot(self.history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "        axes[0].set_title(f'{self.model_name} - Model Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[1].plot(self.history.history['loss'], label='Train Loss', linewidth=2)\n",
    "        axes[1].plot(self.history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "        axes[1].set_title(f'{self.model_name} - Model Loss', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Training plots saved to: {self.plots_dir / 'training_history.png'}\")\n",
    "\n",
    "\n",
    "class EfficientNetDefectDetector(DefectDetectionModel):\n",
    "    \"\"\"EfficientNet-B0 for defect detection\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, img_height=224, img_width=224):\n",
    "        super().__init__(dataset_path, \"EfficientNetB0\", img_height, img_width)\n",
    "    \n",
    "    def build_model(self, learning_rate=0.001, dropout_rate=0.3):\n",
    "        \"\"\"Build EfficientNet model with transfer learning\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"BUILDING EFFICIENTNET MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load pre-trained EfficientNet (expects [0, 1] normalized images)\n",
    "        base_model = EfficientNetB0(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(self.img_height, self.img_width, 3)\n",
    "        )\n",
    "        \n",
    "        # Freeze base model initially\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Build model\n",
    "        inputs = keras.Input(shape=(self.img_height, self.img_width, 3))\n",
    "        \n",
    "        # Scale from [0,1] to [0,255] for EfficientNet\n",
    "        x = layers.Rescaling(scale=255.0)(inputs)\n",
    "        x = base_model(x, training=False)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate / 2)(x)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(\"\\n‚úì Model built successfully!\")\n",
    "        print(f\"‚úì Total parameters: {model.count_params():,}\")\n",
    "        print(f\"‚úì Base model frozen: {not base_model.trainable}\")\n",
    "        \n",
    "        # Save architecture\n",
    "        with open(self.model_dir / 'model_architecture.txt', 'w') as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "class ResNet50DefectDetector(DefectDetectionModel):\n",
    "    \"\"\"ResNet-50 for defect detection\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, img_height=224, img_width=224):\n",
    "        super().__init__(dataset_path, \"ResNet50\", img_height, img_width)\n",
    "    \n",
    "    def build_model(self, learning_rate=0.001, dropout_rate=0.3):\n",
    "        \"\"\"Build ResNet-50 model with transfer learning\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"BUILDING RESNET-50 MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load pre-trained ResNet50 with preprocessing included\n",
    "        base_model = ResNet50(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(self.img_height, self.img_width, 3)\n",
    "        )\n",
    "        \n",
    "        # Freeze base model initially\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Build model - Input is [0, 1], ResNet preprocessing converts to [-1, 1]\n",
    "        inputs = keras.Input(shape=(self.img_height, self.img_width, 3))\n",
    "        \n",
    "        # Use Keras preprocessing for ResNet (handles normalization)\n",
    "        x = tf.keras.applications.resnet.preprocess_input(inputs * 255.0)\n",
    "        x = base_model(x, training=False)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(1024, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate / 2)(x)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(\"\\n‚úì Model built successfully!\")\n",
    "        print(f\"‚úì Total parameters: {model.count_params():,}\")\n",
    "        print(f\"‚úì Base model frozen: {not base_model.trainable}\")\n",
    "        \n",
    "        # Save architecture\n",
    "        with open(self.model_dir / 'model_architecture.txt', 'w') as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compare_models(results_dict):\n",
    "    \"\"\"Compare results from different models\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    comparison_data = []\n",
    "    for model_name, results in results_dict.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Test Accuracy': f\"{results['test_accuracy']*100:.2f}%\",\n",
    "            'Test Loss': f\"{results['test_loss']:.4f}\",\n",
    "            'Top-5 Accuracy': f\"{results.get('test_top5_accuracy', 0)*100:.2f}%\" if 'test_top5_accuracy' in results else 'N/A'\n",
    "        })\n",
    "    \n",
    "    print(\"\\nüìä Performance Comparison:\")\n",
    "    print(\"-\" * 70)\n",
    "    for data in comparison_data:\n",
    "        print(f\"\\n{data['Model']}:\")\n",
    "        print(f\"  Accuracy: {data['Test Accuracy']}\")\n",
    "        print(f\"  Loss: {data['Test Loss']}\")\n",
    "        print(f\"  Top-5 Acc: {data['Top-5 Accuracy']}\")\n",
    "    \n",
    "    # Save comparison\n",
    "    with open('model_comparison.json', 'w') as f:\n",
    "        json.dump(comparison_data, f, indent=2)\n",
    "    \n",
    "    print(\"\\n‚úì Comparison saved to: model_comparison.json\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ADVANCED DEFECT DETECTION PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    DATASET_PATH = \"final_merged_dataset\"\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. EfficientNet\n",
    "    print(\"\\n\\n\" + \"üî∑\"*35)\n",
    "    print(\"TRAINING EFFICIENTNET\")\n",
    "    print(\"üî∑\"*35)\n",
    "    \n",
    "    efficientnet = EfficientNetDefectDetector(DATASET_PATH)\n",
    "    efficientnet.prepare_data(batch_size=BATCH_SIZE)\n",
    "    efficientnet.build_model(learning_rate=0.001)\n",
    "    efficientnet.train(epochs=EPOCHS)\n",
    "    results['EfficientNet'] = efficientnet.evaluate()\n",
    "    efficientnet.plot_training_history()\n",
    "    \n",
    "    # 2. ResNet-50\n",
    "    print(\"\\n\\n\" + \"üî∂\"*35)\n",
    "    print(\"TRAINING RESNET-50\")\n",
    "    print(\"üî∂\"*35)\n",
    "    \n",
    "    resnet = ResNet50DefectDetector(DATASET_PATH)\n",
    "    resnet.prepare_data(batch_size=BATCH_SIZE)\n",
    "    resnet.build_model(learning_rate=0.001)\n",
    "    resnet.train(epochs=EPOCHS)\n",
    "    results['ResNet50'] = resnet.evaluate()\n",
    "    resnet.plot_training_history()\n",
    "    \n",
    "    # 3. Vision Transformer\n",
    "    print(\"\\n\\n\" + \"üîπ\"*35)\n",
    "    print(\"TRAINING VISION TRANSFORMER\")\n",
    "    print(\"üîπ\"*35)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Compare all models\n",
    "    compare_models(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ ALL MODELS TRAINED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nüìÅ Output directories:\")\n",
    "    print(\"  - EfficientNetB0_output/\")\n",
    "    print(\"  - ResNet50_output/\")\n",
    "    \n",
    "    print(\"\\n‚úì Check model_comparison.json for detailed comparison\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ddc1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: VISION TRANSFORMER TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "class VisionTransformerDefectDetector:\n",
    "    \"\"\"Vision Transformer (ViT) for defect detection\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, img_height=224, img_width=224, patch_size=16):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.model_name = \"VisionTransformer\"\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_height // patch_size) * (img_width // patch_size)\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.class_names = []\n",
    "        self.num_classes = 0\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir = Path(\"VisionTransformer_output\")\n",
    "        self.model_dir = self.output_dir / \"models\"\n",
    "        self.plots_dir = self.output_dir / \"plots\"\n",
    "        self.logs_dir = self.output_dir / \"logs\"\n",
    "        \n",
    "        for dir_path in [self.output_dir, self.model_dir, self.plots_dir, self.logs_dir]:\n",
    "            dir_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def prepare_data(self, validation_split=0.2, test_split=0.1, batch_size=32):\n",
    "        \"\"\"Prepare dataset\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"DATA PREPARATION - {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Get all image paths and labels\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        print(\"\\nüìÅ Scanning dataset...\")\n",
    "        for class_dir in self.dataset_path.rglob(\"*\"):\n",
    "            if class_dir.is_dir():\n",
    "                images = (list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\")) + \n",
    "                         list(class_dir.glob(\"*.jpeg\")) + list(class_dir.glob(\"*.bmp\")))\n",
    "                \n",
    "                if images:\n",
    "                    class_name = class_dir.name\n",
    "                    if class_name not in self.class_names:\n",
    "                        self.class_names.append(class_name)\n",
    "                    \n",
    "                    for img_path in images:\n",
    "                        image_paths.append(str(img_path))\n",
    "                        labels.append(class_name)\n",
    "        \n",
    "        self.class_names.sort()\n",
    "        self.num_classes = len(self.class_names)\n",
    "        print(f\"‚úì Found {len(image_paths)} images\")\n",
    "        print(f\"‚úì Number of classes: {self.num_classes}\")\n",
    "        \n",
    "        # Create label mapping\n",
    "        label_to_index = {label: idx for idx, label in enumerate(self.class_names)}\n",
    "        labels_encoded = [label_to_index[label] for label in labels]\n",
    "        \n",
    "        # Create tf.data.Dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels_encoded))\n",
    "        dataset = dataset.shuffle(len(image_paths), seed=42)\n",
    "        \n",
    "        # Calculate splits\n",
    "        train_size = int(len(image_paths) * (1 - validation_split - test_split))\n",
    "        val_size = int(len(image_paths) * validation_split)\n",
    "        test_size = len(image_paths) - train_size - val_size\n",
    "        \n",
    "        # Split datasets\n",
    "        train_ds = dataset.take(train_size)\n",
    "        val_ds = dataset.skip(train_size).take(val_size)\n",
    "        test_ds = dataset.skip(train_size + val_size)\n",
    "        \n",
    "        # Process datasets\n",
    "        self.train_dataset = train_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=True),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.val_dataset = val_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=False),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.test_dataset = test_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=False),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        print(f\"\\n‚úì Train samples: {train_size}\")\n",
    "        print(f\"‚úì Validation samples: {val_size}\")\n",
    "        print(f\"‚úì Test samples: {test_size}\")\n",
    "        \n",
    "        return train_size, val_size, test_size\n",
    "    \n",
    "    def _load_and_preprocess(self, image_path, label, augment=False):\n",
    "        \"\"\"Load and preprocess image\"\"\"\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, [self.img_height, self.img_width])\n",
    "        \n",
    "        # Data augmentation\n",
    "        if augment:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_flip_up_down(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.rot90(img, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "        \n",
    "        img = tf.clip_by_value(img, 0.0, 255.0)\n",
    "        img = img / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        label_onehot = tf.one_hot(label, self.num_classes)\n",
    "        return img, label_onehot\n",
    "    \n",
    "    def build_model(self, transformer_layers=8, num_heads=8, mlp_dim=2048, \n",
    "                    dropout_rate=0.1, learning_rate=0.001):\n",
    "        \"\"\"Build Vision Transformer model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"BUILDING VISION TRANSFORMER MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        inputs = keras.Input(shape=(self.img_height, self.img_width, 3))\n",
    "        \n",
    "        # Create patches\n",
    "        patches = self._create_patches(inputs)\n",
    "        \n",
    "        # Patch embedding\n",
    "        projection_dim = 768\n",
    "        patch_embeddings = layers.Dense(projection_dim)(patches)\n",
    "        \n",
    "        # Position embedding\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        position_embeddings = layers.Embedding(\n",
    "            input_dim=self.num_patches, output_dim=projection_dim\n",
    "        )(positions)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        encoded = patch_embeddings + position_embeddings\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for _ in range(transformer_layers):\n",
    "            # Layer normalization 1\n",
    "            x1 = layers.LayerNormalization(epsilon=1e-6)(encoded)\n",
    "            \n",
    "            # Multi-head attention\n",
    "            attention_output = layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, key_dim=projection_dim // num_heads, dropout=dropout_rate\n",
    "            )(x1, x1)\n",
    "            \n",
    "            # Skip connection 1\n",
    "            x2 = layers.Add()([attention_output, encoded])\n",
    "            \n",
    "            # Layer normalization 2\n",
    "            x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "            \n",
    "            # MLP\n",
    "            x3 = layers.Dense(mlp_dim, activation='gelu')(x3)\n",
    "            x3 = layers.Dropout(dropout_rate)(x3)\n",
    "            x3 = layers.Dense(projection_dim)(x3)\n",
    "            x3 = layers.Dropout(dropout_rate)(x3)\n",
    "            \n",
    "            # Skip connection 2\n",
    "            encoded = layers.Add()([x3, x2])\n",
    "        \n",
    "        # Final layer normalization\n",
    "        representation = layers.LayerNormalization(epsilon=1e-6)(encoded)\n",
    "        \n",
    "        # Global average pooling\n",
    "        representation = layers.GlobalAveragePooling1D()(representation)\n",
    "        \n",
    "        # Classification head\n",
    "        representation = layers.Dropout(dropout_rate)(representation)\n",
    "        features = layers.Dense(mlp_dim, activation='gelu')(representation)\n",
    "        features = layers.Dropout(dropout_rate)(features)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(features)\n",
    "        \n",
    "        # Create model\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        # Compile - FIXED: Use Adam instead of AdamW\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(\"\\n‚úì Vision Transformer built successfully!\")\n",
    "        print(f\"‚úì Total parameters: {model.count_params():,}\")\n",
    "        print(f\"‚úì Number of patches: {self.num_patches}\")\n",
    "        print(f\"‚úì Transformer layers: {transformer_layers}\")\n",
    "        print(f\"‚úì Attention heads: {num_heads}\")\n",
    "        \n",
    "        # Save architecture\n",
    "        with open(self.model_dir / 'model_architecture.txt', 'w') as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _create_patches(self, images):\n",
    "        \"\"\"Extract patches from images\"\"\"\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches\n",
    "    \n",
    "    def train(self, epochs=50, early_stopping_patience=10):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"TRAINING {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built. Call build_model() first.\")\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=early_stopping_patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.CSVLogger(\n",
    "                str(self.logs_dir / 'training_log.csv')\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting training for {epochs} epochs...\")\n",
    "        print(f\"‚è±Ô∏è  Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            self.train_dataset,\n",
    "            epochs=epochs,\n",
    "            validation_data=self.val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úì Training completed!\")\n",
    "        print(f\"‚è±Ô∏è  End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Save model weights\n",
    "        self.model.save_weights(str(self.model_dir / 'model_weights.h5'))\n",
    "        print(f\"‚úì Model weights saved to: {self.model_dir / 'model_weights.h5'}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"EVALUATING {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìä Evaluating on test set...\")\n",
    "        test_results = self.model.evaluate(self.test_dataset, verbose=1)\n",
    "        \n",
    "        print(\"\\n‚úì Test Results:\")\n",
    "        print(f\"  Loss: {test_results[0]:.4f}\")\n",
    "        print(f\"  Accuracy: {test_results[1]:.4f}\")\n",
    "        print(f\"  Top-5 Accuracy: {test_results[2]:.4f}\")\n",
    "        \n",
    "        # Get predictions for confusion matrix\n",
    "        print(\"\\nüìä Generating detailed metrics...\")\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for images, labels in self.test_dataset:\n",
    "            predictions = self.model.predict(images, verbose=0)\n",
    "            y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'test_loss': float(test_results[0]),\n",
    "            'test_accuracy': float(test_results[1]),\n",
    "            'test_top5_accuracy': float(test_results[2]),\n",
    "            'num_classes': self.num_classes,\n",
    "            'evaluation_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(self.logs_dir / 'test_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        self._plot_confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Generate classification report\n",
    "        self._save_classification_report(y_true, y_pred)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _plot_confusion_matrix(self, y_true, y_pred):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(max(10, self.num_classes), max(8, self.num_classes * 0.8)))\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        sns.heatmap(cm_normalized, annot=False, cmap='Blues', \n",
    "                    xticklabels=self.class_names if self.num_classes <= 20 else False,\n",
    "                    yticklabels=self.class_names if self.num_classes <= 20 else False)\n",
    "        \n",
    "        plt.title(f'{self.model_name} - Confusion Matrix (Normalized)')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Confusion matrix saved to: {self.plots_dir / 'confusion_matrix.png'}\")\n",
    "    \n",
    "    def _save_classification_report(self, y_true, y_pred):\n",
    "        \"\"\"Save classification report\"\"\"\n",
    "        report = classification_report(y_true, y_pred, target_names=self.class_names, \n",
    "                                      zero_division=0)\n",
    "        \n",
    "        with open(self.logs_dir / 'classification_report.txt', 'w') as f:\n",
    "            f.write(f\"{self.model_name} - Classification Report\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"‚úì Classification report saved to: {self.logs_dir / 'classification_report.txt'}\")\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"‚ö†Ô∏è  No training history available\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüìà Generating training plots...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        axes[0].plot(self.history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "        axes[0].plot(self.history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "        axes[0].set_title(f'{self.model_name} - Model Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[1].plot(self.history.history['loss'], label='Train Loss', linewidth=2)\n",
    "        axes[1].plot(self.history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "        axes[1].set_title(f'{self.model_name} - Model Loss', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Training plots saved to: {self.plots_dir / 'training_history.png'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64746bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComparison:\n",
    "    \"\"\"Compare multiple trained models\"\"\"\n",
    "    \n",
    "    def __init__(self, model_dirs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_dirs: List of model output directories\n",
    "        \"\"\"\n",
    "        self.model_dirs = [Path(d) for d in model_dirs]\n",
    "        self.results = {}\n",
    "        self.load_results()\n",
    "    \n",
    "    def load_results(self):\n",
    "        \"\"\"Load results from all models\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LOADING MODEL RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for model_dir in self.model_dirs:\n",
    "            results_file = model_dir / \"logs\" / \"test_results.json\"\n",
    "            \n",
    "            if results_file.exists():\n",
    "                with open(results_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    model_name = model_dir.name.replace(\"_output\", \"\")\n",
    "                    self.results[model_name] = data\n",
    "                    print(f\"‚úì Loaded results for {model_name}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Results not found for {model_dir}\")\n",
    "    \n",
    "    def generate_comparison_table(self):\n",
    "        \"\"\"Generate comparison table\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        comparison_data = []\n",
    "        \n",
    "        for model_name, results in self.results.items():\n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'Test Accuracy (%)': f\"{results['test_accuracy']*100:.2f}\",\n",
    "                'Test Loss': f\"{results['test_loss']:.4f}\",\n",
    "                'Top-5 Accuracy (%)': f\"{results.get('test_top5_accuracy', 0)*100:.2f}\",\n",
    "                'Number of Classes': results['num_classes']\n",
    "            })\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Sort by accuracy\n",
    "        df['Accuracy_Numeric'] = df['Test Accuracy (%)'].astype(float)\n",
    "        df = df.sort_values('Accuracy_Numeric', ascending=False)\n",
    "        df = df.drop('Accuracy_Numeric', axis=1)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(df.to_string(index=False))\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Save to CSV\n",
    "        df.to_csv('model_comparison.csv', index=False)\n",
    "        print(f\"\\n‚úì Comparison table saved to: model_comparison.csv\")\n",
    "        \n",
    "        # Save to JSON\n",
    "        with open('model_comparison.json', 'w') as f:\n",
    "            json.dump(comparison_data, f, indent=2)\n",
    "        print(f\"‚úì Comparison data saved to: model_comparison.json\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def plot_comparison_charts(self):\n",
    "        \"\"\"Generate comparison visualizations\"\"\"\n",
    "        print(\"\\nüìä Generating comparison charts...\")\n",
    "        \n",
    "        if len(self.results) == 0:\n",
    "            print(\"‚ö†Ô∏è  No results to compare\")\n",
    "            return\n",
    "        \n",
    "        models = list(self.results.keys())\n",
    "        accuracies = [self.results[m]['test_accuracy'] * 100 for m in models]\n",
    "        losses = [self.results[m]['test_loss'] for m in models]\n",
    "        top5_accs = [self.results[m].get('test_top5_accuracy', 0) * 100 for m in models]\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # Color scheme\n",
    "        colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "        \n",
    "        # 1. Test Accuracy Comparison\n",
    "        bars1 = axes[0].bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "        axes[0].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "        axes[0].set_ylim([0, 100])\n",
    "        axes[0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars1, accuracies):\n",
    "            height = bar.get_height()\n",
    "            axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 2. Test Loss Comparison\n",
    "        bars2 = axes[1].bar(models, losses, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "        axes[1].set_title('Test Loss Comparison', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_ylabel('Loss', fontsize=12)\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, loss in zip(bars2, losses):\n",
    "            height = bar.get_height()\n",
    "            axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{loss:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 3. Top-5 Accuracy Comparison\n",
    "        bars3 = axes[2].bar(models, top5_accs, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "        axes[2].set_title('Top-5 Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "        axes[2].set_ylabel('Top-5 Accuracy (%)', fontsize=12)\n",
    "        axes[2].set_ylim([0, 100])\n",
    "        axes[2].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars3, top5_accs):\n",
    "            height = bar.get_height()\n",
    "            axes[2].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('model_comparison_charts.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Comparison charts saved to: model_comparison_charts.png\")\n",
    "    \n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"Generate comprehensive summary report\"\"\"\n",
    "        print(\"\\nüìù Generating summary report...\")\n",
    "        \n",
    "        report_file = 'model_comparison_report.txt'\n",
    "        \n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"DEFECT DETECTION MODEL COMPARISON REPORT\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            # Individual model results\n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            f.write(\"INDIVIDUAL MODEL RESULTS\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            for model_name, results in self.results.items():\n",
    "                f.write(f\"{model_name}:\\n\")\n",
    "                f.write(f\"  Test Accuracy: {results['test_accuracy']*100:.2f}%\\n\")\n",
    "                f.write(f\"  Test Loss: {results['test_loss']:.4f}\\n\")\n",
    "                f.write(f\"  Top-5 Accuracy: {results.get('test_top5_accuracy', 0)*100:.2f}%\\n\")\n",
    "                f.write(f\"  Number of Classes: {results['num_classes']}\\n\\n\")\n",
    "            \n",
    "            # Best model\n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            f.write(\"BEST PERFORMING MODEL\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            best_model = max(self.results.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "            f.write(f\"üèÜ Winner: {best_model[0]}\\n\")\n",
    "            f.write(f\"   Accuracy: {best_model[1]['test_accuracy']*100:.2f}%\\n\")\n",
    "            f.write(f\"   Loss: {best_model[1]['test_loss']:.4f}\\n\\n\")\n",
    "            \n",
    "            # Recommendations\n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            f.write(\"RECOMMENDATIONS\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            accuracies = {name: results['test_accuracy'] for name, results in self.results.items()}\n",
    "            \n",
    "            if max(accuracies.values()) < 0.80:\n",
    "                f.write(\"‚ö†Ô∏è  All models show accuracy < 80%\\n\")\n",
    "                f.write(\"   Consider:\\n\")\n",
    "                f.write(\"   - Training for more epochs\\n\")\n",
    "                f.write(\"   - Fine-tuning pre-trained layers\\n\")\n",
    "                f.write(\"   - Increasing model complexity\\n\")\n",
    "                f.write(\"   - Data augmentation strategies\\n\\n\")\n",
    "            \n",
    "            if max(accuracies.values()) >= 0.80:\n",
    "                f.write(\"‚úì Good performance achieved!\\n\")\n",
    "                f.write(f\"  Best model ({best_model[0]}) is recommended for deployment.\\n\\n\")\n",
    "            \n",
    "            # Model strengths\n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            f.write(\"MODEL STRENGTHS\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            if 'EfficientNetB0' in self.results:\n",
    "                f.write(\"EfficientNetB0:\\n\")\n",
    "                f.write(\"  + Fast inference speed\\n\")\n",
    "                f.write(\"  + Good balance of accuracy and efficiency\\n\")\n",
    "                f.write(\"  + Suitable for production deployment\\n\\n\")\n",
    "            \n",
    "            if 'ResNet50' in self.results:\n",
    "                f.write(\"ResNet50:\\n\")\n",
    "                f.write(\"  + Robust and well-tested architecture\\n\")\n",
    "                f.write(\"  + Good feature extraction\\n\")\n",
    "                f.write(\"  + Reliable baseline performance\\n\\n\")\n",
    "            \n",
    "            if 'VisionTransformer' in self.results:\n",
    "                f.write(\"VisionTransformer:\\n\")\n",
    "                f.write(\"  + State-of-the-art architecture\\n\")\n",
    "                f.write(\"  + Captures global context well\\n\")\n",
    "                f.write(\"  + Best for large datasets\\n\\n\")\n",
    "            \n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        print(f\"‚úì Summary report saved to: {report_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc626770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage:\n",
      "  python script.py train_vit   - Train Vision Transformer only\n",
      "  python script.py compare     - Compare all trained models\n",
      "  python script.py             - Train ViT and compare all\n"
     ]
    }
   ],
   "source": [
    "def train_vision_transformer():\n",
    "    \"\"\"Train Vision Transformer model\"\"\"\n",
    "    print(\"\\n\\n\" + \"üîπ\"*35)\n",
    "    print(\"TRAINING VISION TRANSFORMER\")\n",
    "    print(\"üîπ\"*35)\n",
    "    \n",
    "    DATASET_PATH = \"final_merged_dataset\"\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    vit = VisionTransformerDefectDetector(DATASET_PATH)\n",
    "    vit.prepare_data(batch_size=BATCH_SIZE)\n",
    "    vit.build_model(transformer_layers=8, num_heads=8, learning_rate=0.001)\n",
    "    vit.train(epochs=EPOCHS)\n",
    "    results = vit.evaluate()\n",
    "    vit.plot_training_history()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_all_models():\n",
    "    \"\"\"Compare all trained models\"\"\"\n",
    "    print(\"\\n\\n\" + \"üìä\"*35)\n",
    "    print(\"COMPARING ALL MODELS\")\n",
    "    print(\"üìä\"*35)\n",
    "    \n",
    "    # Model directories\n",
    "    model_dirs = [\n",
    "        \"EfficientNetB0_output\",\n",
    "        \"ResNet50_output\",\n",
    "        \"VisionTransformer_output\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize comparison\n",
    "    comparison = ModelComparison(model_dirs)\n",
    "    \n",
    "    # Generate comparison table\n",
    "    df = comparison.generate_comparison_table()\n",
    "    \n",
    "    # Generate comparison charts\n",
    "    comparison.plot_comparison_charts()\n",
    "    \n",
    "    # Generate summary report\n",
    "    comparison.generate_summary_report()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ COMPARISON COMPLETED!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nüìÅ Output files:\")\n",
    "    print(\"  - model_comparison.csv\")\n",
    "    print(\"  - model_comparison.json\")\n",
    "    print(\"  - model_comparison_charts.png\")\n",
    "    print(\"  - model_comparison_report.txt\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    import sys\n",
    "    \n",
    "    if len(sys.argv) > 1:\n",
    "        if sys.argv[1] == \"train_vit\":\n",
    "            # Only train Vision Transformer\n",
    "            train_vision_transformer()\n",
    "        elif sys.argv[1] == \"compare\":\n",
    "            # Only do comparison\n",
    "            compare_all_models()\n",
    "        else:\n",
    "            print(\"Usage:\")\n",
    "            print(\"  python script.py train_vit   - Train Vision Transformer only\")\n",
    "            print(\"  python script.py compare     - Compare all trained models\")\n",
    "            print(\"  python script.py             - Train ViT and compare all\")\n",
    "    else:\n",
    "        # Default: Train ViT and then compare all models\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPLETE PIPELINE: TRAIN VIT + COMPARE ALL MODELS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Train Vision Transformer\n",
    "        train_vision_transformer()\n",
    "        \n",
    "        # Compare all models\n",
    "        compare_all_models()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üéâ ALL TASKS COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eddef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπ\n",
      "TRAINING VISION TRANSFORMER\n",
      "üîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπ\n",
      "\n",
      "======================================================================\n",
      "DATA PREPARATION - VisionTransformer\n",
      "======================================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images\n",
      "‚úì Number of classes: 63\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1961\n",
      "‚úì Test samples: 982\n",
      "\n",
      "======================================================================\n",
      "BUILDING VISION TRANSFORMER MODEL\n",
      "======================================================================\n",
      "\n",
      "‚úì Vision Transformer built successfully!\n",
      "‚úì Total parameters: 46,407,999\n",
      "‚úì Number of patches: 196\n",
      "‚úì Transformer layers: 8\n",
      "‚úì Attention heads: 8\n",
      "\n",
      "======================================================================\n",
      "TRAINING VisionTransformer\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting training for 50 epochs...\n",
      "‚è±Ô∏è  Start time: 2025-11-14 19:08:57\n",
      "\n",
      "Epoch 1/50\n",
      "215/215 [==============================] - 90s 400ms/step - loss: 2.4519 - accuracy: 0.4111 - top_5_accuracy: 0.6736 - val_loss: 2.0448 - val_accuracy: 0.4595 - val_top_5_accuracy: 0.7251 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "215/215 [==============================] - 86s 399ms/step - loss: 1.9515 - accuracy: 0.5052 - top_5_accuracy: 0.7409 - val_loss: 1.7426 - val_accuracy: 0.5502 - val_top_5_accuracy: 0.7680 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "215/215 [==============================] - 86s 399ms/step - loss: 1.8032 - accuracy: 0.5448 - top_5_accuracy: 0.7570 - val_loss: 1.6960 - val_accuracy: 0.5660 - val_top_5_accuracy: 0.7746 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "215/215 [==============================] - 86s 399ms/step - loss: 1.7955 - accuracy: 0.5336 - top_5_accuracy: 0.7647 - val_loss: 1.7940 - val_accuracy: 0.5278 - val_top_5_accuracy: 0.7629 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "215/215 [==============================] - 86s 400ms/step - loss: 1.7467 - accuracy: 0.5559 - top_5_accuracy: 0.7646 - val_loss: 1.9694 - val_accuracy: 0.5293 - val_top_5_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "215/215 [==============================] - 86s 400ms/step - loss: 1.8206 - accuracy: 0.5369 - top_5_accuracy: 0.7583 - val_loss: 1.9824 - val_accuracy: 0.4788 - val_top_5_accuracy: 0.7221 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "215/215 [==============================] - 86s 398ms/step - loss: 1.7419 - accuracy: 0.5547 - top_5_accuracy: 0.7656 - val_loss: 1.7003 - val_accuracy: 0.5645 - val_top_5_accuracy: 0.7894 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "215/215 [==============================] - 85s 397ms/step - loss: 1.6839 - accuracy: 0.5650 - top_5_accuracy: 0.7796 - val_loss: 1.6720 - val_accuracy: 0.5507 - val_top_5_accuracy: 0.7848 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "215/215 [==============================] - 86s 398ms/step - loss: 1.6885 - accuracy: 0.5556 - top_5_accuracy: 0.7789 - val_loss: 1.6276 - val_accuracy: 0.5614 - val_top_5_accuracy: 0.7782 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "215/215 [==============================] - 86s 398ms/step - loss: 1.6626 - accuracy: 0.5647 - top_5_accuracy: 0.7805 - val_loss: 1.6235 - val_accuracy: 0.5890 - val_top_5_accuracy: 0.7925 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "215/215 [==============================] - 85s 397ms/step - loss: 1.7531 - accuracy: 0.5433 - top_5_accuracy: 0.7594 - val_loss: 1.9555 - val_accuracy: 0.4610 - val_top_5_accuracy: 0.7537 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "215/215 [==============================] - 85s 397ms/step - loss: 1.8060 - accuracy: 0.5352 - top_5_accuracy: 0.7540 - val_loss: 1.8564 - val_accuracy: 0.5436 - val_top_5_accuracy: 0.7552 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "215/215 [==============================] - 86s 397ms/step - loss: 1.7143 - accuracy: 0.5473 - top_5_accuracy: 0.7677 - val_loss: 1.6832 - val_accuracy: 0.5665 - val_top_5_accuracy: 0.7828 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "215/215 [==============================] - 85s 397ms/step - loss: 1.6715 - accuracy: 0.5645 - top_5_accuracy: 0.7782 - val_loss: 1.6500 - val_accuracy: 0.5625 - val_top_5_accuracy: 0.7986 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 1.6515 - accuracy: 0.5634 - top_5_accuracy: 0.7798\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 86s 399ms/step - loss: 1.6515 - accuracy: 0.5634 - top_5_accuracy: 0.7798 - val_loss: 1.6547 - val_accuracy: 0.5630 - val_top_5_accuracy: 0.7649 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "215/215 [==============================] - 87s 404ms/step - loss: 1.5901 - accuracy: 0.5664 - top_5_accuracy: 0.7975 - val_loss: 1.5221 - val_accuracy: 0.5818 - val_top_5_accuracy: 0.8129 - lr: 5.0000e-04\n",
      "Epoch 17/50\n",
      "215/215 [==============================] - 86s 401ms/step - loss: 1.6137 - accuracy: 0.5633 - top_5_accuracy: 0.7850 - val_loss: 1.6327 - val_accuracy: 0.5599 - val_top_5_accuracy: 0.7782 - lr: 5.0000e-04\n",
      "Epoch 18/50\n",
      "215/215 [==============================] - 86s 400ms/step - loss: 1.5800 - accuracy: 0.5668 - top_5_accuracy: 0.7958 - val_loss: 1.5561 - val_accuracy: 0.5808 - val_top_5_accuracy: 0.8088 - lr: 5.0000e-04\n",
      "Epoch 19/50\n",
      "215/215 [==============================] - 86s 400ms/step - loss: 1.5866 - accuracy: 0.5725 - top_5_accuracy: 0.7975 - val_loss: 1.6209 - val_accuracy: 0.5762 - val_top_5_accuracy: 0.7863 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "215/215 [==============================] - 86s 399ms/step - loss: 1.5717 - accuracy: 0.5728 - top_5_accuracy: 0.7977 - val_loss: 1.5012 - val_accuracy: 0.5834 - val_top_5_accuracy: 0.8011 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "215/215 [==============================] - 86s 401ms/step - loss: 1.5952 - accuracy: 0.5646 - top_5_accuracy: 0.7956 - val_loss: 1.6446 - val_accuracy: 0.5727 - val_top_5_accuracy: 0.7552 - lr: 5.0000e-04\n",
      "Epoch 22/50\n",
      "215/215 [==============================] - 86s 402ms/step - loss: 1.5858 - accuracy: 0.5650 - top_5_accuracy: 0.7964 - val_loss: 1.5161 - val_accuracy: 0.5839 - val_top_5_accuracy: 0.8123 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "215/215 [==============================] - 86s 397ms/step - loss: 1.6455 - accuracy: 0.5564 - top_5_accuracy: 0.7827 - val_loss: 1.4796 - val_accuracy: 0.5905 - val_top_5_accuracy: 0.8159 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "215/215 [==============================] - 86s 400ms/step - loss: 1.5879 - accuracy: 0.5661 - top_5_accuracy: 0.7966 - val_loss: 1.5351 - val_accuracy: 0.5732 - val_top_5_accuracy: 0.8042 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "215/215 [==============================] - 86s 400ms/step - loss: 1.5986 - accuracy: 0.5714 - top_5_accuracy: 0.7913 - val_loss: 1.6012 - val_accuracy: 0.5905 - val_top_5_accuracy: 0.7797 - lr: 5.0000e-04\n",
      "Epoch 26/50\n",
      "215/215 [==============================] - 86s 401ms/step - loss: 1.5751 - accuracy: 0.5661 - top_5_accuracy: 0.7924 - val_loss: 1.6177 - val_accuracy: 0.5620 - val_top_5_accuracy: 0.7914 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "215/215 [==============================] - 87s 404ms/step - loss: 1.6014 - accuracy: 0.5662 - top_5_accuracy: 0.7927 - val_loss: 1.6423 - val_accuracy: 0.5482 - val_top_5_accuracy: 0.7904 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 1.5412 - accuracy: 0.5744 - top_5_accuracy: 0.8047\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 87s 406ms/step - loss: 1.5412 - accuracy: 0.5744 - top_5_accuracy: 0.8047 - val_loss: 1.6109 - val_accuracy: 0.5752 - val_top_5_accuracy: 0.7680 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "215/215 [==============================] - 87s 406ms/step - loss: 1.5578 - accuracy: 0.5671 - top_5_accuracy: 0.8051 - val_loss: 1.5455 - val_accuracy: 0.5778 - val_top_5_accuracy: 0.8078 - lr: 2.5000e-04\n",
      "Epoch 30/50\n",
      "215/215 [==============================] - 87s 406ms/step - loss: 1.5274 - accuracy: 0.5755 - top_5_accuracy: 0.8117 - val_loss: 1.5660 - val_accuracy: 0.5655 - val_top_5_accuracy: 0.7986 - lr: 2.5000e-04\n",
      "Epoch 31/50\n",
      "215/215 [==============================] - 87s 405ms/step - loss: 1.5395 - accuracy: 0.5671 - top_5_accuracy: 0.8010 - val_loss: 1.4693 - val_accuracy: 0.5946 - val_top_5_accuracy: 0.8108 - lr: 2.5000e-04\n",
      "Epoch 32/50\n",
      "215/215 [==============================] - 87s 406ms/step - loss: 1.5325 - accuracy: 0.5732 - top_5_accuracy: 0.8060 - val_loss: 1.5400 - val_accuracy: 0.5665 - val_top_5_accuracy: 0.8118 - lr: 2.5000e-04\n",
      "Epoch 33/50\n",
      "215/215 [==============================] - 88s 407ms/step - loss: 1.5212 - accuracy: 0.5701 - top_5_accuracy: 0.8064 - val_loss: 1.4638 - val_accuracy: 0.5788 - val_top_5_accuracy: 0.8225 - lr: 2.5000e-04\n",
      "Epoch 34/50\n",
      "215/215 [==============================] - 87s 405ms/step - loss: 1.5067 - accuracy: 0.5744 - top_5_accuracy: 0.8130 - val_loss: 1.5321 - val_accuracy: 0.5783 - val_top_5_accuracy: 0.7909 - lr: 2.5000e-04\n",
      "Epoch 35/50\n",
      "215/215 [==============================] - 87s 406ms/step - loss: 1.5265 - accuracy: 0.5709 - top_5_accuracy: 0.8117 - val_loss: 1.5744 - val_accuracy: 0.5829 - val_top_5_accuracy: 0.7965 - lr: 2.5000e-04\n",
      "Epoch 36/50\n",
      "215/215 [==============================] - 87s 405ms/step - loss: 1.5057 - accuracy: 0.5742 - top_5_accuracy: 0.8055 - val_loss: 1.5006 - val_accuracy: 0.5722 - val_top_5_accuracy: 0.8139 - lr: 2.5000e-04\n",
      "Epoch 37/50\n",
      "215/215 [==============================] - 88s 407ms/step - loss: 1.5396 - accuracy: 0.5742 - top_5_accuracy: 0.8074 - val_loss: 1.5501 - val_accuracy: 0.5788 - val_top_5_accuracy: 0.7858 - lr: 2.5000e-04\n",
      "Epoch 38/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 1.5551 - accuracy: 0.5693 - top_5_accuracy: 0.8051\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "215/215 [==============================] - 87s 406ms/step - loss: 1.5551 - accuracy: 0.5693 - top_5_accuracy: 0.8051 - val_loss: 1.5899 - val_accuracy: 0.5589 - val_top_5_accuracy: 0.7950 - lr: 2.5000e-04\n",
      "Epoch 39/50\n",
      "215/215 [==============================] - 88s 408ms/step - loss: 1.5217 - accuracy: 0.5719 - top_5_accuracy: 0.8096 - val_loss: 1.5053 - val_accuracy: 0.5854 - val_top_5_accuracy: 0.8062 - lr: 1.2500e-04\n",
      "Epoch 40/50\n",
      "215/215 [==============================] - 87s 406ms/step - loss: 1.5157 - accuracy: 0.5786 - top_5_accuracy: 0.8147 - val_loss: 1.5204 - val_accuracy: 0.5645 - val_top_5_accuracy: 0.8129 - lr: 1.2500e-04\n",
      "Epoch 41/50\n",
      "215/215 [==============================] - 87s 406ms/step - loss: 1.4951 - accuracy: 0.5725 - top_5_accuracy: 0.8188 - val_loss: 1.5266 - val_accuracy: 0.5727 - val_top_5_accuracy: 0.8251 - lr: 1.2500e-04\n",
      "Epoch 42/50\n",
      "215/215 [==============================] - 88s 409ms/step - loss: 1.4881 - accuracy: 0.5758 - top_5_accuracy: 0.8205 - val_loss: 1.4375 - val_accuracy: 0.5849 - val_top_5_accuracy: 0.8256 - lr: 1.2500e-04\n",
      "Epoch 43/50\n",
      "215/215 [==============================] - 87s 407ms/step - loss: 1.4718 - accuracy: 0.5859 - top_5_accuracy: 0.8191 - val_loss: 1.4971 - val_accuracy: 0.5732 - val_top_5_accuracy: 0.8266 - lr: 1.2500e-04\n",
      "Epoch 44/50\n",
      "215/215 [==============================] - 88s 410ms/step - loss: 1.4922 - accuracy: 0.5754 - top_5_accuracy: 0.8175 - val_loss: 1.5176 - val_accuracy: 0.5829 - val_top_5_accuracy: 0.8190 - lr: 1.2500e-04\n",
      "Epoch 45/50\n",
      "215/215 [==============================] - 88s 410ms/step - loss: 1.4859 - accuracy: 0.5729 - top_5_accuracy: 0.8154 - val_loss: 1.4809 - val_accuracy: 0.5824 - val_top_5_accuracy: 0.8266 - lr: 1.2500e-04\n",
      "Epoch 46/50\n",
      "215/215 [==============================] - 88s 409ms/step - loss: 1.4723 - accuracy: 0.5795 - top_5_accuracy: 0.8159 - val_loss: 1.5573 - val_accuracy: 0.5737 - val_top_5_accuracy: 0.8149 - lr: 1.2500e-04\n",
      "Epoch 47/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 1.4657 - accuracy: 0.5796 - top_5_accuracy: 0.8230\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "215/215 [==============================] - 88s 410ms/step - loss: 1.4657 - accuracy: 0.5796 - top_5_accuracy: 0.8230 - val_loss: 1.5593 - val_accuracy: 0.5844 - val_top_5_accuracy: 0.8180 - lr: 1.2500e-04\n",
      "Epoch 48/50\n",
      "215/215 [==============================] - 88s 409ms/step - loss: 1.4755 - accuracy: 0.5760 - top_5_accuracy: 0.8138 - val_loss: 1.4788 - val_accuracy: 0.5900 - val_top_5_accuracy: 0.8195 - lr: 6.2500e-05\n",
      "Epoch 49/50\n",
      "215/215 [==============================] - 88s 408ms/step - loss: 1.4579 - accuracy: 0.5783 - top_5_accuracy: 0.8230 - val_loss: 1.4556 - val_accuracy: 0.5818 - val_top_5_accuracy: 0.8271 - lr: 6.2500e-05\n",
      "Epoch 50/50\n",
      "215/215 [==============================] - 88s 409ms/step - loss: 1.4717 - accuracy: 0.5748 - top_5_accuracy: 0.8201 - val_loss: 1.4984 - val_accuracy: 0.5798 - val_top_5_accuracy: 0.8241 - lr: 6.2500e-05\n",
      "\n",
      "‚úì Training completed!\n",
      "‚è±Ô∏è  End time: 2025-11-14 20:21:19\n",
      "‚úì Model weights saved to: VisionTransformer_output\\models\\model_weights.h5\n",
      "\n",
      "======================================================================\n",
      "EVALUATING VisionTransformer\n",
      "======================================================================\n",
      "\n",
      "üìä Evaluating on test set...\n",
      "31/31 [==============================] - 4s 123ms/step - loss: 1.5361 - accuracy: 0.5743 - top_5_accuracy: 0.8096\n",
      "\n",
      "‚úì Test Results:\n",
      "  Loss: 1.5361\n",
      "  Accuracy: 0.5743\n",
      "  Top-5 Accuracy: 0.8096\n",
      "\n",
      "üìä Generating detailed metrics...\n",
      "‚úì Confusion matrix saved to: VisionTransformer_output\\plots\\confusion_matrix.png\n",
      "‚úì Classification report saved to: VisionTransformer_output\\logs\\classification_report.txt\n",
      "\n",
      "üìà Generating training plots...\n",
      "‚úì Training plots saved to: VisionTransformer_output\\plots\\training_history.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 1.5360524654388428,\n",
       " 'test_accuracy': 0.5743380784988403,\n",
       " 'test_top5_accuracy': 0.8095722794532776,\n",
       " 'num_classes': 63,\n",
       " 'evaluation_time': '2025-11-14T20:21:29.184857'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vision_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6520e04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìä\n",
      "COMPARING ALL MODELS\n",
      "üìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìäüìä\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL RESULTS\n",
      "======================================================================\n",
      "‚úì Loaded results for EfficientNetB0\n",
      "‚úì Loaded results for ResNet50\n",
      "‚úì Loaded results for VisionTransformer\n",
      "\n",
      "======================================================================\n",
      "MODEL PERFORMANCE COMPARISON\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "            Model Test Accuracy (%) Test Loss Top-5 Accuracy (%)  Number of Classes\n",
      "   EfficientNetB0             83.71    0.4259              96.64                 63\n",
      "         ResNet50             80.35    0.4938              97.15                 63\n",
      "VisionTransformer             57.43    1.5361              80.96                 63\n",
      "======================================================================\n",
      "\n",
      "‚úì Comparison table saved to: model_comparison.csv\n",
      "‚úì Comparison data saved to: model_comparison.json\n",
      "\n",
      "üìä Generating comparison charts...\n",
      "‚úì Comparison charts saved to: model_comparison_charts.png\n",
      "\n",
      "üìù Generating summary report...\n",
      "‚úì Summary report saved to: model_comparison_report.txt\n",
      "\n",
      "======================================================================\n",
      "üéâ COMPARISON COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "üìÅ Output files:\n",
      "  - model_comparison.csv\n",
      "  - model_comparison.json\n",
      "  - model_comparison_charts.png\n",
      "  - model_comparison_report.txt\n"
     ]
    }
   ],
   "source": [
    "compare_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89dcfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import InceptionV3, VGG16, VGG19\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "class DefectDetectionModel:\n",
    "    \"\"\"Base class for defect detection models\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, model_name, img_height=224, img_width=224):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.model_name = model_name\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.class_names = []\n",
    "        self.num_classes = 0\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir = Path(f\"{model_name}_output\")\n",
    "        self.model_dir = self.output_dir / \"models\"\n",
    "        self.plots_dir = self.output_dir / \"plots\"\n",
    "        self.logs_dir = self.output_dir / \"logs\"\n",
    "        \n",
    "        for dir_path in [self.output_dir, self.model_dir, self.plots_dir, self.logs_dir]:\n",
    "            dir_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def prepare_data(self, validation_split=0.2, test_split=0.1, batch_size=32):\n",
    "        \"\"\"Prepare dataset with efficient TensorFlow pipeline\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"DATA PREPARATION - {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Get all image paths and labels\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        print(\"\\nüìÅ Scanning dataset...\")\n",
    "        for class_dir in self.dataset_path.rglob(\"*\"):\n",
    "            if class_dir.is_dir():\n",
    "                images = (list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\")) + \n",
    "                         list(class_dir.glob(\"*.jpeg\")) + list(class_dir.glob(\"*.bmp\")))\n",
    "                \n",
    "                if images:\n",
    "                    class_name = class_dir.name\n",
    "                    if class_name not in self.class_names:\n",
    "                        self.class_names.append(class_name)\n",
    "                    \n",
    "                    for img_path in images:\n",
    "                        image_paths.append(str(img_path))\n",
    "                        labels.append(class_name)\n",
    "        \n",
    "        self.class_names.sort()\n",
    "        self.num_classes = len(self.class_names)\n",
    "        print(f\"‚úì Found {len(image_paths)} images\")\n",
    "        print(f\"‚úì Number of classes: {self.num_classes}\")\n",
    "        print(f\"‚úì Classes: {self.class_names[:10]}{'...' if len(self.class_names) > 10 else ''}\")\n",
    "        \n",
    "        # Create label mapping\n",
    "        label_to_index = {label: idx for idx, label in enumerate(self.class_names)}\n",
    "        labels_encoded = [label_to_index[label] for label in labels]\n",
    "        \n",
    "        # Create tf.data.Dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels_encoded))\n",
    "        dataset = dataset.shuffle(len(image_paths), seed=42)\n",
    "        \n",
    "        # Calculate splits\n",
    "        train_size = int(len(image_paths) * (1 - validation_split - test_split))\n",
    "        val_size = int(len(image_paths) * validation_split)\n",
    "        test_size = len(image_paths) - train_size - val_size\n",
    "        \n",
    "        # Split datasets\n",
    "        train_ds = dataset.take(train_size)\n",
    "        val_ds = dataset.skip(train_size).take(val_size)\n",
    "        test_ds = dataset.skip(train_size + val_size)\n",
    "        \n",
    "        # Process datasets\n",
    "        self.train_dataset = train_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=True),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.val_dataset = val_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=False),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.test_dataset = test_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=False),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        print(f\"\\n‚úì Train samples: {train_size}\")\n",
    "        print(f\"‚úì Validation samples: {val_size}\")\n",
    "        print(f\"‚úì Test samples: {test_size}\")\n",
    "        \n",
    "        return train_size, val_size, test_size\n",
    "    \n",
    "    def _load_and_preprocess(self, image_path, label, augment=False):\n",
    "        \"\"\"Load and preprocess image\"\"\"\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, [self.img_height, self.img_width])\n",
    "        \n",
    "        # Data augmentation\n",
    "        if augment:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_flip_up_down(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.rot90(img, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "        \n",
    "        img = tf.clip_by_value(img, 0.0, 255.0)\n",
    "        img = img / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        label_onehot = tf.one_hot(label, self.num_classes)\n",
    "        return img, label_onehot\n",
    "    \n",
    "    def train(self, epochs=50, early_stopping_patience=10):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"TRAINING {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built. Call build_model() first.\")\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=early_stopping_patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.CSVLogger(\n",
    "                str(self.logs_dir / 'training_log.csv')\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting training for {epochs} epochs...\")\n",
    "        print(f\"‚è±Ô∏è  Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            self.train_dataset,\n",
    "            epochs=epochs,\n",
    "            validation_data=self.val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úì Training completed!\")\n",
    "        print(f\"‚è±Ô∏è  End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Save model weights\n",
    "        self.model.save_weights(str(self.model_dir / 'model_weights.h5'))\n",
    "        print(f\"‚úì Model weights saved to: {self.model_dir / 'model_weights.h5'}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"EVALUATING {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìä Evaluating on test set...\")\n",
    "        test_results = self.model.evaluate(self.test_dataset, verbose=1)\n",
    "        \n",
    "        print(\"\\n‚úì Test Results:\")\n",
    "        print(f\"  Loss: {test_results[0]:.4f}\")\n",
    "        print(f\"  Accuracy: {test_results[1]:.4f}\")\n",
    "        if len(test_results) > 2:\n",
    "            print(f\"  Top-5 Accuracy: {test_results[2]:.4f}\")\n",
    "        \n",
    "        # Get predictions for confusion matrix\n",
    "        print(\"\\nüìä Generating detailed metrics...\")\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for images, labels in self.test_dataset:\n",
    "            predictions = self.model.predict(images, verbose=0)\n",
    "            y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'test_loss': float(test_results[0]),\n",
    "            'test_accuracy': float(test_results[1]),\n",
    "            'num_classes': self.num_classes,\n",
    "            'evaluation_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        if len(test_results) > 2:\n",
    "            results['test_top5_accuracy'] = float(test_results[2])\n",
    "        \n",
    "        with open(self.logs_dir / 'test_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        self._plot_confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Generate classification report\n",
    "        self._save_classification_report(y_true, y_pred)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _plot_confusion_matrix(self, y_true, y_pred):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(max(10, self.num_classes), max(8, self.num_classes * 0.8)))\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        sns.heatmap(cm_normalized, annot=False, cmap='Blues', \n",
    "                    xticklabels=self.class_names if self.num_classes <= 20 else False,\n",
    "                    yticklabels=self.class_names if self.num_classes <= 20 else False)\n",
    "        \n",
    "        plt.title(f'{self.model_name} - Confusion Matrix (Normalized)')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Confusion matrix saved to: {self.plots_dir / 'confusion_matrix.png'}\")\n",
    "    \n",
    "    def _save_classification_report(self, y_true, y_pred):\n",
    "        \"\"\"Save classification report\"\"\"\n",
    "        report = classification_report(y_true, y_pred, target_names=self.class_names, \n",
    "                                      zero_division=0)\n",
    "        \n",
    "        with open(self.logs_dir / 'classification_report.txt', 'w') as f:\n",
    "            f.write(f\"{self.model_name} - Classification Report\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"‚úì Classification report saved to: {self.logs_dir / 'classification_report.txt'}\")\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"‚ö†Ô∏è  No training history available\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüìà Generating training plots...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        axes[0].plot(self.history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "        axes[0].plot(self.history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "        axes[0].set_title(f'{self.model_name} - Model Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[1].plot(self.history.history['loss'], label='Train Loss', linewidth=2)\n",
    "        axes[1].plot(self.history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "        axes[1].set_title(f'{self.model_name} - Model Loss', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Training plots saved to: {self.plots_dir / 'training_history.png'}\")\n",
    "\n",
    "\n",
    "class InceptionV3DefectDetector(DefectDetectionModel):\n",
    "    \"\"\"InceptionV3 for defect detection\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, img_height=299, img_width=299):\n",
    "        # InceptionV3 requires 299x299 input\n",
    "        super().__init__(dataset_path, \"InceptionV3\", img_height, img_width)\n",
    "    \n",
    "    def build_model(self, learning_rate=0.001, dropout_rate=0.3):\n",
    "        \"\"\"Build InceptionV3 model with transfer learning\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"BUILDING INCEPTIONV3 MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load pre-trained InceptionV3\n",
    "        base_model = InceptionV3(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(self.img_height, self.img_width, 3)\n",
    "        )\n",
    "        \n",
    "        # Freeze base model initially\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Build model\n",
    "        inputs = keras.Input(shape=(self.img_height, self.img_width, 3))\n",
    "        \n",
    "        # InceptionV3 preprocessing (scale to [-1, 1])\n",
    "        x = layers.Rescaling(scale=2.0, offset=-1.0)(inputs)\n",
    "        \n",
    "        x = base_model(x, training=False)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(1024, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate / 2)(x)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(\"\\n‚úì Model built successfully!\")\n",
    "        print(f\"‚úì Total parameters: {model.count_params():,}\")\n",
    "        print(f\"‚úì Base model frozen: {not base_model.trainable}\")\n",
    "        print(f\"‚úì Input size: {self.img_height}x{self.img_width}\")\n",
    "        \n",
    "        # Save architecture\n",
    "        with open(self.model_dir / 'model_architecture.txt', 'w') as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def unfreeze_and_fine_tune(self, fine_tune_from_layer=249, learning_rate=0.0001):\n",
    "        \"\"\"Unfreeze layers for fine-tuning\"\"\"\n",
    "        print(f\"\\nüîì Unfreezing layers from layer {fine_tune_from_layer}...\")\n",
    "        \n",
    "        base_model = self.model.layers[2]  # Get InceptionV3 base\n",
    "        base_model.trainable = True\n",
    "        \n",
    "        # Freeze layers before fine_tune_from_layer\n",
    "        for layer in base_model.layers[:fine_tune_from_layer]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Recompile\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Fine-tuning enabled with learning rate: {learning_rate}\")\n",
    "\n",
    "\n",
    "class VGG16DefectDetector(DefectDetectionModel):\n",
    "    \"\"\"VGG16 for defect detection\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, img_height=224, img_width=224):\n",
    "        super().__init__(dataset_path, \"VGG16\", img_height, img_width)\n",
    "    \n",
    "    def build_model(self, learning_rate=0.001, dropout_rate=0.5):\n",
    "        \"\"\"Build VGG16 model with transfer learning\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"BUILDING VGG16 MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load pre-trained VGG16\n",
    "        base_model = VGG16(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(self.img_height, self.img_width, 3)\n",
    "        )\n",
    "        \n",
    "        # Freeze base model initially\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Build model\n",
    "        inputs = keras.Input(shape=(self.img_height, self.img_width, 3))\n",
    "        \n",
    "        # VGG preprocessing (mean subtraction)\n",
    "        x = tf.keras.applications.vgg16.preprocess_input(inputs * 255.0)\n",
    "        \n",
    "        x = base_model(x, training=False)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(4096, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(2048, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(1024, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate / 2)(x)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(\"\\n‚úì Model built successfully!\")\n",
    "        print(f\"‚úì Total parameters: {model.count_params():,}\")\n",
    "        print(f\"‚úì Base model frozen: {not base_model.trainable}\")\n",
    "        \n",
    "        # Save architecture\n",
    "        with open(self.model_dir / 'model_architecture.txt', 'w') as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def unfreeze_and_fine_tune(self, fine_tune_from_layer=15, learning_rate=0.0001):\n",
    "        \"\"\"Unfreeze layers for fine-tuning\"\"\"\n",
    "        print(f\"\\nüîì Unfreezing layers from layer {fine_tune_from_layer}...\")\n",
    "        \n",
    "        base_model = self.model.layers[2]  # Get VGG16 base\n",
    "        base_model.trainable = True\n",
    "        \n",
    "        # Freeze layers before fine_tune_from_layer\n",
    "        for layer in base_model.layers[:fine_tune_from_layer]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Recompile\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Fine-tuning enabled with learning rate: {learning_rate}\")\n",
    "\n",
    "\n",
    "class VGG19DefectDetector(DefectDetectionModel):\n",
    "    \"\"\"VGG19 for defect detection\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, img_height=224, img_width=224):\n",
    "        super().__init__(dataset_path, \"VGG19\", img_height, img_width)\n",
    "    \n",
    "    def build_model(self, learning_rate=0.001, dropout_rate=0.5):\n",
    "        \"\"\"Build VGG19 model with transfer learning\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"BUILDING VGG19 MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load pre-trained VGG19\n",
    "        base_model = VGG19(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(self.img_height, self.img_width, 3)\n",
    "        )\n",
    "        \n",
    "        # Freeze base model initially\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Build model\n",
    "        inputs = keras.Input(shape=(self.img_height, self.img_width, 3))\n",
    "        \n",
    "        # VGG preprocessing (mean subtraction)\n",
    "        x = tf.keras.applications.vgg19.preprocess_input(inputs * 255.0)\n",
    "        \n",
    "        x = base_model(x, training=False)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(4096, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(2048, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(1024, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate / 2)(x)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(\"\\n‚úì Model built successfully!\")\n",
    "        print(f\"‚úì Total parameters: {model.count_params():,}\")\n",
    "        print(f\"‚úì Base model frozen: {not base_model.trainable}\")\n",
    "        \n",
    "        # Save architecture\n",
    "        with open(self.model_dir / 'model_architecture.txt', 'w') as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def unfreeze_and_fine_tune(self, fine_tune_from_layer=17, learning_rate=0.0001):\n",
    "        \"\"\"Unfreeze layers for fine-tuning\"\"\"\n",
    "        print(f\"\\nüîì Unfreezing layers from layer {fine_tune_from_layer}...\")\n",
    "        \n",
    "        base_model = self.model.layers[2]  # Get VGG19 base\n",
    "        base_model.trainable = True\n",
    "        \n",
    "        # Freeze layers before fine_tune_from_layer\n",
    "        for layer in base_model.layers[:fine_tune_from_layer]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Recompile\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Fine-tuning enabled with learning rate: {learning_rate}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def train_inceptionv3(dataset_path=\"final_merged_dataset\", epochs=50, batch_size=32):\n",
    "    \"\"\"Train InceptionV3 model\"\"\"\n",
    "    print(\"\\n\\n\" + \"üî∂\"*35)\n",
    "    print(\"TRAINING INCEPTIONV3\")\n",
    "    print(\"üî∂\"*35)\n",
    "    \n",
    "    model = InceptionV3DefectDetector(dataset_path)\n",
    "    model.prepare_data(batch_size=batch_size)\n",
    "    model.build_model(learning_rate=0.001)\n",
    "    model.train(epochs=epochs)\n",
    "    results = model.evaluate()\n",
    "    model.plot_training_history()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def train_vgg16(dataset_path=\"final_merged_dataset\", epochs=50, batch_size=32):\n",
    "    \"\"\"Train VGG16 model\"\"\"\n",
    "    print(\"\\n\\n\" + \"üî∑\"*35)\n",
    "    print(\"TRAINING VGG16\")\n",
    "    print(\"üî∑\"*35)\n",
    "    \n",
    "    model = VGG16DefectDetector(dataset_path)\n",
    "    model.prepare_data(batch_size=batch_size)\n",
    "    model.build_model(learning_rate=0.001)\n",
    "    model.train(epochs=epochs)\n",
    "    results = model.evaluate()\n",
    "    model.plot_training_history()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def train_vgg19(dataset_path=\"final_merged_dataset\", epochs=50, batch_size=32):\n",
    "    \"\"\"Train VGG19 model\"\"\"\n",
    "    print(\"\\n\\n\" + \"üîπ\"*35)\n",
    "    print(\"TRAINING VGG19\")\n",
    "    print(\"üîπ\"*35)\n",
    "    \n",
    "    model = VGG19DefectDetector(dataset_path)\n",
    "    model.prepare_data(batch_size=batch_size)\n",
    "    model.build_model(learning_rate=0.001)\n",
    "    model.train(epochs=epochs)\n",
    "    results = model.evaluate()\n",
    "    model.plot_training_history()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def train_all_three(dataset_path=\"final_merged_dataset\", epochs=30, batch_size=32):\n",
    "    \"\"\"Train all three models\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING ALL THREE MODELS: INCEPTIONV3, VGG16, VGG19\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Train InceptionV3\n",
    "    results['InceptionV3'] = train_inceptionv3(dataset_path, epochs, batch_size)\n",
    "    \n",
    "    # Train VGG16\n",
    "    results['VGG16'] = train_vgg16(dataset_path, epochs, batch_size)\n",
    "    \n",
    "    # Train VGG19\n",
    "    results['VGG19'] = train_vgg19(dataset_path, epochs, batch_size)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ ALL THREE MODELS TRAINED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nüìÅ Output directories:\")\n",
    "    print(\"  - InceptionV3_output/\")\n",
    "    print(\"  - VGG16_output/\")\n",
    "    print(\"  - VGG19_output/\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56cefa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂\n",
      "TRAINING INCEPTIONV3\n",
      "üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂\n",
      "\n",
      "======================================================================\n",
      "DATA PREPARATION - InceptionV3\n",
      "======================================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images\n",
      "‚úì Number of classes: 63\n",
      "‚úì Classes: ['Missing_hole', 'Missing_hole_rotation', 'Mouse_bite', 'Mouse_bite_rotation', 'Open_circuit', 'Open_circuit_rotation', 'PCB_USED', 'Short', 'Short_rotation', 'Spur']...\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1961\n",
      "‚úì Test samples: 982\n",
      "\n",
      "======================================================================\n",
      "BUILDING INCEPTIONV3 MODEL\n",
      "======================================================================\n",
      "\n",
      "‚úì Model built successfully!\n",
      "‚úì Total parameters: 24,466,271\n",
      "‚úì Base model frozen: True\n",
      "‚úì Input size: 299x299\n",
      "\n",
      "======================================================================\n",
      "TRAINING InceptionV3\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting training for 50 epochs...\n",
      "‚è±Ô∏è  Start time: 2025-11-18 20:51:38\n",
      "\n",
      "Epoch 1/50\n",
      "215/215 [==============================] - 104s 453ms/step - loss: 1.6010 - accuracy: 0.6226 - top_5_accuracy: 0.8019 - val_loss: 1.2104 - val_accuracy: 0.6757 - val_top_5_accuracy: 0.8781 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "215/215 [==============================] - 63s 291ms/step - loss: 1.2210 - accuracy: 0.6593 - top_5_accuracy: 0.8734 - val_loss: 1.0545 - val_accuracy: 0.6940 - val_top_5_accuracy: 0.9077 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "215/215 [==============================] - 61s 283ms/step - loss: 1.0951 - accuracy: 0.6736 - top_5_accuracy: 0.8922 - val_loss: 0.9421 - val_accuracy: 0.7063 - val_top_5_accuracy: 0.9169 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "215/215 [==============================] - 63s 290ms/step - loss: 1.0358 - accuracy: 0.6855 - top_5_accuracy: 0.9046 - val_loss: 0.9575 - val_accuracy: 0.6894 - val_top_5_accuracy: 0.9250 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "215/215 [==============================] - 62s 289ms/step - loss: 1.0067 - accuracy: 0.6870 - top_5_accuracy: 0.9082 - val_loss: 0.8784 - val_accuracy: 0.7206 - val_top_5_accuracy: 0.9296 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "215/215 [==============================] - 59s 276ms/step - loss: 0.9759 - accuracy: 0.7015 - top_5_accuracy: 0.9151 - val_loss: 0.8755 - val_accuracy: 0.7098 - val_top_5_accuracy: 0.9342 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "215/215 [==============================] - 55s 257ms/step - loss: 0.9385 - accuracy: 0.7023 - top_5_accuracy: 0.9200 - val_loss: 0.8639 - val_accuracy: 0.7272 - val_top_5_accuracy: 0.9347 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "215/215 [==============================] - 62s 287ms/step - loss: 0.9534 - accuracy: 0.7023 - top_5_accuracy: 0.9212 - val_loss: 0.8743 - val_accuracy: 0.6961 - val_top_5_accuracy: 0.9388 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "215/215 [==============================] - 60s 278ms/step - loss: 0.9415 - accuracy: 0.7011 - top_5_accuracy: 0.9244 - val_loss: 0.8018 - val_accuracy: 0.7343 - val_top_5_accuracy: 0.9393 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "215/215 [==============================] - 57s 266ms/step - loss: 0.9248 - accuracy: 0.7015 - top_5_accuracy: 0.9279 - val_loss: 0.8289 - val_accuracy: 0.7231 - val_top_5_accuracy: 0.9378 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "215/215 [==============================] - 59s 273ms/step - loss: 0.8976 - accuracy: 0.7127 - top_5_accuracy: 0.9291 - val_loss: 0.7364 - val_accuracy: 0.7517 - val_top_5_accuracy: 0.9475 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "215/215 [==============================] - 56s 260ms/step - loss: 0.8580 - accuracy: 0.7177 - top_5_accuracy: 0.9334 - val_loss: 0.8116 - val_accuracy: 0.7277 - val_top_5_accuracy: 0.9419 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "215/215 [==============================] - 59s 270ms/step - loss: 0.8885 - accuracy: 0.7078 - top_5_accuracy: 0.9277 - val_loss: 0.7995 - val_accuracy: 0.7302 - val_top_5_accuracy: 0.9220 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "215/215 [==============================] - 57s 266ms/step - loss: 0.8547 - accuracy: 0.7202 - top_5_accuracy: 0.9353 - val_loss: 0.7534 - val_accuracy: 0.7420 - val_top_5_accuracy: 0.9531 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "215/215 [==============================] - 56s 262ms/step - loss: 0.8599 - accuracy: 0.7234 - top_5_accuracy: 0.9307 - val_loss: 0.7348 - val_accuracy: 0.7425 - val_top_5_accuracy: 0.9546 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "215/215 [==============================] - 53s 246ms/step - loss: 0.8248 - accuracy: 0.7296 - top_5_accuracy: 0.9331 - val_loss: 0.6887 - val_accuracy: 0.7654 - val_top_5_accuracy: 0.9592 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "215/215 [==============================] - 52s 241ms/step - loss: 0.8323 - accuracy: 0.7231 - top_5_accuracy: 0.9362 - val_loss: 0.7165 - val_accuracy: 0.7552 - val_top_5_accuracy: 0.9490 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "215/215 [==============================] - 56s 259ms/step - loss: 0.8392 - accuracy: 0.7272 - top_5_accuracy: 0.9372 - val_loss: 0.7029 - val_accuracy: 0.7455 - val_top_5_accuracy: 0.9490 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "215/215 [==============================] - 53s 247ms/step - loss: 0.8105 - accuracy: 0.7317 - top_5_accuracy: 0.9352 - val_loss: 0.7707 - val_accuracy: 0.7379 - val_top_5_accuracy: 0.9505 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "215/215 [==============================] - 51s 237ms/step - loss: 0.8161 - accuracy: 0.7330 - top_5_accuracy: 0.9414 - val_loss: 0.6862 - val_accuracy: 0.7649 - val_top_5_accuracy: 0.9577 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "215/215 [==============================] - 47s 219ms/step - loss: 0.8309 - accuracy: 0.7241 - top_5_accuracy: 0.9369 - val_loss: 0.6713 - val_accuracy: 0.7629 - val_top_5_accuracy: 0.9541 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "215/215 [==============================] - 44s 203ms/step - loss: 0.7956 - accuracy: 0.7312 - top_5_accuracy: 0.9404 - val_loss: 0.7138 - val_accuracy: 0.7466 - val_top_5_accuracy: 0.9495 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "215/215 [==============================] - 43s 197ms/step - loss: 0.8064 - accuracy: 0.7331 - top_5_accuracy: 0.9393 - val_loss: 0.6607 - val_accuracy: 0.7583 - val_top_5_accuracy: 0.9602 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "215/215 [==============================] - 43s 199ms/step - loss: 0.7883 - accuracy: 0.7328 - top_5_accuracy: 0.9442 - val_loss: 0.6977 - val_accuracy: 0.7506 - val_top_5_accuracy: 0.9490 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "215/215 [==============================] - 44s 204ms/step - loss: 0.7725 - accuracy: 0.7375 - top_5_accuracy: 0.9457 - val_loss: 0.6652 - val_accuracy: 0.7731 - val_top_5_accuracy: 0.9618 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "215/215 [==============================] - 44s 206ms/step - loss: 0.7911 - accuracy: 0.7400 - top_5_accuracy: 0.9412 - val_loss: 0.7030 - val_accuracy: 0.7460 - val_top_5_accuracy: 0.9618 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "215/215 [==============================] - 44s 203ms/step - loss: 0.7756 - accuracy: 0.7397 - top_5_accuracy: 0.9394 - val_loss: 0.6682 - val_accuracy: 0.7542 - val_top_5_accuracy: 0.9587 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.7611 - accuracy: 0.7385 - top_5_accuracy: 0.9457\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 33s 151ms/step - loss: 0.7611 - accuracy: 0.7385 - top_5_accuracy: 0.9457 - val_loss: 0.6841 - val_accuracy: 0.7664 - val_top_5_accuracy: 0.9577 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "215/215 [==============================] - 29s 137ms/step - loss: 0.7257 - accuracy: 0.7490 - top_5_accuracy: 0.9515 - val_loss: 0.6011 - val_accuracy: 0.7787 - val_top_5_accuracy: 0.9653 - lr: 5.0000e-04\n",
      "Epoch 30/50\n",
      "215/215 [==============================] - 30s 137ms/step - loss: 0.6709 - accuracy: 0.7626 - top_5_accuracy: 0.9524 - val_loss: 0.6371 - val_accuracy: 0.7644 - val_top_5_accuracy: 0.9556 - lr: 5.0000e-04\n",
      "Epoch 31/50\n",
      "215/215 [==============================] - 30s 138ms/step - loss: 0.6831 - accuracy: 0.7572 - top_5_accuracy: 0.9543 - val_loss: 0.5527 - val_accuracy: 0.8032 - val_top_5_accuracy: 0.9623 - lr: 5.0000e-04\n",
      "Epoch 32/50\n",
      "215/215 [==============================] - 30s 137ms/step - loss: 0.6688 - accuracy: 0.7624 - top_5_accuracy: 0.9537 - val_loss: 0.5910 - val_accuracy: 0.7879 - val_top_5_accuracy: 0.9628 - lr: 5.0000e-04\n",
      "Epoch 33/50\n",
      "215/215 [==============================] - 30s 140ms/step - loss: 0.6637 - accuracy: 0.7678 - top_5_accuracy: 0.9579 - val_loss: 0.5641 - val_accuracy: 0.7812 - val_top_5_accuracy: 0.9653 - lr: 5.0000e-04\n",
      "Epoch 34/50\n",
      "215/215 [==============================] - 30s 137ms/step - loss: 0.6341 - accuracy: 0.7690 - top_5_accuracy: 0.9610 - val_loss: 0.5485 - val_accuracy: 0.7965 - val_top_5_accuracy: 0.9643 - lr: 5.0000e-04\n",
      "Epoch 35/50\n",
      "215/215 [==============================] - 30s 137ms/step - loss: 0.6585 - accuracy: 0.7626 - top_5_accuracy: 0.9559 - val_loss: 0.5569 - val_accuracy: 0.7930 - val_top_5_accuracy: 0.9633 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "215/215 [==============================] - 30s 141ms/step - loss: 0.6615 - accuracy: 0.7620 - top_5_accuracy: 0.9573 - val_loss: 0.5622 - val_accuracy: 0.7858 - val_top_5_accuracy: 0.9730 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "215/215 [==============================] - 30s 141ms/step - loss: 0.6412 - accuracy: 0.7669 - top_5_accuracy: 0.9576 - val_loss: 0.5509 - val_accuracy: 0.7930 - val_top_5_accuracy: 0.9699 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "215/215 [==============================] - 29s 137ms/step - loss: 0.6473 - accuracy: 0.7671 - top_5_accuracy: 0.9615 - val_loss: 0.5653 - val_accuracy: 0.7863 - val_top_5_accuracy: 0.9597 - lr: 5.0000e-04\n",
      "Epoch 39/50\n",
      "215/215 [==============================] - 30s 138ms/step - loss: 0.6485 - accuracy: 0.7658 - top_5_accuracy: 0.9607 - val_loss: 0.5369 - val_accuracy: 0.7965 - val_top_5_accuracy: 0.9638 - lr: 5.0000e-04\n",
      "Epoch 40/50\n",
      "215/215 [==============================] - 29s 137ms/step - loss: 0.6491 - accuracy: 0.7659 - top_5_accuracy: 0.9570 - val_loss: 0.5545 - val_accuracy: 0.7889 - val_top_5_accuracy: 0.9704 - lr: 5.0000e-04\n",
      "Epoch 41/50\n",
      "215/215 [==============================] - 30s 137ms/step - loss: 0.6361 - accuracy: 0.7717 - top_5_accuracy: 0.9567 - val_loss: 0.5408 - val_accuracy: 0.7940 - val_top_5_accuracy: 0.9633 - lr: 5.0000e-04\n",
      "Epoch 42/50\n",
      "215/215 [==============================] - 30s 138ms/step - loss: 0.6412 - accuracy: 0.7636 - top_5_accuracy: 0.9553 - val_loss: 0.5593 - val_accuracy: 0.7935 - val_top_5_accuracy: 0.9623 - lr: 5.0000e-04\n",
      "Epoch 43/50\n",
      "215/215 [==============================] - 30s 138ms/step - loss: 0.6107 - accuracy: 0.7780 - top_5_accuracy: 0.9583 - val_loss: 0.5419 - val_accuracy: 0.7935 - val_top_5_accuracy: 0.9699 - lr: 5.0000e-04\n",
      "Epoch 44/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.6211 - accuracy: 0.7744 - top_5_accuracy: 0.9639\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 29s 137ms/step - loss: 0.6211 - accuracy: 0.7744 - top_5_accuracy: 0.9639 - val_loss: 0.5640 - val_accuracy: 0.7970 - val_top_5_accuracy: 0.9638 - lr: 5.0000e-04\n",
      "Epoch 45/50\n",
      "215/215 [==============================] - 30s 137ms/step - loss: 0.5992 - accuracy: 0.7790 - top_5_accuracy: 0.9602 - val_loss: 0.5178 - val_accuracy: 0.8042 - val_top_5_accuracy: 0.9643 - lr: 2.5000e-04\n",
      "Epoch 46/50\n",
      "215/215 [==============================] - 29s 137ms/step - loss: 0.5828 - accuracy: 0.7849 - top_5_accuracy: 0.9626 - val_loss: 0.5343 - val_accuracy: 0.7996 - val_top_5_accuracy: 0.9602 - lr: 2.5000e-04\n",
      "Epoch 47/50\n",
      "215/215 [==============================] - 30s 137ms/step - loss: 0.5813 - accuracy: 0.7873 - top_5_accuracy: 0.9640 - val_loss: 0.4874 - val_accuracy: 0.8144 - val_top_5_accuracy: 0.9720 - lr: 2.5000e-04\n",
      "Epoch 48/50\n",
      "215/215 [==============================] - 30s 137ms/step - loss: 0.5563 - accuracy: 0.7907 - top_5_accuracy: 0.9659 - val_loss: 0.4836 - val_accuracy: 0.8134 - val_top_5_accuracy: 0.9735 - lr: 2.5000e-04\n",
      "Epoch 49/50\n",
      "215/215 [==============================] - 30s 137ms/step - loss: 0.5659 - accuracy: 0.7875 - top_5_accuracy: 0.9645 - val_loss: 0.5173 - val_accuracy: 0.8027 - val_top_5_accuracy: 0.9750 - lr: 2.5000e-04\n",
      "Epoch 50/50\n",
      "215/215 [==============================] - 30s 137ms/step - loss: 0.5732 - accuracy: 0.7866 - top_5_accuracy: 0.9624 - val_loss: 0.4825 - val_accuracy: 0.8072 - val_top_5_accuracy: 0.9714 - lr: 2.5000e-04\n",
      "\n",
      "‚úì Training completed!\n",
      "‚è±Ô∏è  End time: 2025-11-18 21:28:13\n",
      "‚úì Model weights saved to: InceptionV3_output\\models\\model_weights.h5\n",
      "\n",
      "======================================================================\n",
      "EVALUATING InceptionV3\n",
      "======================================================================\n",
      "\n",
      "üìä Evaluating on test set...\n",
      "31/31 [==============================] - 4s 112ms/step - loss: 0.4571 - accuracy: 0.8198 - top_5_accuracy: 0.9654\n",
      "\n",
      "‚úì Test Results:\n",
      "  Loss: 0.4571\n",
      "  Accuracy: 0.8198\n",
      "  Top-5 Accuracy: 0.9654\n",
      "\n",
      "üìä Generating detailed metrics...\n",
      "‚úì Confusion matrix saved to: InceptionV3_output\\plots\\confusion_matrix.png\n",
      "‚úì Classification report saved to: InceptionV3_output\\logs\\classification_report.txt\n",
      "\n",
      "üìà Generating training plots...\n",
      "‚úì Training plots saved to: InceptionV3_output\\plots\\training_history.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.4571196436882019,\n",
       " 'test_accuracy': 0.8197556138038635,\n",
       " 'num_classes': 63,\n",
       " 'evaluation_time': '2025-11-18T21:28:29.666453',\n",
       " 'test_top5_accuracy': 0.9653767943382263}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inceptionv3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672aa702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "TRAINING VGG16\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "\n",
      "======================================================================\n",
      "DATA PREPARATION - VGG16\n",
      "======================================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images\n",
      "‚úì Number of classes: 63\n",
      "‚úì Classes: ['Missing_hole', 'Missing_hole_rotation', 'Mouse_bite', 'Mouse_bite_rotation', 'Open_circuit', 'Open_circuit_rotation', 'PCB_USED', 'Short', 'Short_rotation', 'Spur']...\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1961\n",
      "‚úì Test samples: 982\n",
      "\n",
      "======================================================================\n",
      "BUILDING VGG16 MODEL\n",
      "======================================================================\n",
      "\n",
      "‚úì Model built successfully!\n",
      "‚úì Total parameters: 128,032,639\n",
      "‚úì Base model frozen: True\n",
      "\n",
      "======================================================================\n",
      "TRAINING VGG16\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting training for 50 epochs...\n",
      "‚è±Ô∏è  Start time: 2025-11-18 21:31:20\n",
      "\n",
      "Epoch 1/50\n",
      "215/215 [==============================] - 63s 265ms/step - loss: 13.4528 - accuracy: 0.5024 - top_5_accuracy: 0.6494 - val_loss: 2.2588 - val_accuracy: 0.4691 - val_top_5_accuracy: 0.6609 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "215/215 [==============================] - 49s 226ms/step - loss: 2.4995 - accuracy: 0.5368 - top_5_accuracy: 0.6938 - val_loss: 1.5817 - val_accuracy: 0.6165 - val_top_5_accuracy: 0.7807 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "215/215 [==============================] - 50s 231ms/step - loss: 2.0679 - accuracy: 0.5872 - top_5_accuracy: 0.7426 - val_loss: 1.5259 - val_accuracy: 0.6303 - val_top_5_accuracy: 0.8016 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "215/215 [==============================] - 48s 222ms/step - loss: 1.9131 - accuracy: 0.6009 - top_5_accuracy: 0.7599 - val_loss: 1.4366 - val_accuracy: 0.6079 - val_top_5_accuracy: 0.8078 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "215/215 [==============================] - 47s 215ms/step - loss: 1.7789 - accuracy: 0.6073 - top_5_accuracy: 0.7607 - val_loss: 1.3854 - val_accuracy: 0.6339 - val_top_5_accuracy: 0.8287 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "215/215 [==============================] - 45s 208ms/step - loss: 1.7264 - accuracy: 0.6079 - top_5_accuracy: 0.7733 - val_loss: 1.4026 - val_accuracy: 0.6181 - val_top_5_accuracy: 0.8169 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "215/215 [==============================] - 41s 190ms/step - loss: 1.6811 - accuracy: 0.6140 - top_5_accuracy: 0.7802 - val_loss: 1.3549 - val_accuracy: 0.6094 - val_top_5_accuracy: 0.8205 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "215/215 [==============================] - 46s 214ms/step - loss: 1.6500 - accuracy: 0.6122 - top_5_accuracy: 0.7796 - val_loss: 1.3911 - val_accuracy: 0.6145 - val_top_5_accuracy: 0.8118 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "215/215 [==============================] - 42s 197ms/step - loss: 1.6436 - accuracy: 0.6121 - top_5_accuracy: 0.7821 - val_loss: 1.2930 - val_accuracy: 0.6232 - val_top_5_accuracy: 0.8276 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "215/215 [==============================] - 43s 198ms/step - loss: 1.6264 - accuracy: 0.6099 - top_5_accuracy: 0.7841 - val_loss: 1.3004 - val_accuracy: 0.6262 - val_top_5_accuracy: 0.8332 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "215/215 [==============================] - 42s 196ms/step - loss: 1.6541 - accuracy: 0.6147 - top_5_accuracy: 0.7835 - val_loss: 1.2818 - val_accuracy: 0.6390 - val_top_5_accuracy: 0.8348 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "215/215 [==============================] - 44s 201ms/step - loss: 1.7482 - accuracy: 0.6093 - top_5_accuracy: 0.7783 - val_loss: 1.4065 - val_accuracy: 0.6191 - val_top_5_accuracy: 0.8083 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "215/215 [==============================] - 41s 188ms/step - loss: 1.8006 - accuracy: 0.6039 - top_5_accuracy: 0.7650 - val_loss: 1.4236 - val_accuracy: 0.6318 - val_top_5_accuracy: 0.8251 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "215/215 [==============================] - 37s 174ms/step - loss: 1.7738 - accuracy: 0.6125 - top_5_accuracy: 0.7621 - val_loss: 1.3082 - val_accuracy: 0.6359 - val_top_5_accuracy: 0.8236 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "215/215 [==============================] - 35s 162ms/step - loss: 1.7422 - accuracy: 0.6084 - top_5_accuracy: 0.7604 - val_loss: 1.4158 - val_accuracy: 0.6181 - val_top_5_accuracy: 0.7960 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 1.7838 - accuracy: 0.6143 - top_5_accuracy: 0.7446\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 36s 167ms/step - loss: 1.7838 - accuracy: 0.6143 - top_5_accuracy: 0.7446 - val_loss: 1.5843 - val_accuracy: 0.6226 - val_top_5_accuracy: 0.7578 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "215/215 [==============================] - 34s 156ms/step - loss: 1.6819 - accuracy: 0.6146 - top_5_accuracy: 0.7519 - val_loss: 1.3072 - val_accuracy: 0.6308 - val_top_5_accuracy: 0.8174 - lr: 5.0000e-04\n",
      "Epoch 18/50\n",
      "215/215 [==============================] - 36s 169ms/step - loss: 1.5324 - accuracy: 0.6242 - top_5_accuracy: 0.7878 - val_loss: 1.2403 - val_accuracy: 0.6410 - val_top_5_accuracy: 0.8465 - lr: 5.0000e-04\n",
      "Epoch 19/50\n",
      "215/215 [==============================] - 37s 173ms/step - loss: 1.4951 - accuracy: 0.6249 - top_5_accuracy: 0.7968 - val_loss: 1.2435 - val_accuracy: 0.6344 - val_top_5_accuracy: 0.8389 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "215/215 [==============================] - 34s 159ms/step - loss: 1.4799 - accuracy: 0.6307 - top_5_accuracy: 0.7968 - val_loss: 1.2098 - val_accuracy: 0.6456 - val_top_5_accuracy: 0.8414 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "215/215 [==============================] - 35s 163ms/step - loss: 1.4836 - accuracy: 0.6235 - top_5_accuracy: 0.7897 - val_loss: 1.1991 - val_accuracy: 0.6456 - val_top_5_accuracy: 0.8358 - lr: 5.0000e-04\n",
      "Epoch 22/50\n",
      "215/215 [==============================] - 36s 167ms/step - loss: 1.4821 - accuracy: 0.6226 - top_5_accuracy: 0.7937 - val_loss: 1.2526 - val_accuracy: 0.6277 - val_top_5_accuracy: 0.8307 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "215/215 [==============================] - 37s 172ms/step - loss: 1.4671 - accuracy: 0.6201 - top_5_accuracy: 0.7994 - val_loss: 1.2074 - val_accuracy: 0.6405 - val_top_5_accuracy: 0.8531 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "215/215 [==============================] - 35s 164ms/step - loss: 1.4546 - accuracy: 0.6194 - top_5_accuracy: 0.7993 - val_loss: 1.2187 - val_accuracy: 0.6344 - val_top_5_accuracy: 0.8429 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "215/215 [==============================] - 36s 169ms/step - loss: 1.4562 - accuracy: 0.6267 - top_5_accuracy: 0.7999 - val_loss: 1.2084 - val_accuracy: 0.6486 - val_top_5_accuracy: 0.8414 - lr: 5.0000e-04\n",
      "Epoch 26/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 1.4419 - accuracy: 0.6291 - top_5_accuracy: 0.7993\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 35s 161ms/step - loss: 1.4419 - accuracy: 0.6291 - top_5_accuracy: 0.7993 - val_loss: 1.2818 - val_accuracy: 0.6135 - val_top_5_accuracy: 0.8353 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "215/215 [==============================] - 39s 179ms/step - loss: 1.4398 - accuracy: 0.6267 - top_5_accuracy: 0.7902 - val_loss: 1.2416 - val_accuracy: 0.6196 - val_top_5_accuracy: 0.8317 - lr: 2.5000e-04\n",
      "Epoch 28/50\n",
      "215/215 [==============================] - 38s 174ms/step - loss: 1.3896 - accuracy: 0.6281 - top_5_accuracy: 0.8055 - val_loss: 1.1839 - val_accuracy: 0.6400 - val_top_5_accuracy: 0.8373 - lr: 2.5000e-04\n",
      "Epoch 29/50\n",
      "215/215 [==============================] - 37s 171ms/step - loss: 1.3789 - accuracy: 0.6264 - top_5_accuracy: 0.8074 - val_loss: 1.2114 - val_accuracy: 0.6272 - val_top_5_accuracy: 0.8399 - lr: 2.5000e-04\n",
      "Epoch 30/50\n",
      "215/215 [==============================] - 36s 167ms/step - loss: 1.3645 - accuracy: 0.6370 - top_5_accuracy: 0.8112 - val_loss: 1.2137 - val_accuracy: 0.6298 - val_top_5_accuracy: 0.8429 - lr: 2.5000e-04\n",
      "Epoch 31/50\n",
      "215/215 [==============================] - 34s 156ms/step - loss: 1.4009 - accuracy: 0.6239 - top_5_accuracy: 0.8054 - val_loss: 1.1011 - val_accuracy: 0.6660 - val_top_5_accuracy: 0.8460 - lr: 2.5000e-04\n",
      "Epoch 32/50\n",
      "215/215 [==============================] - 36s 168ms/step - loss: 1.3762 - accuracy: 0.6303 - top_5_accuracy: 0.8044 - val_loss: 1.1697 - val_accuracy: 0.6359 - val_top_5_accuracy: 0.8521 - lr: 2.5000e-04\n",
      "Epoch 33/50\n",
      "215/215 [==============================] - 36s 165ms/step - loss: 1.3650 - accuracy: 0.6278 - top_5_accuracy: 0.8117 - val_loss: 1.1566 - val_accuracy: 0.6328 - val_top_5_accuracy: 0.8455 - lr: 2.5000e-04\n",
      "Epoch 34/50\n",
      "215/215 [==============================] - 35s 162ms/step - loss: 1.3599 - accuracy: 0.6328 - top_5_accuracy: 0.8082 - val_loss: 1.1184 - val_accuracy: 0.6430 - val_top_5_accuracy: 0.8419 - lr: 2.5000e-04\n",
      "Epoch 35/50\n",
      "215/215 [==============================] - 38s 174ms/step - loss: 1.3176 - accuracy: 0.6312 - top_5_accuracy: 0.8186 - val_loss: 1.1533 - val_accuracy: 0.6379 - val_top_5_accuracy: 0.8506 - lr: 2.5000e-04\n",
      "Epoch 36/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 1.3583 - accuracy: 0.6251 - top_5_accuracy: 0.8108\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "215/215 [==============================] - 36s 168ms/step - loss: 1.3583 - accuracy: 0.6251 - top_5_accuracy: 0.8108 - val_loss: 1.1835 - val_accuracy: 0.6242 - val_top_5_accuracy: 0.8485 - lr: 2.5000e-04\n",
      "Epoch 37/50\n",
      "215/215 [==============================] - 36s 167ms/step - loss: 1.3485 - accuracy: 0.6262 - top_5_accuracy: 0.8151 - val_loss: 1.1335 - val_accuracy: 0.6451 - val_top_5_accuracy: 0.8460 - lr: 1.2500e-04\n",
      "Epoch 38/50\n",
      "215/215 [==============================] - 36s 164ms/step - loss: 1.3263 - accuracy: 0.6243 - top_5_accuracy: 0.8133 - val_loss: 1.1220 - val_accuracy: 0.6364 - val_top_5_accuracy: 0.8557 - lr: 1.2500e-04\n",
      "Epoch 39/50\n",
      "215/215 [==============================] - 34s 156ms/step - loss: 1.3293 - accuracy: 0.6288 - top_5_accuracy: 0.8163 - val_loss: 1.1240 - val_accuracy: 0.6379 - val_top_5_accuracy: 0.8409 - lr: 1.2500e-04\n",
      "Epoch 40/50\n",
      "215/215 [==============================] - 35s 161ms/step - loss: 1.3350 - accuracy: 0.6265 - top_5_accuracy: 0.8150 - val_loss: 1.1548 - val_accuracy: 0.6298 - val_top_5_accuracy: 0.8531 - lr: 1.2500e-04\n",
      "Epoch 41/50\n",
      "215/215 [==============================] - 33s 151ms/step - loss: 1.3109 - accuracy: 0.6288 - top_5_accuracy: 0.8184 - val_loss: 1.0890 - val_accuracy: 0.6446 - val_top_5_accuracy: 0.8582 - lr: 1.2500e-04\n",
      "Epoch 42/50\n",
      "215/215 [==============================] - 44s 203ms/step - loss: 1.2899 - accuracy: 0.6315 - top_5_accuracy: 0.8214 - val_loss: 1.0950 - val_accuracy: 0.6497 - val_top_5_accuracy: 0.8587 - lr: 1.2500e-04\n",
      "Epoch 43/50\n",
      "215/215 [==============================] - 37s 174ms/step - loss: 1.2802 - accuracy: 0.6395 - top_5_accuracy: 0.8195 - val_loss: 1.1178 - val_accuracy: 0.6395 - val_top_5_accuracy: 0.8511 - lr: 1.2500e-04\n",
      "Epoch 44/50\n",
      "215/215 [==============================] - 35s 160ms/step - loss: 1.2991 - accuracy: 0.6331 - top_5_accuracy: 0.8216 - val_loss: 1.1020 - val_accuracy: 0.6486 - val_top_5_accuracy: 0.8628 - lr: 1.2500e-04\n",
      "Epoch 45/50\n",
      "215/215 [==============================] - 37s 172ms/step - loss: 1.2965 - accuracy: 0.6312 - top_5_accuracy: 0.8208 - val_loss: 1.0844 - val_accuracy: 0.6441 - val_top_5_accuracy: 0.8587 - lr: 1.2500e-04\n",
      "Epoch 46/50\n",
      "215/215 [==============================] - 34s 156ms/step - loss: 1.2698 - accuracy: 0.6334 - top_5_accuracy: 0.8213 - val_loss: 1.1088 - val_accuracy: 0.6349 - val_top_5_accuracy: 0.8547 - lr: 1.2500e-04\n",
      "Epoch 47/50\n",
      "215/215 [==============================] - 32s 149ms/step - loss: 1.2766 - accuracy: 0.6369 - top_5_accuracy: 0.8243 - val_loss: 1.0799 - val_accuracy: 0.6481 - val_top_5_accuracy: 0.8623 - lr: 1.2500e-04\n",
      "Epoch 48/50\n",
      "215/215 [==============================] - 37s 172ms/step - loss: 1.2849 - accuracy: 0.6309 - top_5_accuracy: 0.8133 - val_loss: 1.0582 - val_accuracy: 0.6517 - val_top_5_accuracy: 0.8700 - lr: 1.2500e-04\n",
      "Epoch 49/50\n",
      "215/215 [==============================] - 38s 176ms/step - loss: 1.2645 - accuracy: 0.6360 - top_5_accuracy: 0.8216 - val_loss: 1.0598 - val_accuracy: 0.6512 - val_top_5_accuracy: 0.8649 - lr: 1.2500e-04\n",
      "Epoch 50/50\n",
      "215/215 [==============================] - 35s 163ms/step - loss: 1.2731 - accuracy: 0.6296 - top_5_accuracy: 0.8210 - val_loss: 1.0749 - val_accuracy: 0.6446 - val_top_5_accuracy: 0.8582 - lr: 1.2500e-04\n",
      "\n",
      "‚úì Training completed!\n",
      "‚è±Ô∏è  End time: 2025-11-18 22:03:32\n",
      "‚úì Model weights saved to: VGG16_output\\models\\model_weights.h5\n",
      "\n",
      "======================================================================\n",
      "EVALUATING VGG16\n",
      "======================================================================\n",
      "\n",
      "üìä Evaluating on test set...\n",
      "31/31 [==============================] - 8s 238ms/step - loss: 1.1026 - accuracy: 0.6426 - top_5_accuracy: 0.8462\n",
      "\n",
      "‚úì Test Results:\n",
      "  Loss: 1.1026\n",
      "  Accuracy: 0.6426\n",
      "  Top-5 Accuracy: 0.8462\n",
      "\n",
      "üìä Generating detailed metrics...\n",
      "‚úì Confusion matrix saved to: VGG16_output\\plots\\confusion_matrix.png\n",
      "‚úì Classification report saved to: VGG16_output\\logs\\classification_report.txt\n",
      "\n",
      "üìà Generating training plots...\n",
      "‚úì Training plots saved to: VGG16_output\\plots\\training_history.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 1.1026062965393066,\n",
       " 'test_accuracy': 0.6425662040710449,\n",
       " 'num_classes': 63,\n",
       " 'evaluation_time': '2025-11-18T22:03:48.297608',\n",
       " 'test_top5_accuracy': 0.8462321758270264}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62cc08de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπ\n",
      "TRAINING VGG19\n",
      "üîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπüîπ\n",
      "\n",
      "======================================================================\n",
      "DATA PREPARATION - VGG19\n",
      "======================================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images\n",
      "‚úì Number of classes: 63\n",
      "‚úì Classes: ['Missing_hole', 'Missing_hole_rotation', 'Mouse_bite', 'Mouse_bite_rotation', 'Open_circuit', 'Open_circuit_rotation', 'PCB_USED', 'Short', 'Short_rotation', 'Spur']...\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1961\n",
      "‚úì Test samples: 982\n",
      "\n",
      "======================================================================\n",
      "BUILDING VGG19 MODEL\n",
      "======================================================================\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80134624/80134624 [==============================] - 9s 0us/step\n",
      "\n",
      "‚úì Model built successfully!\n",
      "‚úì Total parameters: 133,342,335\n",
      "‚úì Base model frozen: True\n",
      "\n",
      "======================================================================\n",
      "TRAINING VGG19\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting training for 30 epochs...\n",
      "‚è±Ô∏è  Start time: 2025-11-18 11:36:36\n",
      "\n",
      "Epoch 1/30\n",
      "215/215 [==============================] - 117s 502ms/step - loss: 12.7835 - accuracy: 0.5088 - top_5_accuracy: 0.6689 - val_loss: 2.1291 - val_accuracy: 0.6022 - val_top_5_accuracy: 0.7425 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "215/215 [==============================] - 68s 315ms/step - loss: 2.5216 - accuracy: 0.5441 - top_5_accuracy: 0.7101 - val_loss: 1.6471 - val_accuracy: 0.6124 - val_top_5_accuracy: 0.7996 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "215/215 [==============================] - 63s 293ms/step - loss: 2.0278 - accuracy: 0.5907 - top_5_accuracy: 0.7611 - val_loss: 1.5485 - val_accuracy: 0.6068 - val_top_5_accuracy: 0.8154 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "215/215 [==============================] - 57s 265ms/step - loss: 1.8355 - accuracy: 0.5999 - top_5_accuracy: 0.7806 - val_loss: 1.5705 - val_accuracy: 0.6114 - val_top_5_accuracy: 0.8302 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "215/215 [==============================] - 57s 262ms/step - loss: 1.7468 - accuracy: 0.6079 - top_5_accuracy: 0.7865 - val_loss: 1.2861 - val_accuracy: 0.6262 - val_top_5_accuracy: 0.8455 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "215/215 [==============================] - 61s 283ms/step - loss: 1.6753 - accuracy: 0.6038 - top_5_accuracy: 0.7939 - val_loss: 1.3647 - val_accuracy: 0.6175 - val_top_5_accuracy: 0.8399 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "215/215 [==============================] - 69s 322ms/step - loss: 1.6556 - accuracy: 0.6105 - top_5_accuracy: 0.7945 - val_loss: 1.2915 - val_accuracy: 0.6323 - val_top_5_accuracy: 0.8429 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "215/215 [==============================] - 61s 283ms/step - loss: 1.6317 - accuracy: 0.6141 - top_5_accuracy: 0.8010 - val_loss: 1.2978 - val_accuracy: 0.6145 - val_top_5_accuracy: 0.8424 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "215/215 [==============================] - 59s 272ms/step - loss: 1.6271 - accuracy: 0.6051 - top_5_accuracy: 0.8042 - val_loss: 1.3472 - val_accuracy: 0.6293 - val_top_5_accuracy: 0.8480 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "215/215 [==============================] - 55s 257ms/step - loss: 1.6249 - accuracy: 0.6080 - top_5_accuracy: 0.8060 - val_loss: 1.2640 - val_accuracy: 0.6334 - val_top_5_accuracy: 0.8434 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "215/215 [==============================] - 67s 310ms/step - loss: 1.5644 - accuracy: 0.6141 - top_5_accuracy: 0.8061 - val_loss: 1.2404 - val_accuracy: 0.6435 - val_top_5_accuracy: 0.8557 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "215/215 [==============================] - 67s 310ms/step - loss: 1.5989 - accuracy: 0.6108 - top_5_accuracy: 0.7945 - val_loss: 1.5165 - val_accuracy: 0.6232 - val_top_5_accuracy: 0.8241 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "215/215 [==============================] - 71s 329ms/step - loss: 1.6630 - accuracy: 0.6077 - top_5_accuracy: 0.8019 - val_loss: 1.5521 - val_accuracy: 0.6043 - val_top_5_accuracy: 0.8378 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "215/215 [==============================] - 54s 250ms/step - loss: 1.7756 - accuracy: 0.6095 - top_5_accuracy: 0.7899 - val_loss: 1.4823 - val_accuracy: 0.6211 - val_top_5_accuracy: 0.7710 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "215/215 [==============================] - 59s 273ms/step - loss: 1.8988 - accuracy: 0.6004 - top_5_accuracy: 0.7716 - val_loss: 1.7206 - val_accuracy: 0.6221 - val_top_5_accuracy: 0.7970 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "215/215 [==============================] - ETA: 0s - loss: 1.8781 - accuracy: 0.6074 - top_5_accuracy: 0.7607\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 63s 291ms/step - loss: 1.8781 - accuracy: 0.6074 - top_5_accuracy: 0.7607 - val_loss: 1.4848 - val_accuracy: 0.6170 - val_top_5_accuracy: 0.7874 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "215/215 [==============================] - 64s 296ms/step - loss: 1.6495 - accuracy: 0.6163 - top_5_accuracy: 0.7863 - val_loss: 1.4013 - val_accuracy: 0.5854 - val_top_5_accuracy: 0.8302 - lr: 5.0000e-04\n",
      "Epoch 18/30\n",
      "215/215 [==============================] - 59s 272ms/step - loss: 1.5305 - accuracy: 0.6200 - top_5_accuracy: 0.8111 - val_loss: 1.2302 - val_accuracy: 0.6344 - val_top_5_accuracy: 0.8547 - lr: 5.0000e-04\n",
      "Epoch 19/30\n",
      "215/215 [==============================] - 60s 279ms/step - loss: 1.4693 - accuracy: 0.6246 - top_5_accuracy: 0.8153 - val_loss: 1.2651 - val_accuracy: 0.6272 - val_top_5_accuracy: 0.8521 - lr: 5.0000e-04\n",
      "Epoch 20/30\n",
      "215/215 [==============================] - 56s 258ms/step - loss: 1.4330 - accuracy: 0.6283 - top_5_accuracy: 0.8213 - val_loss: 1.1855 - val_accuracy: 0.6384 - val_top_5_accuracy: 0.8547 - lr: 5.0000e-04\n",
      "Epoch 21/30\n",
      "215/215 [==============================] - 59s 274ms/step - loss: 1.4211 - accuracy: 0.6208 - top_5_accuracy: 0.8184 - val_loss: 1.1691 - val_accuracy: 0.6395 - val_top_5_accuracy: 0.8491 - lr: 5.0000e-04\n",
      "Epoch 22/30\n",
      "215/215 [==============================] - 54s 252ms/step - loss: 1.4301 - accuracy: 0.6202 - top_5_accuracy: 0.8141 - val_loss: 1.2265 - val_accuracy: 0.6283 - val_top_5_accuracy: 0.8465 - lr: 5.0000e-04\n",
      "Epoch 23/30\n",
      "215/215 [==============================] - 51s 239ms/step - loss: 1.4172 - accuracy: 0.6168 - top_5_accuracy: 0.8242 - val_loss: 1.2312 - val_accuracy: 0.6206 - val_top_5_accuracy: 0.8679 - lr: 5.0000e-04\n",
      "Epoch 24/30\n",
      "215/215 [==============================] - 43s 198ms/step - loss: 1.4083 - accuracy: 0.6216 - top_5_accuracy: 0.8168 - val_loss: 1.1545 - val_accuracy: 0.6318 - val_top_5_accuracy: 0.8511 - lr: 5.0000e-04\n",
      "Epoch 25/30\n",
      "215/215 [==============================] - 42s 193ms/step - loss: 1.3768 - accuracy: 0.6313 - top_5_accuracy: 0.8302 - val_loss: 1.1359 - val_accuracy: 0.6527 - val_top_5_accuracy: 0.8587 - lr: 5.0000e-04\n",
      "Epoch 26/30\n",
      "215/215 [==============================] - 45s 209ms/step - loss: 1.4041 - accuracy: 0.6302 - top_5_accuracy: 0.8210 - val_loss: 1.2348 - val_accuracy: 0.6201 - val_top_5_accuracy: 0.8440 - lr: 5.0000e-04\n",
      "Epoch 27/30\n",
      "215/215 [==============================] - 43s 197ms/step - loss: 1.3863 - accuracy: 0.6268 - top_5_accuracy: 0.8162 - val_loss: 1.2084 - val_accuracy: 0.6232 - val_top_5_accuracy: 0.8475 - lr: 5.0000e-04\n",
      "Epoch 28/30\n",
      "215/215 [==============================] - 44s 204ms/step - loss: 1.3774 - accuracy: 0.6309 - top_5_accuracy: 0.8269 - val_loss: 1.1527 - val_accuracy: 0.6481 - val_top_5_accuracy: 0.8475 - lr: 5.0000e-04\n",
      "Epoch 29/30\n",
      "215/215 [==============================] - 43s 200ms/step - loss: 1.3848 - accuracy: 0.6275 - top_5_accuracy: 0.8236 - val_loss: 1.1948 - val_accuracy: 0.6405 - val_top_5_accuracy: 0.8491 - lr: 5.0000e-04\n",
      "Epoch 30/30\n",
      "215/215 [==============================] - ETA: 0s - loss: 1.4003 - accuracy: 0.6348 - top_5_accuracy: 0.8237\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 40s 183ms/step - loss: 1.4003 - accuracy: 0.6348 - top_5_accuracy: 0.8237 - val_loss: 1.1609 - val_accuracy: 0.6395 - val_top_5_accuracy: 0.8542 - lr: 5.0000e-04\n",
      "\n",
      "‚úì Training completed!\n",
      "‚è±Ô∏è  End time: 2025-11-18 12:05:47\n",
      "‚úì Model weights saved to: VGG19_output\\models\\model_weights.h5\n",
      "\n",
      "======================================================================\n",
      "EVALUATING VGG19\n",
      "======================================================================\n",
      "\n",
      "üìä Evaluating on test set...\n",
      "31/31 [==============================] - 8s 241ms/step - loss: 1.1955 - accuracy: 0.6273 - top_5_accuracy: 0.8381\n",
      "\n",
      "‚úì Test Results:\n",
      "  Loss: 1.1955\n",
      "  Accuracy: 0.6273\n",
      "  Top-5 Accuracy: 0.8381\n",
      "\n",
      "üìä Generating detailed metrics...\n",
      "‚úì Confusion matrix saved to: VGG19_output\\plots\\confusion_matrix.png\n",
      "‚úì Classification report saved to: VGG19_output\\logs\\classification_report.txt\n",
      "\n",
      "üìà Generating training plots...\n",
      "‚úì Training plots saved to: VGG19_output\\plots\\training_history.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 1.1955108642578125,\n",
       " 'test_accuracy': 0.6272912621498108,\n",
       " 'num_classes': 63,\n",
       " 'evaluation_time': '2025-11-18T12:06:03.325095',\n",
       " 'test_top5_accuracy': 0.8380855321884155}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vgg19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bbc3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import Xception\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "class XceptionDefectDetector:\n",
    "    \"\"\"Xception for defect detection\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, img_height=299, img_width=299):\n",
    "        # Xception requires minimum 71x71, but 299x299 is optimal\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.model_name = \"Xception\"\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.class_names = []\n",
    "        self.num_classes = 0\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir = Path(\"Xception_output\")\n",
    "        self.model_dir = self.output_dir / \"models\"\n",
    "        self.plots_dir = self.output_dir / \"plots\"\n",
    "        self.logs_dir = self.output_dir / \"logs\"\n",
    "        \n",
    "        for dir_path in [self.output_dir, self.model_dir, self.plots_dir, self.logs_dir]:\n",
    "            dir_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def prepare_data(self, validation_split=0.2, test_split=0.1, batch_size=32):\n",
    "        \"\"\"Prepare dataset with efficient TensorFlow pipeline\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"DATA PREPARATION - {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Get all image paths and labels\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        print(\"\\nüìÅ Scanning dataset...\")\n",
    "        for class_dir in self.dataset_path.rglob(\"*\"):\n",
    "            if class_dir.is_dir():\n",
    "                images = (list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\")) + \n",
    "                         list(class_dir.glob(\"*.jpeg\")) + list(class_dir.glob(\"*.bmp\")))\n",
    "                \n",
    "                if images:\n",
    "                    class_name = class_dir.name\n",
    "                    if class_name not in self.class_names:\n",
    "                        self.class_names.append(class_name)\n",
    "                    \n",
    "                    for img_path in images:\n",
    "                        image_paths.append(str(img_path))\n",
    "                        labels.append(class_name)\n",
    "        \n",
    "        self.class_names.sort()\n",
    "        self.num_classes = len(self.class_names)\n",
    "        print(f\"‚úì Found {len(image_paths)} images\")\n",
    "        print(f\"‚úì Number of classes: {self.num_classes}\")\n",
    "        print(f\"‚úì Classes: {self.class_names[:10]}{'...' if len(self.class_names) > 10 else ''}\")\n",
    "        \n",
    "        # Create label mapping\n",
    "        label_to_index = {label: idx for idx, label in enumerate(self.class_names)}\n",
    "        labels_encoded = [label_to_index[label] for label in labels]\n",
    "        \n",
    "        # Create tf.data.Dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels_encoded))\n",
    "        dataset = dataset.shuffle(len(image_paths), seed=42)\n",
    "        \n",
    "        # Calculate splits\n",
    "        train_size = int(len(image_paths) * (1 - validation_split - test_split))\n",
    "        val_size = int(len(image_paths) * validation_split)\n",
    "        test_size = len(image_paths) - train_size - val_size\n",
    "        \n",
    "        # Split datasets\n",
    "        train_ds = dataset.take(train_size)\n",
    "        val_ds = dataset.skip(train_size).take(val_size)\n",
    "        test_ds = dataset.skip(train_size + val_size)\n",
    "        \n",
    "        # Process datasets\n",
    "        self.train_dataset = train_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=True),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.val_dataset = val_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=False),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.test_dataset = test_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=False),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        print(f\"\\n‚úì Train samples: {train_size}\")\n",
    "        print(f\"‚úì Validation samples: {val_size}\")\n",
    "        print(f\"‚úì Test samples: {test_size}\")\n",
    "        \n",
    "        return train_size, val_size, test_size\n",
    "    \n",
    "    def _load_and_preprocess(self, image_path, label, augment=False):\n",
    "        \"\"\"Load and preprocess image\"\"\"\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, [self.img_height, self.img_width])\n",
    "        \n",
    "        # Data augmentation\n",
    "        if augment:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_flip_up_down(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.rot90(img, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "        \n",
    "        img = tf.clip_by_value(img, 0.0, 255.0)\n",
    "        img = img / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        label_onehot = tf.one_hot(label, self.num_classes)\n",
    "        return img, label_onehot\n",
    "    \n",
    "    def build_model(self, learning_rate=0.001, dropout_rate=0.3):\n",
    "        \"\"\"Build Xception model with transfer learning\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"BUILDING XCEPTION MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load pre-trained Xception\n",
    "        base_model = Xception(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(self.img_height, self.img_width, 3)\n",
    "        )\n",
    "        \n",
    "        # Freeze base model initially\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Build model\n",
    "        inputs = keras.Input(shape=(self.img_height, self.img_width, 3))\n",
    "        \n",
    "        # Xception preprocessing (scale to [-1, 1])\n",
    "        x = layers.Rescaling(scale=2.0, offset=-1.0)(inputs)\n",
    "        \n",
    "        x = base_model(x, training=False)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Dense layers with dropout\n",
    "        x = layers.Dense(1024, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate / 2)(x)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate / 4)(x)\n",
    "        \n",
    "        # Output layer\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(\"\\n‚úì Xception model built successfully!\")\n",
    "        print(f\"‚úì Total parameters: {model.count_params():,}\")\n",
    "        print(f\"‚úì Trainable parameters: {sum([np.prod(v.shape) for v in model.trainable_weights]):,}\")\n",
    "        print(f\"‚úì Base model frozen: {not base_model.trainable}\")\n",
    "        print(f\"‚úì Input size: {self.img_height}x{self.img_width}\")\n",
    "        print(\"\\nüìù Model Architecture Highlights:\")\n",
    "        print(\"  - Depthwise Separable Convolutions\")\n",
    "        print(\"  - 36 Convolutional Layers\")\n",
    "        print(\"  - Efficient parameter usage\")\n",
    "        print(\"  - Excellent for transfer learning\")\n",
    "        \n",
    "        # Save architecture\n",
    "        with open(self.model_dir / 'model_architecture.txt', 'w') as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def unfreeze_and_fine_tune(self, fine_tune_from_layer=100, learning_rate=0.0001):\n",
    "        \"\"\"\n",
    "        Unfreeze layers for fine-tuning\n",
    "        \n",
    "        Args:\n",
    "            fine_tune_from_layer: Layer index to start unfreezing from\n",
    "                                  Xception has 132 layers total\n",
    "                                  Good starting points: 100, 116, 126\n",
    "            learning_rate: Lower learning rate for fine-tuning\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîì Unfreezing layers from layer {fine_tune_from_layer}...\")\n",
    "        \n",
    "        base_model = self.model.layers[2]  # Get Xception base\n",
    "        base_model.trainable = True\n",
    "        \n",
    "        # Freeze layers before fine_tune_from_layer\n",
    "        for layer in base_model.layers[:fine_tune_from_layer]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Count trainable parameters\n",
    "        trainable_params = sum([np.prod(v.shape) for v in self.model.trainable_weights])\n",
    "        \n",
    "        # Recompile with lower learning rate\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Fine-tuning enabled!\")\n",
    "        print(f\"‚úì Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"‚úì Learning rate: {learning_rate}\")\n",
    "    \n",
    "    def train(self, epochs=50, early_stopping_patience=10):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"TRAINING {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built. Call build_model() first.\")\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=early_stopping_patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.CSVLogger(\n",
    "                str(self.logs_dir / 'training_log.csv')\n",
    "            ),\n",
    "            keras.callbacks.TensorBoard(\n",
    "                log_dir=str(self.logs_dir / 'tensorboard'),\n",
    "                histogram_freq=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting training for {epochs} epochs...\")\n",
    "        print(f\"‚è±Ô∏è  Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            self.train_dataset,\n",
    "            epochs=epochs,\n",
    "            validation_data=self.val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úì Training completed!\")\n",
    "        print(f\"‚è±Ô∏è  End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Save model weights\n",
    "        self.model.save_weights(str(self.model_dir / 'model_weights.h5'))\n",
    "        print(f\"‚úì Model weights saved to: {self.model_dir / 'model_weights.h5'}\")\n",
    "        \n",
    "        # Save training history\n",
    "        history_dict = {key: [float(val) for val in values] \n",
    "                       for key, values in self.history.history.items()}\n",
    "        with open(self.logs_dir / 'training_history.json', 'w') as f:\n",
    "            json.dump(history_dict, f, indent=2)\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"EVALUATING {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìä Evaluating on test set...\")\n",
    "        test_results = self.model.evaluate(self.test_dataset, verbose=1)\n",
    "        \n",
    "        print(\"\\n‚úì Test Results:\")\n",
    "        print(f\"  Loss: {test_results[0]:.4f}\")\n",
    "        print(f\"  Accuracy: {test_results[1]:.4f}\")\n",
    "        print(f\"  Top-5 Accuracy: {test_results[2]:.4f}\")\n",
    "        \n",
    "        # Get predictions for confusion matrix\n",
    "        print(\"\\nüìä Generating detailed metrics...\")\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_pred_probs = []\n",
    "        \n",
    "        for images, labels in self.test_dataset:\n",
    "            predictions = self.model.predict(images, verbose=0)\n",
    "            y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "            y_pred_probs.extend(predictions)\n",
    "        \n",
    "        # Calculate per-class accuracy\n",
    "        per_class_accuracy = {}\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            class_mask = np.array(y_true) == i\n",
    "            if class_mask.sum() > 0:\n",
    "                class_acc = (np.array(y_pred)[class_mask] == i).mean()\n",
    "                per_class_accuracy[class_name] = float(class_acc)\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'test_loss': float(test_results[0]),\n",
    "            'test_accuracy': float(test_results[1]),\n",
    "            'test_top5_accuracy': float(test_results[2]),\n",
    "            'num_classes': self.num_classes,\n",
    "            'per_class_accuracy': per_class_accuracy,\n",
    "            'evaluation_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(self.logs_dir / 'test_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        self._plot_confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Generate classification report\n",
    "        self._save_classification_report(y_true, y_pred)\n",
    "        \n",
    "        # Plot top-k accuracy curve\n",
    "        self._plot_topk_accuracy(y_true, y_pred_probs)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _plot_confusion_matrix(self, y_true, y_pred):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(max(12, self.num_classes * 0.5), max(10, self.num_classes * 0.4)))\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        sns.heatmap(cm_normalized, annot=False, cmap='Blues', cbar=True,\n",
    "                    xticklabels=self.class_names if self.num_classes <= 20 else False,\n",
    "                    yticklabels=self.class_names if self.num_classes <= 20 else False,\n",
    "                    square=True)\n",
    "        \n",
    "        plt.title(f'{self.model_name} - Confusion Matrix (Normalized)', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Confusion matrix saved to: {self.plots_dir / 'confusion_matrix.png'}\")\n",
    "    \n",
    "    def _save_classification_report(self, y_true, y_pred):\n",
    "        \"\"\"Save classification report\"\"\"\n",
    "        report = classification_report(y_true, y_pred, target_names=self.class_names, \n",
    "                                      zero_division=0)\n",
    "        \n",
    "        with open(self.logs_dir / 'classification_report.txt', 'w') as f:\n",
    "            f.write(f\"{self.model_name} - Classification Report\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"‚úì Classification report saved to: {self.logs_dir / 'classification_report.txt'}\")\n",
    "    \n",
    "    def _plot_topk_accuracy(self, y_true, y_pred_probs):\n",
    "        \"\"\"Plot top-k accuracy curve\"\"\"\n",
    "        y_pred_probs = np.array(y_pred_probs)\n",
    "        y_true = np.array(y_true)\n",
    "        \n",
    "        k_values = range(1, min(11, self.num_classes + 1))\n",
    "        top_k_accs = []\n",
    "        \n",
    "        for k in k_values:\n",
    "            top_k_preds = np.argsort(y_pred_probs, axis=1)[:, -k:]\n",
    "            correct = np.array([y_true[i] in top_k_preds[i] for i in range(len(y_true))])\n",
    "            top_k_accs.append(correct.mean())\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(k_values, np.array(top_k_accs) * 100, marker='o', linewidth=2, markersize=8)\n",
    "        plt.xlabel('K', fontsize=12)\n",
    "        plt.ylabel('Top-K Accuracy (%)', fontsize=12)\n",
    "        plt.title(f'{self.model_name} - Top-K Accuracy', fontsize=14, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'topk_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Top-K accuracy plot saved to: {self.plots_dir / 'topk_accuracy.png'}\")\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"‚ö†Ô∏è  No training history available\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüìà Generating training plots...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        axes[0, 0].plot(self.history.history['accuracy'], label='Train Accuracy', \n",
    "                       linewidth=2, color='#3498db')\n",
    "        axes[0, 0].plot(self.history.history['val_accuracy'], label='Val Accuracy', \n",
    "                       linewidth=2, color='#e74c3c')\n",
    "        axes[0, 0].set_title(f'{self.model_name} - Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Accuracy')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[0, 1].plot(self.history.history['loss'], label='Train Loss', \n",
    "                       linewidth=2, color='#3498db')\n",
    "        axes[0, 1].plot(self.history.history['val_loss'], label='Val Loss', \n",
    "                       linewidth=2, color='#e74c3c')\n",
    "        axes[0, 1].set_title(f'{self.model_name} - Loss', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot top-5 accuracy\n",
    "        axes[1, 0].plot(self.history.history['top_5_accuracy'], label='Train Top-5', \n",
    "                       linewidth=2, color='#3498db')\n",
    "        axes[1, 0].plot(self.history.history['val_top_5_accuracy'], label='Val Top-5', \n",
    "                       linewidth=2, color='#e74c3c')\n",
    "        axes[1, 0].set_title(f'{self.model_name} - Top-5 Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Top-5 Accuracy')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot learning rate if available\n",
    "        if 'lr' in self.history.history:\n",
    "            axes[1, 1].plot(self.history.history['lr'], linewidth=2, color='#2ecc71')\n",
    "            axes[1, 1].set_title(f'{self.model_name} - Learning Rate', fontsize=14, fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Learning Rate')\n",
    "            axes[1, 1].set_yscale('log')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            # Show summary statistics\n",
    "            final_acc = self.history.history['val_accuracy'][-1]\n",
    "            best_acc = max(self.history.history['val_accuracy'])\n",
    "            axes[1, 1].text(0.5, 0.6, f'Final Val Acc: {final_acc:.4f}', \n",
    "                           ha='center', fontsize=14, fontweight='bold')\n",
    "            axes[1, 1].text(0.5, 0.4, f'Best Val Acc: {best_acc:.4f}', \n",
    "                           ha='center', fontsize=14, fontweight='bold')\n",
    "            axes[1, 1].set_title('Summary', fontsize=14, fontweight='bold')\n",
    "            axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Training plots saved to: {self.plots_dir / 'training_history.png'}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def train_xception(dataset_path=\"final_merged_dataset\", epochs=50, batch_size=32):\n",
    "    \"\"\"Train Xception model\"\"\"\n",
    "    print(\"\\n\\n\" + \"üî∂\"*35)\n",
    "    print(\"TRAINING XCEPTION\")\n",
    "    print(\"üî∂\"*35)\n",
    "    \n",
    "    model = XceptionDefectDetector(dataset_path)\n",
    "    model.prepare_data(batch_size=batch_size)\n",
    "    model.build_model(learning_rate=0.001, dropout_rate=0.3)\n",
    "    model.train(epochs=epochs, early_stopping_patience=10)\n",
    "    results = model.evaluate()\n",
    "    model.plot_training_history()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ XCEPTION TRAINING COMPLETED!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nüìä Final Results:\")\n",
    "    print(f\"  Test Accuracy: {results['test_accuracy']*100:.2f}%\")\n",
    "    print(f\"  Test Loss: {results['test_loss']:.4f}\")\n",
    "    print(f\"  Top-5 Accuracy: {results['test_top5_accuracy']*100:.2f}%\")\n",
    "    print(f\"\\nüìÅ All outputs saved to: Xception_output/\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# For fine-tuning after initial training\n",
    "def fine_tune_xception(model, epochs=20):\n",
    "    \"\"\"\n",
    "    Fine-tune Xception model by unfreezing layers\n",
    "    Call this after initial training for better performance\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINE-TUNING XCEPTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Unfreeze from layer 100 (out of 132 total layers)\n",
    "    model.unfreeze_and_fine_tune(fine_tune_from_layer=100, learning_rate=0.0001)\n",
    "    \n",
    "    # Continue training with lower learning rate\n",
    "    model.train(epochs=epochs, early_stopping_patience=7)\n",
    "    results = model.evaluate()\n",
    "    model.plot_training_history()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3069d347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂\n",
      "TRAINING XCEPTION\n",
      "üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂üî∂\n",
      "\n",
      "======================================================================\n",
      "DATA PREPARATION - Xception\n",
      "======================================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images\n",
      "‚úì Number of classes: 63\n",
      "‚úì Classes: ['Missing_hole', 'Missing_hole_rotation', 'Mouse_bite', 'Mouse_bite_rotation', 'Open_circuit', 'Open_circuit_rotation', 'PCB_USED', 'Short', 'Short_rotation', 'Spur']...\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1961\n",
      "‚úì Test samples: 982\n",
      "\n",
      "======================================================================\n",
      "BUILDING XCEPTION MODEL\n",
      "======================================================================\n",
      "\n",
      "‚úì Xception model built successfully!\n",
      "‚úì Total parameters: 23,640,167\n",
      "‚úì Trainable parameters: 2,774,591\n",
      "‚úì Base model frozen: True\n",
      "‚úì Input size: 299x299\n",
      "\n",
      "üìù Model Architecture Highlights:\n",
      "  - Depthwise Separable Convolutions\n",
      "  - 36 Convolutional Layers\n",
      "  - Efficient parameter usage\n",
      "  - Excellent for transfer learning\n",
      "\n",
      "======================================================================\n",
      "TRAINING Xception\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting training for 50 epochs...\n",
      "‚è±Ô∏è  Start time: 2025-11-18 13:02:55\n",
      "\n",
      "Epoch 1/50\n",
      "215/215 [==============================] - 59s 266ms/step - loss: 1.6032 - accuracy: 0.6242 - top_5_accuracy: 0.7899 - val_loss: 1.7180 - val_accuracy: 0.6298 - val_top_5_accuracy: 0.8613 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "215/215 [==============================] - 53s 242ms/step - loss: 1.2823 - accuracy: 0.6436 - top_5_accuracy: 0.8612 - val_loss: 1.0885 - val_accuracy: 0.6716 - val_top_5_accuracy: 0.8924 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "215/215 [==============================] - 52s 238ms/step - loss: 1.1655 - accuracy: 0.6632 - top_5_accuracy: 0.8794 - val_loss: 0.9695 - val_accuracy: 0.7022 - val_top_5_accuracy: 0.9194 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "215/215 [==============================] - 48s 219ms/step - loss: 1.0873 - accuracy: 0.6759 - top_5_accuracy: 0.8928 - val_loss: 1.0299 - val_accuracy: 0.6711 - val_top_5_accuracy: 0.9169 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "215/215 [==============================] - 45s 209ms/step - loss: 1.0620 - accuracy: 0.6788 - top_5_accuracy: 0.9001 - val_loss: 0.9132 - val_accuracy: 0.7063 - val_top_5_accuracy: 0.9271 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "215/215 [==============================] - 44s 203ms/step - loss: 1.0195 - accuracy: 0.6899 - top_5_accuracy: 0.9076 - val_loss: 0.9603 - val_accuracy: 0.6905 - val_top_5_accuracy: 0.9255 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "215/215 [==============================] - 45s 207ms/step - loss: 1.0004 - accuracy: 0.6909 - top_5_accuracy: 0.9101 - val_loss: 0.8730 - val_accuracy: 0.7129 - val_top_5_accuracy: 0.9266 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "215/215 [==============================] - 47s 216ms/step - loss: 0.9928 - accuracy: 0.6913 - top_5_accuracy: 0.9109 - val_loss: 0.9135 - val_accuracy: 0.6966 - val_top_5_accuracy: 0.9225 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "215/215 [==============================] - 42s 196ms/step - loss: 0.9753 - accuracy: 0.6921 - top_5_accuracy: 0.9181 - val_loss: 0.8746 - val_accuracy: 0.7149 - val_top_5_accuracy: 0.9250 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "215/215 [==============================] - 41s 190ms/step - loss: 0.9538 - accuracy: 0.6909 - top_5_accuracy: 0.9176 - val_loss: 0.8217 - val_accuracy: 0.7221 - val_top_5_accuracy: 0.9429 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "215/215 [==============================] - 44s 203ms/step - loss: 0.9343 - accuracy: 0.7018 - top_5_accuracy: 0.9194 - val_loss: 0.8010 - val_accuracy: 0.7297 - val_top_5_accuracy: 0.9291 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "215/215 [==============================] - 44s 202ms/step - loss: 0.9087 - accuracy: 0.7084 - top_5_accuracy: 0.9282 - val_loss: 0.8354 - val_accuracy: 0.7190 - val_top_5_accuracy: 0.9373 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "215/215 [==============================] - 42s 190ms/step - loss: 0.9129 - accuracy: 0.7055 - top_5_accuracy: 0.9269 - val_loss: 0.8064 - val_accuracy: 0.7206 - val_top_5_accuracy: 0.9403 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "215/215 [==============================] - 40s 185ms/step - loss: 0.9084 - accuracy: 0.7152 - top_5_accuracy: 0.9257 - val_loss: 0.8083 - val_accuracy: 0.7415 - val_top_5_accuracy: 0.9414 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "215/215 [==============================] - 42s 194ms/step - loss: 0.8944 - accuracy: 0.7094 - top_5_accuracy: 0.9263 - val_loss: 0.7608 - val_accuracy: 0.7506 - val_top_5_accuracy: 0.9434 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "215/215 [==============================] - 41s 189ms/step - loss: 0.8648 - accuracy: 0.7196 - top_5_accuracy: 0.9289 - val_loss: 0.7130 - val_accuracy: 0.7527 - val_top_5_accuracy: 0.9607 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "215/215 [==============================] - 40s 185ms/step - loss: 0.8614 - accuracy: 0.7168 - top_5_accuracy: 0.9312 - val_loss: 0.7553 - val_accuracy: 0.7384 - val_top_5_accuracy: 0.9414 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "215/215 [==============================] - 41s 189ms/step - loss: 0.8474 - accuracy: 0.7235 - top_5_accuracy: 0.9333 - val_loss: 0.7708 - val_accuracy: 0.7374 - val_top_5_accuracy: 0.9500 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "215/215 [==============================] - 40s 185ms/step - loss: 0.8408 - accuracy: 0.7266 - top_5_accuracy: 0.9349 - val_loss: 0.7756 - val_accuracy: 0.7323 - val_top_5_accuracy: 0.9419 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "215/215 [==============================] - 40s 185ms/step - loss: 0.8365 - accuracy: 0.7279 - top_5_accuracy: 0.9343 - val_loss: 0.7802 - val_accuracy: 0.7460 - val_top_5_accuracy: 0.9490 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.8543 - accuracy: 0.7186 - top_5_accuracy: 0.9347\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 41s 189ms/step - loss: 0.8543 - accuracy: 0.7186 - top_5_accuracy: 0.9347 - val_loss: 0.7649 - val_accuracy: 0.7471 - val_top_5_accuracy: 0.9449 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "215/215 [==============================] - 41s 186ms/step - loss: 0.7876 - accuracy: 0.7343 - top_5_accuracy: 0.9428 - val_loss: 0.7325 - val_accuracy: 0.7420 - val_top_5_accuracy: 0.9490 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 0.7655 - accuracy: 0.7327 - top_5_accuracy: 0.9425 - val_loss: 0.6495 - val_accuracy: 0.7664 - val_top_5_accuracy: 0.9658 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "215/215 [==============================] - 41s 189ms/step - loss: 0.7598 - accuracy: 0.7366 - top_5_accuracy: 0.9471 - val_loss: 0.6579 - val_accuracy: 0.7557 - val_top_5_accuracy: 0.9541 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 0.7160 - accuracy: 0.7516 - top_5_accuracy: 0.9502 - val_loss: 0.6092 - val_accuracy: 0.7787 - val_top_5_accuracy: 0.9623 - lr: 5.0000e-04\n",
      "Epoch 26/50\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 0.7140 - accuracy: 0.7519 - top_5_accuracy: 0.9455 - val_loss: 0.6679 - val_accuracy: 0.7578 - val_top_5_accuracy: 0.9597 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "215/215 [==============================] - 41s 188ms/step - loss: 0.7264 - accuracy: 0.7449 - top_5_accuracy: 0.9508 - val_loss: 0.6471 - val_accuracy: 0.7568 - val_top_5_accuracy: 0.9623 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "215/215 [==============================] - 40s 185ms/step - loss: 0.7136 - accuracy: 0.7484 - top_5_accuracy: 0.9500 - val_loss: 0.6188 - val_accuracy: 0.7751 - val_top_5_accuracy: 0.9541 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 0.7192 - accuracy: 0.7509 - top_5_accuracy: 0.9483 - val_loss: 0.6522 - val_accuracy: 0.7654 - val_top_5_accuracy: 0.9618 - lr: 5.0000e-04\n",
      "Epoch 30/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.7554 - top_5_accuracy: 0.9508\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 41s 190ms/step - loss: 0.6937 - accuracy: 0.7554 - top_5_accuracy: 0.9508 - val_loss: 0.6430 - val_accuracy: 0.7731 - val_top_5_accuracy: 0.9572 - lr: 5.0000e-04\n",
      "Epoch 31/50\n",
      "215/215 [==============================] - 41s 188ms/step - loss: 0.6828 - accuracy: 0.7572 - top_5_accuracy: 0.9509 - val_loss: 0.5616 - val_accuracy: 0.7879 - val_top_5_accuracy: 0.9643 - lr: 2.5000e-04\n",
      "Epoch 32/50\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 0.6569 - accuracy: 0.7643 - top_5_accuracy: 0.9562 - val_loss: 0.5832 - val_accuracy: 0.7889 - val_top_5_accuracy: 0.9592 - lr: 2.5000e-04\n",
      "Epoch 33/50\n",
      "215/215 [==============================] - 43s 198ms/step - loss: 0.6595 - accuracy: 0.7602 - top_5_accuracy: 0.9560 - val_loss: 0.5891 - val_accuracy: 0.7731 - val_top_5_accuracy: 0.9658 - lr: 2.5000e-04\n",
      "Epoch 34/50\n",
      "215/215 [==============================] - 40s 185ms/step - loss: 0.6282 - accuracy: 0.7722 - top_5_accuracy: 0.9583 - val_loss: 0.5689 - val_accuracy: 0.7787 - val_top_5_accuracy: 0.9638 - lr: 2.5000e-04\n",
      "Epoch 35/50\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 0.6500 - accuracy: 0.7687 - top_5_accuracy: 0.9560 - val_loss: 0.5604 - val_accuracy: 0.7919 - val_top_5_accuracy: 0.9597 - lr: 2.5000e-04\n",
      "Epoch 36/50\n",
      "215/215 [==============================] - 42s 196ms/step - loss: 0.6570 - accuracy: 0.7656 - top_5_accuracy: 0.9569 - val_loss: 0.5719 - val_accuracy: 0.7874 - val_top_5_accuracy: 0.9679 - lr: 2.5000e-04\n",
      "Epoch 37/50\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 0.6401 - accuracy: 0.7664 - top_5_accuracy: 0.9546 - val_loss: 0.5538 - val_accuracy: 0.7935 - val_top_5_accuracy: 0.9633 - lr: 2.5000e-04\n",
      "Epoch 38/50\n",
      "215/215 [==============================] - 40s 185ms/step - loss: 0.6522 - accuracy: 0.7637 - top_5_accuracy: 0.9560 - val_loss: 0.5675 - val_accuracy: 0.7858 - val_top_5_accuracy: 0.9628 - lr: 2.5000e-04\n",
      "Epoch 39/50\n",
      "215/215 [==============================] - 41s 189ms/step - loss: 0.6361 - accuracy: 0.7691 - top_5_accuracy: 0.9560 - val_loss: 0.5267 - val_accuracy: 0.7935 - val_top_5_accuracy: 0.9623 - lr: 2.5000e-04\n",
      "Epoch 40/50\n",
      "215/215 [==============================] - 40s 185ms/step - loss: 0.6331 - accuracy: 0.7713 - top_5_accuracy: 0.9591 - val_loss: 0.5631 - val_accuracy: 0.7874 - val_top_5_accuracy: 0.9725 - lr: 2.5000e-04\n",
      "Epoch 41/50\n",
      "215/215 [==============================] - 40s 185ms/step - loss: 0.6338 - accuracy: 0.7697 - top_5_accuracy: 0.9597 - val_loss: 0.5454 - val_accuracy: 0.7981 - val_top_5_accuracy: 0.9643 - lr: 2.5000e-04\n",
      "Epoch 42/50\n",
      "215/215 [==============================] - 41s 187ms/step - loss: 0.6376 - accuracy: 0.7672 - top_5_accuracy: 0.9532 - val_loss: 0.5587 - val_accuracy: 0.7930 - val_top_5_accuracy: 0.9653 - lr: 2.5000e-04\n",
      "Epoch 43/50\n",
      "215/215 [==============================] - 40s 185ms/step - loss: 0.6162 - accuracy: 0.7770 - top_5_accuracy: 0.9601 - val_loss: 0.5483 - val_accuracy: 0.7919 - val_top_5_accuracy: 0.9653 - lr: 2.5000e-04\n",
      "Epoch 44/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.6096 - accuracy: 0.7761 - top_5_accuracy: 0.9592\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 0.6096 - accuracy: 0.7761 - top_5_accuracy: 0.9592 - val_loss: 0.5686 - val_accuracy: 0.7833 - val_top_5_accuracy: 0.9684 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "215/215 [==============================] - 42s 192ms/step - loss: 0.6001 - accuracy: 0.7770 - top_5_accuracy: 0.9591 - val_loss: 0.5319 - val_accuracy: 0.7996 - val_top_5_accuracy: 0.9699 - lr: 1.2500e-04\n",
      "Epoch 46/50\n",
      "215/215 [==============================] - 41s 192ms/step - loss: 0.5943 - accuracy: 0.7832 - top_5_accuracy: 0.9631 - val_loss: 0.5355 - val_accuracy: 0.7909 - val_top_5_accuracy: 0.9582 - lr: 1.2500e-04\n",
      "Epoch 47/50\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 0.5994 - accuracy: 0.7837 - top_5_accuracy: 0.9630 - val_loss: 0.5070 - val_accuracy: 0.8006 - val_top_5_accuracy: 0.9709 - lr: 1.2500e-04\n",
      "Epoch 48/50\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 0.5845 - accuracy: 0.7840 - top_5_accuracy: 0.9620 - val_loss: 0.5034 - val_accuracy: 0.8154 - val_top_5_accuracy: 0.9771 - lr: 1.2500e-04\n",
      "Epoch 49/50\n",
      "215/215 [==============================] - 40s 185ms/step - loss: 0.5837 - accuracy: 0.7857 - top_5_accuracy: 0.9598 - val_loss: 0.5222 - val_accuracy: 0.8001 - val_top_5_accuracy: 0.9679 - lr: 1.2500e-04\n",
      "Epoch 50/50\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 0.5869 - accuracy: 0.7805 - top_5_accuracy: 0.9629 - val_loss: 0.5183 - val_accuracy: 0.8093 - val_top_5_accuracy: 0.9704 - lr: 1.2500e-04\n",
      "\n",
      "‚úì Training completed!\n",
      "‚è±Ô∏è  End time: 2025-11-18 13:38:06\n",
      "‚úì Model weights saved to: Xception_output\\models\\model_weights.h5\n",
      "\n",
      "======================================================================\n",
      "EVALUATING Xception\n",
      "======================================================================\n",
      "\n",
      "üìä Evaluating on test set...\n",
      "31/31 [==============================] - 5s 132ms/step - loss: 0.4976 - accuracy: 0.8198 - top_5_accuracy: 0.9705\n",
      "\n",
      "‚úì Test Results:\n",
      "  Loss: 0.4976\n",
      "  Accuracy: 0.8198\n",
      "  Top-5 Accuracy: 0.9705\n",
      "\n",
      "üìä Generating detailed metrics...\n",
      "‚úì Confusion matrix saved to: Xception_output\\plots\\confusion_matrix.png\n",
      "‚úì Classification report saved to: Xception_output\\logs\\classification_report.txt\n",
      "‚úì Top-K accuracy plot saved to: Xception_output\\plots\\topk_accuracy.png\n",
      "\n",
      "üìà Generating training plots...\n",
      "‚úì Training plots saved to: Xception_output\\plots\\training_history.png\n",
      "\n",
      "======================================================================\n",
      "üéâ XCEPTION TRAINING COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "üìä Final Results:\n",
      "  Test Accuracy: 81.98%\n",
      "  Test Loss: 0.4976\n",
      "  Top-5 Accuracy: 97.05%\n",
      "\n",
      "üìÅ All outputs saved to: Xception_output/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.49756142497062683,\n",
       " 'test_accuracy': 0.8197556138038635,\n",
       " 'test_top5_accuracy': 0.9704684615135193,\n",
       " 'num_classes': 63,\n",
       " 'per_class_accuracy': {'Missing_hole': 0.0,\n",
       "  'Missing_hole_rotation': 0.3333333333333333,\n",
       "  'Mouse_bite': 0.0,\n",
       "  'Mouse_bite_rotation': 0.0,\n",
       "  'Open_circuit': 0.26666666666666666,\n",
       "  'Open_circuit_rotation': 0.0,\n",
       "  'PCB_USED': 0.0,\n",
       "  'Short': 0.8461538461538461,\n",
       "  'Short_rotation': 0.5,\n",
       "  'Spur': 0.0,\n",
       "  'Spur_rotation': 0.0,\n",
       "  'Spurious_copper': 0.08333333333333333,\n",
       "  'Spurious_copper_rotation': 0.07692307692307693,\n",
       "  'bent': 0.0,\n",
       "  'bent_lead': 1.0,\n",
       "  'bent_wire': 1.0,\n",
       "  'broken': 0.5,\n",
       "  'broken_large': 1.0,\n",
       "  'broken_small': 0.75,\n",
       "  'broken_teeth': 0.5,\n",
       "  'cable_swap': 0.75,\n",
       "  'color': 0.5833333333333334,\n",
       "  'combined': 0.625,\n",
       "  'contamination': 0.8,\n",
       "  'crack': 0.5714285714285714,\n",
       "  'cut': 1.0,\n",
       "  'cut_inner_insulation': 1.0,\n",
       "  'cut_lead': 1.0,\n",
       "  'cut_outer_insulation': 1.0,\n",
       "  'damaged_case': 0.5,\n",
       "  'defective': 1.0,\n",
       "  'fabric_border': 1.0,\n",
       "  'fabric_interior': 0.0,\n",
       "  'faulty_imprint': 0.16666666666666666,\n",
       "  'flip': 1.0,\n",
       "  'fold': 1.0,\n",
       "  'glue': 0.6,\n",
       "  'glue_strip': 1.0,\n",
       "  'good': 0.9973821989528796,\n",
       "  'gray_stroke': 1.0,\n",
       "  'hole': 0.8333333333333334,\n",
       "  'images': 1.0,\n",
       "  'liquid': 1.0,\n",
       "  'manipulated_front': 1.0,\n",
       "  'metal_contamination': 0.7142857142857143,\n",
       "  'misplaced': 1.0,\n",
       "  'missing_cable': 1.0,\n",
       "  'missing_wire': 1.0,\n",
       "  'oil': 1.0,\n",
       "  'pill_type': 1.0,\n",
       "  'poke': 0.4666666666666667,\n",
       "  'poke_insulation': 0.0,\n",
       "  'print': 1.0,\n",
       "  'rough': 1.0,\n",
       "  'scratch': 0.8181818181818182,\n",
       "  'scratch_head': 0.3333333333333333,\n",
       "  'scratch_neck': 0.8,\n",
       "  'split_teeth': 1.0,\n",
       "  'squeeze': 1.0,\n",
       "  'squeezed_teeth': 0.6666666666666666,\n",
       "  'thread': 0.875,\n",
       "  'thread_side': 0.6,\n",
       "  'thread_top': 0.6666666666666666},\n",
       " 'evaluation_time': '2025-11-18T13:38:20.371963'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e02d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pre-trained Vision Transformer for Defect Detection\n",
    "Model: ViT-B16 (Vision Transformer Base with 16x16 patches)\n",
    "Source: keras-cv or vit-keras library\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Install required library (run this first in your notebook)\n",
    "# !pip install vit-keras\n",
    "\n",
    "from vit_keras import vit\n",
    "\n",
    "\n",
    "class PretrainedViTDefectDetector:\n",
    "    \"\"\"\n",
    "    Pre-trained Vision Transformer for defect detection\n",
    "    Model: ViT-B16 (Base model with 16x16 patches)\n",
    "    Pre-trained on ImageNet-21k\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, img_height=224, img_width=224):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.model_name = \"ViT-B16-Pretrained\"\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.class_names = []\n",
    "        self.num_classes = 0\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir = Path(\"ViT_B16_Pretrained_output\")\n",
    "        self.model_dir = self.output_dir / \"models\"\n",
    "        self.plots_dir = self.output_dir / \"plots\"\n",
    "        self.logs_dir = self.output_dir / \"logs\"\n",
    "        \n",
    "        for dir_path in [self.output_dir, self.model_dir, self.plots_dir, self.logs_dir]:\n",
    "            dir_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def prepare_data(self, validation_split=0.2, test_split=0.1, batch_size=32):\n",
    "        \"\"\"Prepare dataset\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"DATA PREPARATION - {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Get all image paths and labels\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        print(\"\\nüìÅ Scanning dataset...\")\n",
    "        for class_dir in self.dataset_path.rglob(\"*\"):\n",
    "            if class_dir.is_dir():\n",
    "                images = (list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\")) + \n",
    "                         list(class_dir.glob(\"*.jpeg\")) + list(class_dir.glob(\"*.bmp\")))\n",
    "                \n",
    "                if images:\n",
    "                    class_name = class_dir.name\n",
    "                    if class_name not in self.class_names:\n",
    "                        self.class_names.append(class_name)\n",
    "                    \n",
    "                    for img_path in images:\n",
    "                        image_paths.append(str(img_path))\n",
    "                        labels.append(class_name)\n",
    "        \n",
    "        self.class_names.sort()\n",
    "        self.num_classes = len(self.class_names)\n",
    "        print(f\"‚úì Found {len(image_paths)} images\")\n",
    "        print(f\"‚úì Number of classes: {self.num_classes}\")\n",
    "        \n",
    "        # Create label mapping\n",
    "        label_to_index = {label: idx for idx, label in enumerate(self.class_names)}\n",
    "        labels_encoded = [label_to_index[label] for label in labels]\n",
    "        \n",
    "        # Create tf.data.Dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels_encoded))\n",
    "        dataset = dataset.shuffle(len(image_paths), seed=42)\n",
    "        \n",
    "        # Calculate splits\n",
    "        train_size = int(len(image_paths) * (1 - validation_split - test_split))\n",
    "        val_size = int(len(image_paths) * validation_split)\n",
    "        test_size = len(image_paths) - train_size - val_size\n",
    "        \n",
    "        # Split datasets\n",
    "        train_ds = dataset.take(train_size)\n",
    "        val_ds = dataset.skip(train_size).take(val_size)\n",
    "        test_ds = dataset.skip(train_size + val_size)\n",
    "        \n",
    "        # Process datasets\n",
    "        self.train_dataset = train_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=True),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.val_dataset = val_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=False),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.test_dataset = test_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=False),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        print(f\"\\n‚úì Train samples: {train_size}\")\n",
    "        print(f\"‚úì Validation samples: {val_size}\")\n",
    "        print(f\"‚úì Test samples: {test_size}\")\n",
    "        \n",
    "        return train_size, val_size, test_size\n",
    "    \n",
    "    def _load_and_preprocess(self, image_path, label, augment=False):\n",
    "        \"\"\"Load and preprocess image\"\"\"\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, [self.img_height, self.img_width])\n",
    "        \n",
    "        # Data augmentation\n",
    "        if augment:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_flip_up_down(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.rot90(img, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "        \n",
    "        img = tf.clip_by_value(img, 0.0, 255.0)\n",
    "        \n",
    "        # ViT expects images in [0, 1] range\n",
    "        img = img / 255.0\n",
    "        \n",
    "        label_onehot = tf.one_hot(label, self.num_classes)\n",
    "        return img, label_onehot\n",
    "    \n",
    "    def build_model(self, model_type='B16', learning_rate=0.001, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Build pre-trained ViT model\n",
    "        \n",
    "        Args:\n",
    "            model_type: 'B16', 'B32', 'L16', 'L32'\n",
    "                B = Base (12 layers, 768 hidden, 12 heads) - ~86M params\n",
    "                L = Large (24 layers, 1024 hidden, 16 heads) - ~307M params\n",
    "                16/32 = patch size\n",
    "            learning_rate: Initial learning rate\n",
    "            dropout_rate: Dropout rate for classification head\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"BUILDING PRE-TRAINED VISION TRANSFORMER (ViT-{model_type})\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load pre-trained ViT model\n",
    "        print(\"\\n‚è≥ Downloading pre-trained weights from ImageNet-21k...\")\n",
    "        \n",
    "        if model_type == 'B16':\n",
    "            vit_model = vit.vit_b16(\n",
    "                image_size=self.img_height,\n",
    "                activation='softmax',\n",
    "                pretrained=True,\n",
    "                include_top=False,\n",
    "                pretrained_top=False\n",
    "            )\n",
    "            model_description = \"ViT-Base/16 (12 layers, 768 hidden, 12 heads, ~86M params)\"\n",
    "        elif model_type == 'B32':\n",
    "            vit_model = vit.vit_b32(\n",
    "                image_size=self.img_height,\n",
    "                activation='softmax',\n",
    "                pretrained=True,\n",
    "                include_top=False,\n",
    "                pretrained_top=False\n",
    "            )\n",
    "            model_description = \"ViT-Base/32 (12 layers, 768 hidden, 12 heads, ~88M params)\"\n",
    "        elif model_type == 'L16':\n",
    "            vit_model = vit.vit_l16(\n",
    "                image_size=self.img_height,\n",
    "                activation='softmax',\n",
    "                pretrained=True,\n",
    "                include_top=False,\n",
    "                pretrained_top=False\n",
    "            )\n",
    "            model_description = \"ViT-Large/16 (24 layers, 1024 hidden, 16 heads, ~307M params)\"\n",
    "        elif model_type == 'L32':\n",
    "            vit_model = vit.vit_l32(\n",
    "                image_size=self.img_height,\n",
    "                activation='softmax',\n",
    "                pretrained=True,\n",
    "                include_top=False,\n",
    "                pretrained_top=False\n",
    "            )\n",
    "            model_description = \"ViT-Large/32 (24 layers, 1024 hidden, 16 heads, ~307M params)\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}. Use 'B16', 'B32', 'L16', or 'L32'\")\n",
    "        \n",
    "        print(f\"‚úì Model: {model_description}\")\n",
    "        \n",
    "        # Freeze the base model initially\n",
    "        vit_model.trainable = False\n",
    "        \n",
    "        # Build classification head\n",
    "        inputs = keras.Input(shape=(self.img_height, self.img_width, 3))\n",
    "        x = vit_model(inputs)\n",
    "        \n",
    "        # Classification head\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(512, activation='gelu')(x)\n",
    "        x = layers.Dropout(dropout_rate / 2)(x)\n",
    "        x = layers.Dense(256, activation='gelu')(x)\n",
    "        x = layers.Dropout(dropout_rate / 4)(x)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        self.vit_base = vit_model\n",
    "        \n",
    "        print(\"\\n‚úì Pre-trained ViT model built successfully!\")\n",
    "        print(f\"‚úì Total parameters: {model.count_params():,}\")\n",
    "        print(f\"‚úì Base model frozen: {not vit_model.trainable}\")\n",
    "        print(f\"‚úì Pre-trained on: ImageNet-21k\")\n",
    "        print(f\"‚úì Model type: {model_type}\")\n",
    "        \n",
    "        # Save architecture\n",
    "        with open(self.model_dir / 'model_architecture.txt', 'w') as f:\n",
    "            f.write(f\"Model: ViT-{model_type}\\n\")\n",
    "            f.write(f\"Description: {model_description}\\n\")\n",
    "            f.write(f\"Pre-trained: ImageNet-21k\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def unfreeze_and_fine_tune(self, learning_rate=0.00001):\n",
    "        \"\"\"Unfreeze ViT base for fine-tuning\"\"\"\n",
    "        print(f\"\\nüîì Unfreezing ViT base for fine-tuning...\")\n",
    "        \n",
    "        self.vit_base.trainable = True\n",
    "        \n",
    "        # Recompile with much lower learning rate\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Fine-tuning enabled!\")\n",
    "        print(f\"‚úì Learning rate: {learning_rate}\")\n",
    "        print(f\"‚úì All transformer layers are now trainable\")\n",
    "    \n",
    "    def train(self, epochs=50, early_stopping_patience=10):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"TRAINING {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built. Call build_model() first.\")\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=early_stopping_patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-8,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.CSVLogger(\n",
    "                str(self.logs_dir / 'training_log.csv')\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting training for {epochs} epochs...\")\n",
    "        print(f\"‚è±Ô∏è  Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            self.train_dataset,\n",
    "            epochs=epochs,\n",
    "            validation_data=self.val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úì Training completed!\")\n",
    "        print(f\"‚è±Ô∏è  End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Save model weights\n",
    "        self.model.save_weights(str(self.model_dir / 'model_weights.h5'))\n",
    "        print(f\"‚úì Model weights saved to: {self.model_dir / 'model_weights.h5'}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"EVALUATING {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìä Evaluating on test set...\")\n",
    "        test_results = self.model.evaluate(self.test_dataset, verbose=1)\n",
    "        \n",
    "        print(\"\\n‚úì Test Results:\")\n",
    "        print(f\"  Loss: {test_results[0]:.4f}\")\n",
    "        print(f\"  Accuracy: {test_results[1]:.4f}\")\n",
    "        print(f\"  Top-5 Accuracy: {test_results[2]:.4f}\")\n",
    "        \n",
    "        # Get predictions\n",
    "        print(\"\\nüìä Generating detailed metrics...\")\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for images, labels in self.test_dataset:\n",
    "            predictions = self.model.predict(images, verbose=0)\n",
    "            y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'model_name': self.model_name,\n",
    "            'test_loss': float(test_results[0]),\n",
    "            'test_accuracy': float(test_results[1]),\n",
    "            'test_top5_accuracy': float(test_results[2]),\n",
    "            'num_classes': self.num_classes,\n",
    "            'pretrained_on': 'ImageNet-21k',\n",
    "            'evaluation_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(self.logs_dir / 'test_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        self._plot_confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Generate classification report\n",
    "        self._save_classification_report(y_true, y_pred)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _plot_confusion_matrix(self, y_true, y_pred):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(max(10, self.num_classes), max(8, self.num_classes * 0.8)))\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        sns.heatmap(cm_normalized, annot=False, cmap='Blues', \n",
    "                    xticklabels=self.class_names if self.num_classes <= 20 else False,\n",
    "                    yticklabels=self.class_names if self.num_classes <= 20 else False)\n",
    "        \n",
    "        plt.title(f'{self.model_name} - Confusion Matrix (Normalized)')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Confusion matrix saved to: {self.plots_dir / 'confusion_matrix.png'}\")\n",
    "    \n",
    "    def _save_classification_report(self, y_true, y_pred):\n",
    "        \"\"\"Save classification report\"\"\"\n",
    "        report = classification_report(y_true, y_pred, target_names=self.class_names, \n",
    "                                      zero_division=0)\n",
    "        \n",
    "        with open(self.logs_dir / 'classification_report.txt', 'w') as f:\n",
    "            f.write(f\"{self.model_name} - Classification Report\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"‚úì Classification report saved to: {self.logs_dir / 'classification_report.txt'}\")\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"‚ö†Ô∏è  No training history available\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüìà Generating training plots...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        axes[0].plot(self.history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "        axes[0].plot(self.history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "        axes[0].set_title(f'{self.model_name} - Model Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[1].plot(self.history.history['loss'], label='Train Loss', linewidth=2)\n",
    "        axes[1].plot(self.history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "        axes[1].set_title(f'{self.model_name} - Model Loss', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Training plots saved to: {self.plots_dir / 'training_history.png'}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def train_pretrained_vit(dataset_path=\"final_merged_dataset\", \n",
    "                         model_type='B16', \n",
    "                         epochs=50, \n",
    "                         batch_size=32):\n",
    "    \"\"\"\n",
    "    Train pre-trained Vision Transformer\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to dataset\n",
    "        model_type: 'B16', 'B32', 'L16', 'L32'\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n\" + \"üî∑\"*35)\n",
    "    print(f\"TRAINING PRE-TRAINED VISION TRANSFORMER (ViT-{model_type})\")\n",
    "    print(\"üî∑\"*35)\n",
    "    \n",
    "    model = PretrainedViTDefectDetector(dataset_path)\n",
    "    model.prepare_data(batch_size=batch_size)\n",
    "    model.build_model(model_type=model_type, learning_rate=0.001)\n",
    "    model.train(epochs=epochs)\n",
    "    results = model.evaluate()\n",
    "    model.plot_training_history()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üéâ PRE-TRAINED ViT-{model_type} TRAINING COMPLETED!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nüìä Final Results:\")\n",
    "    print(f\"  Test Accuracy: {results['test_accuracy']*100:.2f}%\")\n",
    "    print(f\"  Test Loss: {results['test_loss']:.4f}\")\n",
    "    print(f\"  Top-5 Accuracy: {results['test_top5_accuracy']*100:.2f}%\")\n",
    "    print(f\"\\nüìÅ All outputs saved to: ViT_B16_Pretrained_output/\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c09d8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vit-keras\n",
      "  Downloading vit_keras-0.1.2-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\.conda\\envs\\tf\\lib\\site-packages (from vit-keras) (1.13.1)\n",
      "Collecting validators (from vit-keras)\n",
      "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in c:\\users\\user\\.conda\\envs\\tf\\lib\\site-packages (from scipy->vit-keras) (1.26.4)\n",
      "Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\n",
      "Downloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: validators, vit-keras\n",
      "\n",
      "   ---------------------------------------- 0/2 [validators]\n",
      "   ---------------------------------------- 2/2 [vit-keras]\n",
      "\n",
      "Successfully installed validators-0.35.0 vit-keras-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install vit-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85f8ef68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "TRAINING PRE-TRAINED VISION TRANSFORMER (ViT-B16)\n",
      "üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑üî∑\n",
      "\n",
      "======================================================================\n",
      "DATA PREPARATION - ViT-B16-Pretrained\n",
      "======================================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images\n",
      "‚úì Number of classes: 63\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1961\n",
      "‚úì Test samples: 982\n",
      "\n",
      "======================================================================\n",
      "BUILDING PRE-TRAINED VISION TRANSFORMER (ViT-B16)\n",
      "======================================================================\n",
      "\n",
      "‚è≥ Downloading pre-trained weights from ImageNet-21k...\n",
      "Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_16_imagenet21k+imagenet2012.npz\n",
      "347502902/347502902 [==============================] - 197s 1us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\vit_keras\\utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model: ViT-Base/16 (12 layers, 768 hidden, 12 heads, ~86M params)\n",
      "\n",
      "‚úì Pre-trained ViT model built successfully!\n",
      "‚úì Total parameters: 86,339,903\n",
      "‚úì Base model frozen: True\n",
      "‚úì Pre-trained on: ImageNet-21k\n",
      "‚úì Model type: B16\n",
      "\n",
      "======================================================================\n",
      "TRAINING ViT-B16-Pretrained\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting training for 50 epochs...\n",
      "‚è±Ô∏è  Start time: 2025-11-26 10:50:27\n",
      "\n",
      "Epoch 1/50\n",
      "215/215 [==============================] - 110s 464ms/step - loss: 1.4266 - accuracy: 0.6338 - top_5_accuracy: 0.8252 - val_loss: 1.0970 - val_accuracy: 0.6798 - val_top_5_accuracy: 0.8934 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "215/215 [==============================] - 85s 394ms/step - loss: 1.0958 - accuracy: 0.6689 - top_5_accuracy: 0.8937 - val_loss: 0.9923 - val_accuracy: 0.6818 - val_top_5_accuracy: 0.9250 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "215/215 [==============================] - 83s 386ms/step - loss: 1.0124 - accuracy: 0.6833 - top_5_accuracy: 0.8985 - val_loss: 0.9421 - val_accuracy: 0.7114 - val_top_5_accuracy: 0.9184 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "215/215 [==============================] - 82s 381ms/step - loss: 0.9551 - accuracy: 0.6951 - top_5_accuracy: 0.9176 - val_loss: 0.9315 - val_accuracy: 0.6915 - val_top_5_accuracy: 0.9255 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "215/215 [==============================] - 81s 375ms/step - loss: 0.9210 - accuracy: 0.7018 - top_5_accuracy: 0.9231 - val_loss: 0.8902 - val_accuracy: 0.7144 - val_top_5_accuracy: 0.9250 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "215/215 [==============================] - 78s 359ms/step - loss: 0.9090 - accuracy: 0.7056 - top_5_accuracy: 0.9247 - val_loss: 0.9246 - val_accuracy: 0.7042 - val_top_5_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "215/215 [==============================] - 73s 341ms/step - loss: 0.8917 - accuracy: 0.7079 - top_5_accuracy: 0.9301 - val_loss: 0.8398 - val_accuracy: 0.7236 - val_top_5_accuracy: 0.9301 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "215/215 [==============================] - 74s 341ms/step - loss: 0.8692 - accuracy: 0.7101 - top_5_accuracy: 0.9289 - val_loss: 0.8984 - val_accuracy: 0.7053 - val_top_5_accuracy: 0.9383 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "215/215 [==============================] - 75s 346ms/step - loss: 0.8586 - accuracy: 0.7154 - top_5_accuracy: 0.9331 - val_loss: 0.9019 - val_accuracy: 0.7149 - val_top_5_accuracy: 0.9296 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "215/215 [==============================] - 75s 348ms/step - loss: 0.8518 - accuracy: 0.7123 - top_5_accuracy: 0.9347 - val_loss: 0.8016 - val_accuracy: 0.7307 - val_top_5_accuracy: 0.9444 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "215/215 [==============================] - 76s 353ms/step - loss: 0.8303 - accuracy: 0.7253 - top_5_accuracy: 0.9372 - val_loss: 0.7047 - val_accuracy: 0.7496 - val_top_5_accuracy: 0.9470 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "215/215 [==============================] - 72s 332ms/step - loss: 0.8063 - accuracy: 0.7263 - top_5_accuracy: 0.9387 - val_loss: 0.7877 - val_accuracy: 0.7358 - val_top_5_accuracy: 0.9388 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "215/215 [==============================] - 72s 335ms/step - loss: 0.8169 - accuracy: 0.7269 - top_5_accuracy: 0.9377 - val_loss: 0.8052 - val_accuracy: 0.7338 - val_top_5_accuracy: 0.9403 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "215/215 [==============================] - 73s 339ms/step - loss: 0.7888 - accuracy: 0.7264 - top_5_accuracy: 0.9449 - val_loss: 0.7124 - val_accuracy: 0.7562 - val_top_5_accuracy: 0.9465 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "215/215 [==============================] - 71s 329ms/step - loss: 0.7979 - accuracy: 0.7275 - top_5_accuracy: 0.9436 - val_loss: 0.7428 - val_accuracy: 0.7481 - val_top_5_accuracy: 0.9505 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "215/215 [==============================] - 70s 327ms/step - loss: 0.7746 - accuracy: 0.7343 - top_5_accuracy: 0.9432 - val_loss: 0.7034 - val_accuracy: 0.7629 - val_top_5_accuracy: 0.9521 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "215/215 [==============================] - 69s 320ms/step - loss: 0.7817 - accuracy: 0.7299 - top_5_accuracy: 0.9477 - val_loss: 0.7330 - val_accuracy: 0.7511 - val_top_5_accuracy: 0.9582 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "215/215 [==============================] - 69s 321ms/step - loss: 0.7764 - accuracy: 0.7343 - top_5_accuracy: 0.9438 - val_loss: 0.7650 - val_accuracy: 0.7420 - val_top_5_accuracy: 0.9459 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "215/215 [==============================] - 70s 325ms/step - loss: 0.7546 - accuracy: 0.7400 - top_5_accuracy: 0.9470 - val_loss: 0.7892 - val_accuracy: 0.7323 - val_top_5_accuracy: 0.9419 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "215/215 [==============================] - 70s 324ms/step - loss: 0.7568 - accuracy: 0.7356 - top_5_accuracy: 0.9496 - val_loss: 0.7442 - val_accuracy: 0.7537 - val_top_5_accuracy: 0.9546 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.7607 - accuracy: 0.7378 - top_5_accuracy: 0.9484\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 69s 319ms/step - loss: 0.7607 - accuracy: 0.7378 - top_5_accuracy: 0.9484 - val_loss: 0.7428 - val_accuracy: 0.7445 - val_top_5_accuracy: 0.9500 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "215/215 [==============================] - 69s 320ms/step - loss: 0.6899 - accuracy: 0.7569 - top_5_accuracy: 0.9529 - val_loss: 0.7183 - val_accuracy: 0.7409 - val_top_5_accuracy: 0.9449 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "215/215 [==============================] - 69s 319ms/step - loss: 0.7037 - accuracy: 0.7522 - top_5_accuracy: 0.9522 - val_loss: 0.6334 - val_accuracy: 0.7664 - val_top_5_accuracy: 0.9669 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "215/215 [==============================] - 69s 318ms/step - loss: 0.6881 - accuracy: 0.7532 - top_5_accuracy: 0.9489 - val_loss: 0.6716 - val_accuracy: 0.7573 - val_top_5_accuracy: 0.9531 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "215/215 [==============================] - 69s 318ms/step - loss: 0.6638 - accuracy: 0.7569 - top_5_accuracy: 0.9550 - val_loss: 0.6346 - val_accuracy: 0.7695 - val_top_5_accuracy: 0.9572 - lr: 5.0000e-04\n",
      "Epoch 26/50\n",
      "215/215 [==============================] - 69s 320ms/step - loss: 0.6685 - accuracy: 0.7573 - top_5_accuracy: 0.9525 - val_loss: 0.7083 - val_accuracy: 0.7471 - val_top_5_accuracy: 0.9556 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "215/215 [==============================] - 69s 319ms/step - loss: 0.6669 - accuracy: 0.7595 - top_5_accuracy: 0.9573 - val_loss: 0.6769 - val_accuracy: 0.7562 - val_top_5_accuracy: 0.9536 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.6620 - accuracy: 0.7576 - top_5_accuracy: 0.9548\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 69s 319ms/step - loss: 0.6620 - accuracy: 0.7576 - top_5_accuracy: 0.9548 - val_loss: 0.6497 - val_accuracy: 0.7746 - val_top_5_accuracy: 0.9516 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "215/215 [==============================] - 69s 321ms/step - loss: 0.6491 - accuracy: 0.7653 - top_5_accuracy: 0.9610 - val_loss: 0.6234 - val_accuracy: 0.7700 - val_top_5_accuracy: 0.9628 - lr: 2.5000e-04\n",
      "Epoch 30/50\n",
      "215/215 [==============================] - 70s 325ms/step - loss: 0.6225 - accuracy: 0.7717 - top_5_accuracy: 0.9614 - val_loss: 0.6526 - val_accuracy: 0.7721 - val_top_5_accuracy: 0.9536 - lr: 2.5000e-04\n",
      "Epoch 31/50\n",
      "215/215 [==============================] - 69s 323ms/step - loss: 0.6274 - accuracy: 0.7703 - top_5_accuracy: 0.9570 - val_loss: 0.5727 - val_accuracy: 0.7955 - val_top_5_accuracy: 0.9587 - lr: 2.5000e-04\n",
      "Epoch 32/50\n",
      "215/215 [==============================] - 69s 322ms/step - loss: 0.6247 - accuracy: 0.7733 - top_5_accuracy: 0.9615 - val_loss: 0.6338 - val_accuracy: 0.7649 - val_top_5_accuracy: 0.9638 - lr: 2.5000e-04\n",
      "Epoch 33/50\n",
      "215/215 [==============================] - 69s 323ms/step - loss: 0.6215 - accuracy: 0.7688 - top_5_accuracy: 0.9595 - val_loss: 0.6252 - val_accuracy: 0.7685 - val_top_5_accuracy: 0.9577 - lr: 2.5000e-04\n",
      "Epoch 34/50\n",
      "215/215 [==============================] - 70s 327ms/step - loss: 0.6010 - accuracy: 0.7805 - top_5_accuracy: 0.9592 - val_loss: 0.6297 - val_accuracy: 0.7797 - val_top_5_accuracy: 0.9536 - lr: 2.5000e-04\n",
      "Epoch 35/50\n",
      "215/215 [==============================] - 71s 331ms/step - loss: 0.6137 - accuracy: 0.7745 - top_5_accuracy: 0.9591 - val_loss: 0.6432 - val_accuracy: 0.7777 - val_top_5_accuracy: 0.9510 - lr: 2.5000e-04\n",
      "Epoch 36/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.6257 - accuracy: 0.7668 - top_5_accuracy: 0.9594\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "215/215 [==============================] - 71s 331ms/step - loss: 0.6257 - accuracy: 0.7668 - top_5_accuracy: 0.9594 - val_loss: 0.6301 - val_accuracy: 0.7731 - val_top_5_accuracy: 0.9618 - lr: 2.5000e-04\n",
      "Epoch 37/50\n",
      "215/215 [==============================] - 71s 331ms/step - loss: 0.6088 - accuracy: 0.7714 - top_5_accuracy: 0.9618 - val_loss: 0.5915 - val_accuracy: 0.7833 - val_top_5_accuracy: 0.9612 - lr: 1.2500e-04\n",
      "Epoch 38/50\n",
      "215/215 [==============================] - 72s 332ms/step - loss: 0.6079 - accuracy: 0.7795 - top_5_accuracy: 0.9614 - val_loss: 0.6286 - val_accuracy: 0.7675 - val_top_5_accuracy: 0.9582 - lr: 1.2500e-04\n",
      "Epoch 39/50\n",
      "215/215 [==============================] - 72s 334ms/step - loss: 0.6041 - accuracy: 0.7752 - top_5_accuracy: 0.9567 - val_loss: 0.5736 - val_accuracy: 0.7807 - val_top_5_accuracy: 0.9653 - lr: 1.2500e-04\n",
      "Epoch 40/50\n",
      "215/215 [==============================] - 71s 332ms/step - loss: 0.6039 - accuracy: 0.7761 - top_5_accuracy: 0.9605 - val_loss: 0.6262 - val_accuracy: 0.7721 - val_top_5_accuracy: 0.9602 - lr: 1.2500e-04\n",
      "Epoch 41/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.5936 - accuracy: 0.7802 - top_5_accuracy: 0.9636Restoring model weights from the end of the best epoch: 31.\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "215/215 [==============================] - 72s 334ms/step - loss: 0.5936 - accuracy: 0.7802 - top_5_accuracy: 0.9636 - val_loss: 0.5971 - val_accuracy: 0.7802 - val_top_5_accuracy: 0.9607 - lr: 1.2500e-04\n",
      "Epoch 41: early stopping\n",
      "\n",
      "‚úì Training completed!\n",
      "‚è±Ô∏è  End time: 2025-11-26 11:40:25\n",
      "‚úì Model weights saved to: ViT_B16_Pretrained_output\\models\\model_weights.h5\n",
      "\n",
      "======================================================================\n",
      "EVALUATING ViT-B16-Pretrained\n",
      "======================================================================\n",
      "\n",
      "üìä Evaluating on test set...\n",
      "31/31 [==============================] - 7s 221ms/step - loss: 0.6436 - accuracy: 0.7637 - top_5_accuracy: 0.9562\n",
      "\n",
      "‚úì Test Results:\n",
      "  Loss: 0.6436\n",
      "  Accuracy: 0.7637\n",
      "  Top-5 Accuracy: 0.9562\n",
      "\n",
      "üìä Generating detailed metrics...\n",
      "‚úì Confusion matrix saved to: ViT_B16_Pretrained_output\\plots\\confusion_matrix.png\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 61, does not match size of target_names, 63. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6404\\287718596.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_pretrained_vit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'B16'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6404\\4171249671.py\u001b[0m in \u001b[0;36mtrain_pretrained_vit\u001b[1;34m(dataset_path, model_type, epochs, batch_size)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_training_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6404\\4171249671.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;31m# Generate classification report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_classification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6404\\4171249671.py\u001b[0m in \u001b[0;36m_save_classification_report\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_classification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;34m\"\"\"Save classification report\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         report = classification_report(y_true, y_pred, target_names=self.class_names, \n\u001b[0m\u001b[0;32m    385\u001b[0m                                       zero_division=0)\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m                     )\n\u001b[0;32m    215\u001b[0m                 ):\n\u001b[1;32m--> 216\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m                 \u001b[1;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2691\u001b[0m             )\n\u001b[0;32m   2692\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2693\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   2694\u001b[0m                 \u001b[1;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m                 \u001b[1;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 61, does not match size of target_names, 63. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "train_pretrained_vit(model_type='B16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c8974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA PREPARATION - ViT-B16-Pretrained\n",
      "======================================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images\n",
      "‚úì Number of classes: 63\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1961\n",
      "‚úì Test samples: 982\n",
      "\n",
      "======================================================================\n",
      "BUILDING PRE-TRAINED VISION TRANSFORMER (ViT-B16)\n",
      "======================================================================\n",
      "\n",
      "‚è≥ Downloading pre-trained weights from ImageNet-21k...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\vit_keras\\utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model: ViT-Base/16 (12 layers, 768 hidden, 12 heads, ~86M params)\n",
      "\n",
      "‚úì Pre-trained ViT model built successfully!\n",
      "‚úì Total parameters: 86,339,903\n",
      "‚úì Base model frozen: True\n",
      "‚úì Pre-trained on: ImageNet-21k\n",
      "‚úì Model type: B16\n",
      "\n",
      "======================================================================\n",
      "TRAINING ViT-B16-Pretrained\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting training for 20 epochs...\n",
      "‚è±Ô∏è  Start time: 2025-11-26 11:43:51\n",
      "\n",
      "Epoch 1/20\n",
      "215/215 [==============================] - 75s 324ms/step - loss: 1.4211 - accuracy: 0.6342 - top_5_accuracy: 0.8235 - val_loss: 1.0840 - val_accuracy: 0.6823 - val_top_5_accuracy: 0.8934 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 69s 320ms/step - loss: 1.1080 - accuracy: 0.6711 - top_5_accuracy: 0.8913 - val_loss: 0.9550 - val_accuracy: 0.6940 - val_top_5_accuracy: 0.9199 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - 70s 324ms/step - loss: 1.0103 - accuracy: 0.6861 - top_5_accuracy: 0.9060 - val_loss: 0.9486 - val_accuracy: 0.7129 - val_top_5_accuracy: 0.9179 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 69s 319ms/step - loss: 0.9586 - accuracy: 0.6905 - top_5_accuracy: 0.9180 - val_loss: 0.9190 - val_accuracy: 0.6925 - val_top_5_accuracy: 0.9261 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "111/215 [==============>...............] - ETA: 26s - loss: 0.9238 - accuracy: 0.7041 - top_5_accuracy: 0.9209"
     ]
    }
   ],
   "source": [
    "model = PretrainedViTDefectDetector(\"final_merged_dataset\")\n",
    "model.prepare_data(batch_size=32)\n",
    "model.build_model(model_type='B16')\n",
    "model.train(epochs=20)  # Initial training\n",
    "model.unfreeze_and_fine_tune(learning_rate=0.00001)  # Unfreeze\n",
    "model.train(epochs=10)  # Fine-tune\n",
    "results = model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf5bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2, DenseNet121, DenseNet169, DenseNet201\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "class DefectDetectionModel:\n",
    "    \"\"\"Base class for defect detection models\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, model_name, img_height=224, img_width=224):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.model_name = model_name\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.class_names = []\n",
    "        self.num_classes = 0\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir = Path(f\"{model_name}_output\")\n",
    "        self.model_dir = self.output_dir / \"models\"\n",
    "        self.plots_dir = self.output_dir / \"plots\"\n",
    "        self.logs_dir = self.output_dir / \"logs\"\n",
    "        \n",
    "        for dir_path in [self.output_dir, self.model_dir, self.plots_dir, self.logs_dir]:\n",
    "            dir_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def prepare_data(self, validation_split=0.2, test_split=0.1, batch_size=32):\n",
    "        \"\"\"Prepare dataset with efficient TensorFlow pipeline\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"DATA PREPARATION - {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Get all image paths and labels\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        print(\"\\nüìÅ Scanning dataset...\")\n",
    "        for class_dir in self.dataset_path.rglob(\"*\"):\n",
    "            if class_dir.is_dir():\n",
    "                images = (list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\")) + \n",
    "                         list(class_dir.glob(\"*.jpeg\")) + list(class_dir.glob(\"*.bmp\")))\n",
    "                \n",
    "                if images:\n",
    "                    class_name = class_dir.name\n",
    "                    if class_name not in self.class_names:\n",
    "                        self.class_names.append(class_name)\n",
    "                    \n",
    "                    for img_path in images:\n",
    "                        image_paths.append(str(img_path))\n",
    "                        labels.append(class_name)\n",
    "        \n",
    "        self.class_names.sort()\n",
    "        self.num_classes = len(self.class_names)\n",
    "        print(f\"‚úì Found {len(image_paths)} images\")\n",
    "        print(f\"‚úì Number of classes: {self.num_classes}\")\n",
    "        print(f\"‚úì Classes: {self.class_names[:10]}{'...' if len(self.class_names) > 10 else ''}\")\n",
    "        \n",
    "        # Create label mapping\n",
    "        label_to_index = {label: idx for idx, label in enumerate(self.class_names)}\n",
    "        labels_encoded = [label_to_index[label] for label in labels]\n",
    "        \n",
    "        # Create tf.data.Dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels_encoded))\n",
    "        dataset = dataset.shuffle(len(image_paths), seed=42)\n",
    "        \n",
    "        # Calculate splits\n",
    "        train_size = int(len(image_paths) * (1 - validation_split - test_split))\n",
    "        val_size = int(len(image_paths) * validation_split)\n",
    "        test_size = len(image_paths) - train_size - val_size\n",
    "        \n",
    "        # Split datasets\n",
    "        train_ds = dataset.take(train_size)\n",
    "        val_ds = dataset.skip(train_size).take(val_size)\n",
    "        test_ds = dataset.skip(train_size + val_size)\n",
    "        \n",
    "        # Process datasets\n",
    "        self.train_dataset = train_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=True),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.val_dataset = val_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=False),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.test_dataset = test_ds.map(\n",
    "            lambda x, y: self._load_and_preprocess(x, y, augment=False),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        print(f\"\\n‚úì Train samples: {train_size}\")\n",
    "        print(f\"‚úì Validation samples: {val_size}\")\n",
    "        print(f\"‚úì Test samples: {test_size}\")\n",
    "        \n",
    "        return train_size, val_size, test_size\n",
    "    \n",
    "    def _load_and_preprocess(self, image_path, label, augment=False):\n",
    "        \"\"\"Load and preprocess image\"\"\"\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, [self.img_height, self.img_width])\n",
    "        \n",
    "        # Data augmentation\n",
    "        if augment:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_flip_up_down(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.rot90(img, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "        \n",
    "        img = tf.clip_by_value(img, 0.0, 255.0)\n",
    "        img = img / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        label_onehot = tf.one_hot(label, self.num_classes)\n",
    "        return img, label_onehot\n",
    "    \n",
    "    def train(self, epochs=50, early_stopping_patience=10):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"TRAINING {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built. Call build_model() first.\")\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=early_stopping_patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.CSVLogger(\n",
    "                str(self.logs_dir / 'training_log.csv')\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting training for {epochs} epochs...\")\n",
    "        print(f\"‚è±Ô∏è  Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            self.train_dataset,\n",
    "            epochs=epochs,\n",
    "            validation_data=self.val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úì Training completed!\")\n",
    "        print(f\"‚è±Ô∏è  End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Save model weights\n",
    "        self.model.save_weights(str(self.model_dir / 'model_weights.h5'))\n",
    "        print(f\"‚úì Model weights saved to: {self.model_dir / 'model_weights.h5'}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"EVALUATING {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìä Evaluating on test set...\")\n",
    "        test_results = self.model.evaluate(self.test_dataset, verbose=1)\n",
    "        \n",
    "        print(\"\\n‚úì Test Results:\")\n",
    "        print(f\"  Loss: {test_results[0]:.4f}\")\n",
    "        print(f\"  Accuracy: {test_results[1]:.4f}\")\n",
    "        if len(test_results) > 2:\n",
    "            print(f\"  Top-5 Accuracy: {test_results[2]:.4f}\")\n",
    "        \n",
    "        # Get predictions for confusion matrix\n",
    "        print(\"\\nüìä Generating detailed metrics...\")\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for images, labels in self.test_dataset:\n",
    "            predictions = self.model.predict(images, verbose=0)\n",
    "            y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'test_loss': float(test_results[0]),\n",
    "            'test_accuracy': float(test_results[1]),\n",
    "            'num_classes': self.num_classes,\n",
    "            'evaluation_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        if len(test_results) > 2:\n",
    "            results['test_top5_accuracy'] = float(test_results[2])\n",
    "        \n",
    "        with open(self.logs_dir / 'test_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        self._plot_confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Generate classification report\n",
    "        self._save_classification_report(y_true, y_pred)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _plot_confusion_matrix(self, y_true, y_pred):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(max(10, self.num_classes), max(8, self.num_classes * 0.8)))\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        sns.heatmap(cm_normalized, annot=False, cmap='Blues', \n",
    "                    xticklabels=self.class_names if self.num_classes <= 20 else False,\n",
    "                    yticklabels=self.class_names if self.num_classes <= 20 else False)\n",
    "        \n",
    "        plt.title(f'{self.model_name} - Confusion Matrix (Normalized)')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Confusion matrix saved to: {self.plots_dir / 'confusion_matrix.png'}\")\n",
    "    \n",
    "    def _save_classification_report(self, y_true, y_pred):\n",
    "        \"\"\"Save classification report\"\"\"\n",
    "        unique_labels = sorted(list(set(y_true) | set(y_pred)))\n",
    "        target_names_subset = [self.class_names[i] for i in unique_labels]\n",
    "        \n",
    "        report = classification_report(y_true, y_pred, \n",
    "                                      labels=unique_labels,\n",
    "                                      target_names=target_names_subset, \n",
    "                                      zero_division=0)\n",
    "        \n",
    "        with open(self.logs_dir / 'classification_report.txt', 'w') as f:\n",
    "            f.write(f\"{self.model_name} - Classification Report\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            f.write(f\"Classes in test set: {len(unique_labels)}/{self.num_classes}\\n\\n\")\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"‚úì Classification report saved to: {self.logs_dir / 'classification_report.txt'}\")\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"‚ö†Ô∏è  No training history available\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüìà Generating training plots...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        axes[0].plot(self.history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "        axes[0].plot(self.history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "        axes[0].set_title(f'{self.model_name} - Model Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1].plot(self.history.history['loss'], label='Train Loss', linewidth=2)\n",
    "        axes[1].plot(self.history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "        axes[1].set_title(f'{self.model_name} - Model Loss', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plots_dir / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Training plots saved to: {self.plots_dir / 'training_history.png'}\")\n",
    "\n",
    "\n",
    "class MobileNetV2DefectDetector(DefectDetectionModel):\n",
    "    \"\"\"MobileNetV2 for defect detection - Lightweight and efficient\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, img_height=224, img_width=224):\n",
    "        super().__init__(dataset_path, \"MobileNetV2\", img_height, img_width)\n",
    "    \n",
    "    def build_model(self, learning_rate=0.001, dropout_rate=0.3, alpha=1.0):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"BUILDING MOBILENETV2 MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        base_model = MobileNetV2(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(self.img_height, self.img_width, 3),\n",
    "            alpha=alpha\n",
    "        )\n",
    "        \n",
    "        base_model.trainable = False\n",
    "        \n",
    "        inputs = keras.Input(shape=(self.img_height, self.img_width, 3))\n",
    "        x = layers.Rescaling(scale=2.0, offset=-1.0)(inputs)\n",
    "        x = base_model(x, training=False)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate / 2)(x)\n",
    "        \n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        with open(self.model_dir / 'model_architecture.txt', 'w') as f:\n",
    "            f.write(f\"Model: MobileNetV2 (alpha={alpha})\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def unfreeze_and_fine_tune(self, fine_tune_from_layer=100, learning_rate=0.0001):\n",
    "        base_model = self.model.layers[2]\n",
    "        base_model.trainable = True\n",
    "        \n",
    "        for layer in base_model.layers[:fine_tune_from_layer]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "\n",
    "\n",
    "class DenseNetDefectDetector(DefectDetectionModel):\n",
    "    \"\"\"DenseNet for defect detection\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, model_variant='121', img_height=224, img_width=224):\n",
    "        self.model_variant = model_variant\n",
    "        super().__init__(dataset_path, f\"DenseNet{model_variant}\", img_height, img_width)\n",
    "    \n",
    "    def build_model(self, learning_rate=0.001, dropout_rate=0.3):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"BUILDING DENSENET{self.model_variant} MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if self.model_variant == '121':\n",
    "            base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=(self.img_height, self.img_width, 3))\n",
    "        elif self.model_variant == '169':\n",
    "            base_model = DenseNet169(include_top=False, weights='imagenet', input_shape=(self.img_height, self.img_width, 3))\n",
    "        elif self.model_variant == '201':\n",
    "            base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(self.img_height, self.img_width, 3))\n",
    "        else:\n",
    "            raise ValueError(\"Variant must be 121, 169 or 201\")\n",
    "        \n",
    "        base_model.trainable = False\n",
    "        \n",
    "        inputs = keras.Input(shape=(self.img_height, self.img_width, 3))\n",
    "        x = tf.keras.applications.densenet.preprocess_input(inputs * 255.0)\n",
    "        x = base_model(x, training=False)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Dense(1024, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate/2)(x)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate/4)(x)\n",
    "        \n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def unfreeze_and_fine_tune(self, fine_tune_from_layer=None, learning_rate=0.0001):\n",
    "        defaults = {'121': 311, '169': 425, '201': 481}\n",
    "        if fine_tune_from_layer is None:\n",
    "            fine_tune_from_layer = defaults[self.model_variant]\n",
    "        \n",
    "        base_model = self.model.layers[2]\n",
    "        base_model.trainable = True\n",
    "        \n",
    "        for layer in base_model.layers[:fine_tune_from_layer]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "        )\n",
    "\n",
    "\n",
    "def train_mobilenetv2(dataset_path=\"final_merged_dataset\", epochs=30, batch_size=32, alpha=1.0):\n",
    "    model = MobileNetV2DefectDetector(dataset_path)\n",
    "    model.prepare_data(batch_size=batch_size)\n",
    "    model.build_model(learning_rate=0.001, alpha=alpha)\n",
    "    model.train(epochs=epochs)\n",
    "    results = model.evaluate()\n",
    "    model.plot_training_history()\n",
    "    return results\n",
    "\n",
    "\n",
    "def train_densenet(dataset_path=\"final_merged_dataset\", variant='121', epochs=30, batch_size=32):\n",
    "    model = DenseNetDefectDetector(dataset_path, model_variant=variant)\n",
    "    model.prepare_data(batch_size=batch_size)\n",
    "    model.build_model(learning_rate=0.001)\n",
    "    model.train(epochs=epochs)\n",
    "    results = model.evaluate()\n",
    "    model.plot_training_history()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a4b286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA PREPARATION - MobileNetV2\n",
      "======================================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images\n",
      "‚úì Number of classes: 63\n",
      "‚úì Classes: ['Missing_hole', 'Missing_hole_rotation', 'Mouse_bite', 'Mouse_bite_rotation', 'Open_circuit', 'Open_circuit_rotation', 'PCB_USED', 'Short', 'Short_rotation', 'Spur']...\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1961\n",
      "‚úì Test samples: 982\n",
      "\n",
      "======================================================================\n",
      "BUILDING MOBILENETV2 MODEL\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING MobileNetV2\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting training for 50 epochs...\n",
      "‚è±Ô∏è  Start time: 2025-11-27 22:51:38\n",
      "\n",
      "Epoch 1/50\n",
      "215/215 [==============================] - 73s 332ms/step - loss: 1.5692 - accuracy: 0.6221 - top_5_accuracy: 0.8023 - val_loss: 1.1267 - val_accuracy: 0.6690 - val_top_5_accuracy: 0.8924 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "215/215 [==============================] - 57s 266ms/step - loss: 1.2241 - accuracy: 0.6524 - top_5_accuracy: 0.8756 - val_loss: 1.0294 - val_accuracy: 0.6854 - val_top_5_accuracy: 0.9128 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "215/215 [==============================] - 37s 171ms/step - loss: 1.1126 - accuracy: 0.6714 - top_5_accuracy: 0.8897 - val_loss: 0.9456 - val_accuracy: 0.7032 - val_top_5_accuracy: 0.9138 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "215/215 [==============================] - 33s 152ms/step - loss: 1.0456 - accuracy: 0.6803 - top_5_accuracy: 0.8989 - val_loss: 0.9097 - val_accuracy: 0.7032 - val_top_5_accuracy: 0.9266 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "215/215 [==============================] - 34s 158ms/step - loss: 1.0164 - accuracy: 0.6839 - top_5_accuracy: 0.9053 - val_loss: 0.8358 - val_accuracy: 0.7190 - val_top_5_accuracy: 0.9424 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "215/215 [==============================] - 34s 160ms/step - loss: 0.9811 - accuracy: 0.6937 - top_5_accuracy: 0.9125 - val_loss: 0.8834 - val_accuracy: 0.7007 - val_top_5_accuracy: 0.9414 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "215/215 [==============================] - 36s 170ms/step - loss: 0.9612 - accuracy: 0.6956 - top_5_accuracy: 0.9189 - val_loss: 0.8509 - val_accuracy: 0.7206 - val_top_5_accuracy: 0.9363 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "215/215 [==============================] - 38s 176ms/step - loss: 0.9416 - accuracy: 0.6950 - top_5_accuracy: 0.9241 - val_loss: 0.8793 - val_accuracy: 0.7032 - val_top_5_accuracy: 0.9383 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "215/215 [==============================] - 38s 179ms/step - loss: 0.9370 - accuracy: 0.6953 - top_5_accuracy: 0.9291 - val_loss: 0.8350 - val_accuracy: 0.7083 - val_top_5_accuracy: 0.9352 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "215/215 [==============================] - 37s 174ms/step - loss: 0.9141 - accuracy: 0.7020 - top_5_accuracy: 0.9286 - val_loss: 0.7984 - val_accuracy: 0.7267 - val_top_5_accuracy: 0.9403 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "215/215 [==============================] - 38s 177ms/step - loss: 0.9059 - accuracy: 0.7058 - top_5_accuracy: 0.9277 - val_loss: 0.7228 - val_accuracy: 0.7440 - val_top_5_accuracy: 0.9516 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "215/215 [==============================] - 35s 164ms/step - loss: 0.8770 - accuracy: 0.7103 - top_5_accuracy: 0.9289 - val_loss: 0.7984 - val_accuracy: 0.7282 - val_top_5_accuracy: 0.9449 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "215/215 [==============================] - 35s 164ms/step - loss: 0.8770 - accuracy: 0.7111 - top_5_accuracy: 0.9330 - val_loss: 0.7602 - val_accuracy: 0.7369 - val_top_5_accuracy: 0.9454 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "215/215 [==============================] - 35s 163ms/step - loss: 0.8668 - accuracy: 0.7180 - top_5_accuracy: 0.9339 - val_loss: 0.7226 - val_accuracy: 0.7506 - val_top_5_accuracy: 0.9551 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "215/215 [==============================] - 35s 162ms/step - loss: 0.8506 - accuracy: 0.7202 - top_5_accuracy: 0.9359 - val_loss: 0.7574 - val_accuracy: 0.7353 - val_top_5_accuracy: 0.9516 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "215/215 [==============================] - 36s 166ms/step - loss: 0.8477 - accuracy: 0.7194 - top_5_accuracy: 0.9353 - val_loss: 0.6970 - val_accuracy: 0.7573 - val_top_5_accuracy: 0.9587 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "215/215 [==============================] - 33s 154ms/step - loss: 0.8372 - accuracy: 0.7184 - top_5_accuracy: 0.9391 - val_loss: 0.7340 - val_accuracy: 0.7420 - val_top_5_accuracy: 0.9556 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "215/215 [==============================] - 32s 149ms/step - loss: 0.8459 - accuracy: 0.7231 - top_5_accuracy: 0.9379 - val_loss: 0.7560 - val_accuracy: 0.7455 - val_top_5_accuracy: 0.9561 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "215/215 [==============================] - 33s 152ms/step - loss: 0.8232 - accuracy: 0.7240 - top_5_accuracy: 0.9426 - val_loss: 0.7695 - val_accuracy: 0.7374 - val_top_5_accuracy: 0.9516 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "215/215 [==============================] - 30s 142ms/step - loss: 0.8174 - accuracy: 0.7285 - top_5_accuracy: 0.9425 - val_loss: 0.6733 - val_accuracy: 0.7675 - val_top_5_accuracy: 0.9582 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "215/215 [==============================] - 21s 97ms/step - loss: 0.8232 - accuracy: 0.7248 - top_5_accuracy: 0.9430 - val_loss: 0.6651 - val_accuracy: 0.7603 - val_top_5_accuracy: 0.9567 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "215/215 [==============================] - 22s 102ms/step - loss: 0.7978 - accuracy: 0.7307 - top_5_accuracy: 0.9430 - val_loss: 0.7358 - val_accuracy: 0.7348 - val_top_5_accuracy: 0.9480 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "215/215 [==============================] - 22s 103ms/step - loss: 0.8079 - accuracy: 0.7229 - top_5_accuracy: 0.9416 - val_loss: 0.6604 - val_accuracy: 0.7603 - val_top_5_accuracy: 0.9684 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "215/215 [==============================] - 23s 105ms/step - loss: 0.8105 - accuracy: 0.7264 - top_5_accuracy: 0.9436 - val_loss: 0.7014 - val_accuracy: 0.7460 - val_top_5_accuracy: 0.9541 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "215/215 [==============================] - 23s 107ms/step - loss: 0.7837 - accuracy: 0.7358 - top_5_accuracy: 0.9458 - val_loss: 0.6691 - val_accuracy: 0.7588 - val_top_5_accuracy: 0.9587 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "215/215 [==============================] - 22s 102ms/step - loss: 0.7910 - accuracy: 0.7307 - top_5_accuracy: 0.9449 - val_loss: 0.7155 - val_accuracy: 0.7369 - val_top_5_accuracy: 0.9607 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "215/215 [==============================] - 23s 105ms/step - loss: 0.7819 - accuracy: 0.7340 - top_5_accuracy: 0.9446 - val_loss: 0.6963 - val_accuracy: 0.7435 - val_top_5_accuracy: 0.9597 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.7712 - accuracy: 0.7369 - top_5_accuracy: 0.9470\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 22s 103ms/step - loss: 0.7712 - accuracy: 0.7369 - top_5_accuracy: 0.9471 - val_loss: 0.6654 - val_accuracy: 0.7680 - val_top_5_accuracy: 0.9531 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "215/215 [==============================] - 22s 104ms/step - loss: 0.7259 - accuracy: 0.7442 - top_5_accuracy: 0.9543 - val_loss: 0.6288 - val_accuracy: 0.7772 - val_top_5_accuracy: 0.9653 - lr: 5.0000e-04\n",
      "Epoch 30/50\n",
      "215/215 [==============================] - 32s 151ms/step - loss: 0.6982 - accuracy: 0.7503 - top_5_accuracy: 0.9535 - val_loss: 0.6587 - val_accuracy: 0.7619 - val_top_5_accuracy: 0.9628 - lr: 5.0000e-04\n",
      "Epoch 31/50\n",
      "215/215 [==============================] - 35s 162ms/step - loss: 0.7104 - accuracy: 0.7489 - top_5_accuracy: 0.9534 - val_loss: 0.5880 - val_accuracy: 0.7848 - val_top_5_accuracy: 0.9653 - lr: 5.0000e-04\n",
      "Epoch 32/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.6965 - accuracy: 0.7541 - top_5_accuracy: 0.9572 - val_loss: 0.6111 - val_accuracy: 0.7792 - val_top_5_accuracy: 0.9669 - lr: 5.0000e-04\n",
      "Epoch 33/50\n",
      "215/215 [==============================] - 15s 70ms/step - loss: 0.6801 - accuracy: 0.7550 - top_5_accuracy: 0.9563 - val_loss: 0.6124 - val_accuracy: 0.7608 - val_top_5_accuracy: 0.9638 - lr: 5.0000e-04\n",
      "Epoch 34/50\n",
      "215/215 [==============================] - 13s 62ms/step - loss: 0.6677 - accuracy: 0.7610 - top_5_accuracy: 0.9582 - val_loss: 0.6064 - val_accuracy: 0.7710 - val_top_5_accuracy: 0.9628 - lr: 5.0000e-04\n",
      "Epoch 35/50\n",
      "215/215 [==============================] - 13s 59ms/step - loss: 0.6855 - accuracy: 0.7560 - top_5_accuracy: 0.9562 - val_loss: 0.5950 - val_accuracy: 0.7741 - val_top_5_accuracy: 0.9628 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "215/215 [==============================] - 13s 59ms/step - loss: 0.6921 - accuracy: 0.7503 - top_5_accuracy: 0.9563 - val_loss: 0.5877 - val_accuracy: 0.7792 - val_top_5_accuracy: 0.9704 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "215/215 [==============================] - 13s 61ms/step - loss: 0.6726 - accuracy: 0.7544 - top_5_accuracy: 0.9601 - val_loss: 0.5778 - val_accuracy: 0.7823 - val_top_5_accuracy: 0.9669 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "215/215 [==============================] - 14s 63ms/step - loss: 0.6829 - accuracy: 0.7548 - top_5_accuracy: 0.9564 - val_loss: 0.5893 - val_accuracy: 0.7726 - val_top_5_accuracy: 0.9653 - lr: 5.0000e-04\n",
      "Epoch 39/50\n",
      "215/215 [==============================] - 13s 61ms/step - loss: 0.6829 - accuracy: 0.7562 - top_5_accuracy: 0.9524 - val_loss: 0.5617 - val_accuracy: 0.7904 - val_top_5_accuracy: 0.9669 - lr: 5.0000e-04\n",
      "Epoch 40/50\n",
      "215/215 [==============================] - 13s 62ms/step - loss: 0.6688 - accuracy: 0.7576 - top_5_accuracy: 0.9594 - val_loss: 0.5899 - val_accuracy: 0.7797 - val_top_5_accuracy: 0.9694 - lr: 5.0000e-04\n",
      "Epoch 41/50\n",
      "215/215 [==============================] - 13s 62ms/step - loss: 0.6618 - accuracy: 0.7591 - top_5_accuracy: 0.9576 - val_loss: 0.5555 - val_accuracy: 0.7853 - val_top_5_accuracy: 0.9663 - lr: 5.0000e-04\n",
      "Epoch 42/50\n",
      "215/215 [==============================] - 14s 63ms/step - loss: 0.6610 - accuracy: 0.7613 - top_5_accuracy: 0.9573 - val_loss: 0.5775 - val_accuracy: 0.7889 - val_top_5_accuracy: 0.9623 - lr: 5.0000e-04\n",
      "Epoch 43/50\n",
      "215/215 [==============================] - 13s 62ms/step - loss: 0.6443 - accuracy: 0.7704 - top_5_accuracy: 0.9572 - val_loss: 0.5678 - val_accuracy: 0.7843 - val_top_5_accuracy: 0.9725 - lr: 5.0000e-04\n",
      "Epoch 44/50\n",
      "215/215 [==============================] - 13s 60ms/step - loss: 0.6544 - accuracy: 0.7623 - top_5_accuracy: 0.9610 - val_loss: 0.5781 - val_accuracy: 0.7884 - val_top_5_accuracy: 0.9663 - lr: 5.0000e-04\n",
      "Epoch 45/50\n",
      "215/215 [==============================] - 13s 61ms/step - loss: 0.6671 - accuracy: 0.7595 - top_5_accuracy: 0.9566 - val_loss: 0.5742 - val_accuracy: 0.7909 - val_top_5_accuracy: 0.9679 - lr: 5.0000e-04\n",
      "Epoch 46/50\n",
      "213/215 [============================>.] - ETA: 0s - loss: 0.6516 - accuracy: 0.7647 - top_5_accuracy: 0.9594\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 13s 61ms/step - loss: 0.6499 - accuracy: 0.7652 - top_5_accuracy: 0.9594 - val_loss: 0.6019 - val_accuracy: 0.7726 - val_top_5_accuracy: 0.9633 - lr: 5.0000e-04\n",
      "Epoch 47/50\n",
      "215/215 [==============================] - 13s 62ms/step - loss: 0.6236 - accuracy: 0.7719 - top_5_accuracy: 0.9602 - val_loss: 0.5338 - val_accuracy: 0.7996 - val_top_5_accuracy: 0.9669 - lr: 2.5000e-04\n",
      "Epoch 48/50\n",
      "215/215 [==============================] - 13s 61ms/step - loss: 0.6040 - accuracy: 0.7747 - top_5_accuracy: 0.9636 - val_loss: 0.5366 - val_accuracy: 0.7981 - val_top_5_accuracy: 0.9714 - lr: 2.5000e-04\n",
      "Epoch 49/50\n",
      "215/215 [==============================] - 13s 62ms/step - loss: 0.6163 - accuracy: 0.7731 - top_5_accuracy: 0.9618 - val_loss: 0.5522 - val_accuracy: 0.7858 - val_top_5_accuracy: 0.9714 - lr: 2.5000e-04\n",
      "Epoch 50/50\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 0.6340 - accuracy: 0.7681 - top_5_accuracy: 0.9615 - val_loss: 0.5235 - val_accuracy: 0.8057 - val_top_5_accuracy: 0.9714 - lr: 2.5000e-04\n",
      "\n",
      "‚úì Training completed!\n",
      "‚è±Ô∏è  End time: 2025-11-27 23:13:15\n",
      "‚úì Model weights saved to: MobileNetV2_output\\models\\model_weights.h5\n",
      "\n",
      "======================================================================\n",
      "EVALUATING MobileNetV2\n",
      "======================================================================\n",
      "\n",
      "üìä Evaluating on test set...\n",
      "31/31 [==============================] - 2s 46ms/step - loss: 0.5134 - accuracy: 0.7984 - top_5_accuracy: 0.9654\n",
      "\n",
      "‚úì Test Results:\n",
      "  Loss: 0.5134\n",
      "  Accuracy: 0.7984\n",
      "  Top-5 Accuracy: 0.9654\n",
      "\n",
      "üìä Generating detailed metrics...\n",
      "‚úì Confusion matrix saved to: MobileNetV2_output\\plots\\confusion_matrix.png\n",
      "‚úì Classification report saved to: MobileNetV2_output\\logs\\classification_report.txt\n",
      "\n",
      "üìà Generating training plots...\n",
      "‚úì Training plots saved to: MobileNetV2_output\\plots\\training_history.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.5133593082427979,\n",
       " 'test_accuracy': 0.7983706593513489,\n",
       " 'num_classes': 63,\n",
       " 'evaluation_time': '2025-11-27T23:13:20.223068',\n",
       " 'test_top5_accuracy': 0.9653767943382263}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mobilenetv2(dataset_path=\"final_merged_dataset\", epochs=50, batch_size=32, alpha=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cea3a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA PREPARATION - DenseNet121\n",
      "======================================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images\n",
      "‚úì Number of classes: 63\n",
      "‚úì Classes: ['Missing_hole', 'Missing_hole_rotation', 'Mouse_bite', 'Mouse_bite_rotation', 'Open_circuit', 'Open_circuit_rotation', 'PCB_USED', 'Short', 'Short_rotation', 'Spur']...\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1961\n",
      "‚úì Test samples: 982\n",
      "\n",
      "======================================================================\n",
      "BUILDING DENSENET121 MODEL\n",
      "======================================================================\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "29084464/29084464 [==============================] - 17s 1us/step\n",
      "\n",
      "======================================================================\n",
      "TRAINING DenseNet121\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting training for 50 epochs...\n",
      "‚è±Ô∏è  Start time: 2025-11-27 23:14:37\n",
      "\n",
      "Epoch 1/50\n",
      "215/215 [==============================] - 42s 181ms/step - loss: 1.5311 - accuracy: 0.6184 - top_5_accuracy: 0.8079 - val_loss: 1.2220 - val_accuracy: 0.6390 - val_top_5_accuracy: 0.8899 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "215/215 [==============================] - 34s 158ms/step - loss: 1.1944 - accuracy: 0.6556 - top_5_accuracy: 0.8753 - val_loss: 1.0421 - val_accuracy: 0.6757 - val_top_5_accuracy: 0.8950 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "215/215 [==============================] - 29s 133ms/step - loss: 1.0922 - accuracy: 0.6666 - top_5_accuracy: 0.8928 - val_loss: 0.9556 - val_accuracy: 0.7068 - val_top_5_accuracy: 0.9153 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 1.0335 - accuracy: 0.6759 - top_5_accuracy: 0.9049 - val_loss: 0.9947 - val_accuracy: 0.6685 - val_top_5_accuracy: 0.9199 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 1.0036 - accuracy: 0.6830 - top_5_accuracy: 0.9088 - val_loss: 0.8662 - val_accuracy: 0.7165 - val_top_5_accuracy: 0.9220 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.9766 - accuracy: 0.6908 - top_5_accuracy: 0.9151 - val_loss: 0.9025 - val_accuracy: 0.6966 - val_top_5_accuracy: 0.9347 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.9356 - accuracy: 0.6986 - top_5_accuracy: 0.9171 - val_loss: 0.8848 - val_accuracy: 0.6996 - val_top_5_accuracy: 0.9373 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.9260 - accuracy: 0.7027 - top_5_accuracy: 0.9240 - val_loss: 0.9105 - val_accuracy: 0.6884 - val_top_5_accuracy: 0.9332 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.9218 - accuracy: 0.7060 - top_5_accuracy: 0.9251 - val_loss: 0.8346 - val_accuracy: 0.7114 - val_top_5_accuracy: 0.9347 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.9031 - accuracy: 0.7043 - top_5_accuracy: 0.9301 - val_loss: 0.8029 - val_accuracy: 0.7328 - val_top_5_accuracy: 0.9465 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.8856 - accuracy: 0.7082 - top_5_accuracy: 0.9301 - val_loss: 0.7297 - val_accuracy: 0.7460 - val_top_5_accuracy: 0.9459 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.8510 - accuracy: 0.7157 - top_5_accuracy: 0.9365 - val_loss: 0.8428 - val_accuracy: 0.7124 - val_top_5_accuracy: 0.9465 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.8551 - accuracy: 0.7180 - top_5_accuracy: 0.9305 - val_loss: 0.7820 - val_accuracy: 0.7328 - val_top_5_accuracy: 0.9408 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.8169 - accuracy: 0.7260 - top_5_accuracy: 0.9417 - val_loss: 0.6907 - val_accuracy: 0.7568 - val_top_5_accuracy: 0.9495 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "215/215 [==============================] - 27s 127ms/step - loss: 0.8341 - accuracy: 0.7234 - top_5_accuracy: 0.9369 - val_loss: 0.7223 - val_accuracy: 0.7496 - val_top_5_accuracy: 0.9567 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.8178 - accuracy: 0.7231 - top_5_accuracy: 0.9407 - val_loss: 0.6923 - val_accuracy: 0.7588 - val_top_5_accuracy: 0.9602 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.7959 - accuracy: 0.7305 - top_5_accuracy: 0.9419 - val_loss: 0.7275 - val_accuracy: 0.7481 - val_top_5_accuracy: 0.9429 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.8214 - accuracy: 0.7272 - top_5_accuracy: 0.9407 - val_loss: 0.7443 - val_accuracy: 0.7389 - val_top_5_accuracy: 0.9510 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.7913 - accuracy: 0.7301 - top_5_accuracy: 0.9433\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.7905 - accuracy: 0.7304 - top_5_accuracy: 0.9433 - val_loss: 0.7542 - val_accuracy: 0.7404 - val_top_5_accuracy: 0.9480 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.7429 - accuracy: 0.7412 - top_5_accuracy: 0.9474 - val_loss: 0.6135 - val_accuracy: 0.7772 - val_top_5_accuracy: 0.9607 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.7299 - accuracy: 0.7439 - top_5_accuracy: 0.9518 - val_loss: 0.6069 - val_accuracy: 0.7705 - val_top_5_accuracy: 0.9592 - lr: 5.0000e-04\n",
      "Epoch 22/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.7144 - accuracy: 0.7446 - top_5_accuracy: 0.9513 - val_loss: 0.6893 - val_accuracy: 0.7506 - val_top_5_accuracy: 0.9556 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.7181 - accuracy: 0.7449 - top_5_accuracy: 0.9497 - val_loss: 0.6110 - val_accuracy: 0.7726 - val_top_5_accuracy: 0.9674 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.7004 - accuracy: 0.7486 - top_5_accuracy: 0.9554 - val_loss: 0.6401 - val_accuracy: 0.7624 - val_top_5_accuracy: 0.9597 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.6875 - accuracy: 0.7503 - top_5_accuracy: 0.9564 - val_loss: 0.5851 - val_accuracy: 0.7858 - val_top_5_accuracy: 0.9612 - lr: 5.0000e-04\n",
      "Epoch 26/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.6902 - accuracy: 0.7560 - top_5_accuracy: 0.9540 - val_loss: 0.6312 - val_accuracy: 0.7583 - val_top_5_accuracy: 0.9618 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.6653 - accuracy: 0.7557 - top_5_accuracy: 0.9559 - val_loss: 0.6053 - val_accuracy: 0.7619 - val_top_5_accuracy: 0.9628 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.6767 - accuracy: 0.7589 - top_5_accuracy: 0.9543 - val_loss: 0.5777 - val_accuracy: 0.7894 - val_top_5_accuracy: 0.9582 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.6701 - accuracy: 0.7559 - top_5_accuracy: 0.9572 - val_loss: 0.6128 - val_accuracy: 0.7736 - val_top_5_accuracy: 0.9648 - lr: 5.0000e-04\n",
      "Epoch 30/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.6575 - accuracy: 0.7627 - top_5_accuracy: 0.9591 - val_loss: 0.6175 - val_accuracy: 0.7710 - val_top_5_accuracy: 0.9612 - lr: 5.0000e-04\n",
      "Epoch 31/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.6578 - accuracy: 0.7620 - top_5_accuracy: 0.9591 - val_loss: 0.5546 - val_accuracy: 0.8001 - val_top_5_accuracy: 0.9633 - lr: 5.0000e-04\n",
      "Epoch 32/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.6565 - accuracy: 0.7645 - top_5_accuracy: 0.9598 - val_loss: 0.5850 - val_accuracy: 0.7817 - val_top_5_accuracy: 0.9638 - lr: 5.0000e-04\n",
      "Epoch 33/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.6680 - accuracy: 0.7527 - top_5_accuracy: 0.9576 - val_loss: 0.5725 - val_accuracy: 0.7843 - val_top_5_accuracy: 0.9669 - lr: 5.0000e-04\n",
      "Epoch 34/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.6403 - accuracy: 0.7662 - top_5_accuracy: 0.9582 - val_loss: 0.5978 - val_accuracy: 0.7741 - val_top_5_accuracy: 0.9577 - lr: 5.0000e-04\n",
      "Epoch 35/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.6497 - accuracy: 0.7664 - top_5_accuracy: 0.9582 - val_loss: 0.5576 - val_accuracy: 0.7955 - val_top_5_accuracy: 0.9689 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.6571 - accuracy: 0.7547 - top_5_accuracy: 0.9556\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.6581 - accuracy: 0.7547 - top_5_accuracy: 0.9556 - val_loss: 0.5560 - val_accuracy: 0.7960 - val_top_5_accuracy: 0.9714 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.6226 - accuracy: 0.7693 - top_5_accuracy: 0.9602 - val_loss: 0.5419 - val_accuracy: 0.7874 - val_top_5_accuracy: 0.9674 - lr: 2.5000e-04\n",
      "Epoch 38/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.6199 - accuracy: 0.7694 - top_5_accuracy: 0.9620 - val_loss: 0.5396 - val_accuracy: 0.7914 - val_top_5_accuracy: 0.9709 - lr: 2.5000e-04\n",
      "Epoch 39/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.6086 - accuracy: 0.7774 - top_5_accuracy: 0.9611 - val_loss: 0.5079 - val_accuracy: 0.8042 - val_top_5_accuracy: 0.9679 - lr: 2.5000e-04\n",
      "Epoch 40/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.6101 - accuracy: 0.7735 - top_5_accuracy: 0.9610 - val_loss: 0.5317 - val_accuracy: 0.7919 - val_top_5_accuracy: 0.9740 - lr: 2.5000e-04\n",
      "Epoch 41/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.5999 - accuracy: 0.7751 - top_5_accuracy: 0.9624 - val_loss: 0.5324 - val_accuracy: 0.7960 - val_top_5_accuracy: 0.9699 - lr: 2.5000e-04\n",
      "Epoch 42/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.6014 - accuracy: 0.7811 - top_5_accuracy: 0.9614 - val_loss: 0.5194 - val_accuracy: 0.8016 - val_top_5_accuracy: 0.9663 - lr: 2.5000e-04\n",
      "Epoch 43/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.5772 - accuracy: 0.7844 - top_5_accuracy: 0.9659 - val_loss: 0.5277 - val_accuracy: 0.8006 - val_top_5_accuracy: 0.9714 - lr: 2.5000e-04\n",
      "Epoch 44/50\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.5855 - accuracy: 0.7821 - top_5_accuracy: 0.9663\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.5847 - accuracy: 0.7824 - top_5_accuracy: 0.9664 - val_loss: 0.5289 - val_accuracy: 0.8006 - val_top_5_accuracy: 0.9709 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.5756 - accuracy: 0.7835 - top_5_accuracy: 0.9661 - val_loss: 0.4963 - val_accuracy: 0.8129 - val_top_5_accuracy: 0.9740 - lr: 1.2500e-04\n",
      "Epoch 46/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.5650 - accuracy: 0.7879 - top_5_accuracy: 0.9652 - val_loss: 0.5256 - val_accuracy: 0.7863 - val_top_5_accuracy: 0.9750 - lr: 1.2500e-04\n",
      "Epoch 47/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.5632 - accuracy: 0.7885 - top_5_accuracy: 0.9674 - val_loss: 0.4782 - val_accuracy: 0.8169 - val_top_5_accuracy: 0.9735 - lr: 1.2500e-04\n",
      "Epoch 48/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.5589 - accuracy: 0.7870 - top_5_accuracy: 0.9668 - val_loss: 0.4705 - val_accuracy: 0.8215 - val_top_5_accuracy: 0.9755 - lr: 1.2500e-04\n",
      "Epoch 49/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.5461 - accuracy: 0.7948 - top_5_accuracy: 0.9659 - val_loss: 0.4976 - val_accuracy: 0.8052 - val_top_5_accuracy: 0.9735 - lr: 1.2500e-04\n",
      "Epoch 50/50\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.5655 - accuracy: 0.7809 - top_5_accuracy: 0.9662 - val_loss: 0.4859 - val_accuracy: 0.8123 - val_top_5_accuracy: 0.9745 - lr: 1.2500e-04\n",
      "\n",
      "‚úì Training completed!\n",
      "‚è±Ô∏è  End time: 2025-11-27 23:37:34\n",
      "‚úì Model weights saved to: DenseNet121_output\\models\\model_weights.h5\n",
      "\n",
      "======================================================================\n",
      "EVALUATING DenseNet121\n",
      "======================================================================\n",
      "\n",
      "üìä Evaluating on test set...\n",
      "31/31 [==============================] - 3s 96ms/step - loss: 0.4709 - accuracy: 0.8157 - top_5_accuracy: 0.9725\n",
      "\n",
      "‚úì Test Results:\n",
      "  Loss: 0.4709\n",
      "  Accuracy: 0.8157\n",
      "  Top-5 Accuracy: 0.9725\n",
      "\n",
      "üìä Generating detailed metrics...\n",
      "‚úì Confusion matrix saved to: DenseNet121_output\\plots\\confusion_matrix.png\n",
      "‚úì Classification report saved to: DenseNet121_output\\logs\\classification_report.txt\n",
      "\n",
      "üìà Generating training plots...\n",
      "‚úì Training plots saved to: DenseNet121_output\\plots\\training_history.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.47094088792800903,\n",
       " 'test_accuracy': 0.8156822919845581,\n",
       " 'num_classes': 63,\n",
       " 'evaluation_time': '2025-11-27T23:37:43.492160',\n",
       " 'test_top5_accuracy': 0.9725050926208496}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_densenet(dataset_path=\"final_merged_dataset\", variant='121', epochs=50, batch_size=32)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a49e9a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA PREPARATION - EfficientNetB0\n",
      "======================================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images\n",
      "‚úì Number of classes: 63\n",
      "‚úì Classes: ['Missing_hole', 'Missing_hole_rotation', 'Mouse_bite', 'Mouse_bite_rotation', 'Open_circuit', 'Open_circuit_rotation', 'PCB_USED', 'Short', 'Short_rotation', 'Spur']...\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1961\n",
      "‚úì Test samples: 982\n",
      "\n",
      "======================================================================\n",
      "BUILDING EFFICIENTNET MODEL\n",
      "======================================================================\n",
      "\n",
      "‚úì Model built successfully!\n",
      "‚úì Total parameters: 4,858,082\n",
      "‚úì Base model frozen: True\n",
      "‚úì Mixup/CutMix augmentation applied!\n",
      "Epoch 1/50\n",
      "215/215 [==============================] - 95s 430ms/step - loss: 1.9224 - accuracy: 0.5860 - top_5_accuracy: 0.7503 - val_loss: 1.4944 - val_accuracy: 0.6548 - val_top_5_accuracy: 0.8348 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "215/215 [==============================] - 50s 232ms/step - loss: 1.5709 - accuracy: 0.6259 - top_5_accuracy: 0.8242 - val_loss: 1.0848 - val_accuracy: 0.6849 - val_top_5_accuracy: 0.8924 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "215/215 [==============================] - 54s 250ms/step - loss: 1.4379 - accuracy: 0.6409 - top_5_accuracy: 0.8501 - val_loss: 1.0072 - val_accuracy: 0.7175 - val_top_5_accuracy: 0.9077 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "215/215 [==============================] - 53s 244ms/step - loss: 1.3704 - accuracy: 0.6489 - top_5_accuracy: 0.8622 - val_loss: 0.9705 - val_accuracy: 0.6864 - val_top_5_accuracy: 0.9138 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "215/215 [==============================] - 55s 252ms/step - loss: 1.3318 - accuracy: 0.6495 - top_5_accuracy: 0.8689 - val_loss: 0.8880 - val_accuracy: 0.7267 - val_top_5_accuracy: 0.9220 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "215/215 [==============================] - 56s 259ms/step - loss: 1.3612 - accuracy: 0.6558 - top_5_accuracy: 0.8706 - val_loss: 0.9392 - val_accuracy: 0.7017 - val_top_5_accuracy: 0.9235 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "215/215 [==============================] - 55s 254ms/step - loss: 1.2629 - accuracy: 0.6637 - top_5_accuracy: 0.8820 - val_loss: 0.9081 - val_accuracy: 0.7053 - val_top_5_accuracy: 0.9317 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "215/215 [==============================] - 40s 188ms/step - loss: 1.2198 - accuracy: 0.6705 - top_5_accuracy: 0.8905 - val_loss: 0.8604 - val_accuracy: 0.7109 - val_top_5_accuracy: 0.9261 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "215/215 [==============================] - 26s 119ms/step - loss: 1.2385 - accuracy: 0.6683 - top_5_accuracy: 0.8905 - val_loss: 0.8555 - val_accuracy: 0.7134 - val_top_5_accuracy: 0.9322 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "215/215 [==============================] - 28s 130ms/step - loss: 1.2517 - accuracy: 0.6672 - top_5_accuracy: 0.8916 - val_loss: 0.7910 - val_accuracy: 0.7328 - val_top_5_accuracy: 0.9368 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "215/215 [==============================] - 31s 144ms/step - loss: 1.2366 - accuracy: 0.6723 - top_5_accuracy: 0.8881 - val_loss: 0.7878 - val_accuracy: 0.7389 - val_top_5_accuracy: 0.9312 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "215/215 [==============================] - 28s 130ms/step - loss: 1.1660 - accuracy: 0.6836 - top_5_accuracy: 0.8941 - val_loss: 0.7994 - val_accuracy: 0.7389 - val_top_5_accuracy: 0.9439 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "215/215 [==============================] - 30s 138ms/step - loss: 1.2084 - accuracy: 0.6763 - top_5_accuracy: 0.8960 - val_loss: 0.7970 - val_accuracy: 0.7338 - val_top_5_accuracy: 0.9388 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "215/215 [==============================] - 28s 130ms/step - loss: 1.1708 - accuracy: 0.6902 - top_5_accuracy: 0.8937 - val_loss: 0.7187 - val_accuracy: 0.7547 - val_top_5_accuracy: 0.9536 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "215/215 [==============================] - 30s 137ms/step - loss: 1.2076 - accuracy: 0.6776 - top_5_accuracy: 0.8960 - val_loss: 0.7497 - val_accuracy: 0.7496 - val_top_5_accuracy: 0.9500 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "215/215 [==============================] - 26s 120ms/step - loss: 1.1570 - accuracy: 0.6884 - top_5_accuracy: 0.9015 - val_loss: 0.7113 - val_accuracy: 0.7639 - val_top_5_accuracy: 0.9510 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "215/215 [==============================] - 27s 127ms/step - loss: 1.1218 - accuracy: 0.6921 - top_5_accuracy: 0.9025 - val_loss: 0.7591 - val_accuracy: 0.7394 - val_top_5_accuracy: 0.9495 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "215/215 [==============================] - 23s 107ms/step - loss: 1.1604 - accuracy: 0.6851 - top_5_accuracy: 0.8991 - val_loss: 0.7494 - val_accuracy: 0.7496 - val_top_5_accuracy: 0.9459 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "215/215 [==============================] - 24s 113ms/step - loss: 1.1340 - accuracy: 0.6874 - top_5_accuracy: 0.9056 - val_loss: 0.7598 - val_accuracy: 0.7287 - val_top_5_accuracy: 0.9500 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "215/215 [==============================] - 22s 102ms/step - loss: 1.1526 - accuracy: 0.6875 - top_5_accuracy: 0.9001 - val_loss: 0.7032 - val_accuracy: 0.7603 - val_top_5_accuracy: 0.9607 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "215/215 [==============================] - 20s 94ms/step - loss: 1.1481 - accuracy: 0.6881 - top_5_accuracy: 0.9058 - val_loss: 0.6868 - val_accuracy: 0.7557 - val_top_5_accuracy: 0.9510 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "215/215 [==============================] - 24s 111ms/step - loss: 1.1205 - accuracy: 0.6908 - top_5_accuracy: 0.9042 - val_loss: 0.7614 - val_accuracy: 0.7379 - val_top_5_accuracy: 0.9393 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "215/215 [==============================] - 20s 94ms/step - loss: 1.1430 - accuracy: 0.6893 - top_5_accuracy: 0.9027 - val_loss: 0.6957 - val_accuracy: 0.7547 - val_top_5_accuracy: 0.9567 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "215/215 [==============================] - 23s 106ms/step - loss: 1.1657 - accuracy: 0.6891 - top_5_accuracy: 0.9030 - val_loss: 0.7122 - val_accuracy: 0.7593 - val_top_5_accuracy: 0.9475 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "215/215 [==============================] - 25s 115ms/step - loss: 1.0939 - accuracy: 0.6998 - top_5_accuracy: 0.9062 - val_loss: 0.6525 - val_accuracy: 0.7675 - val_top_5_accuracy: 0.9551 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "215/215 [==============================] - 19s 89ms/step - loss: 1.1298 - accuracy: 0.6886 - top_5_accuracy: 0.9025 - val_loss: 0.7070 - val_accuracy: 0.7481 - val_top_5_accuracy: 0.9572 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "215/215 [==============================] - 25s 118ms/step - loss: 1.0815 - accuracy: 0.7021 - top_5_accuracy: 0.9079 - val_loss: 0.6997 - val_accuracy: 0.7583 - val_top_5_accuracy: 0.9516 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "215/215 [==============================] - 24s 109ms/step - loss: 1.1525 - accuracy: 0.6932 - top_5_accuracy: 0.9012 - val_loss: 0.6764 - val_accuracy: 0.7695 - val_top_5_accuracy: 0.9495 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "215/215 [==============================] - 19s 89ms/step - loss: 1.0687 - accuracy: 0.7040 - top_5_accuracy: 0.9149 - val_loss: 0.7002 - val_accuracy: 0.7583 - val_top_5_accuracy: 0.9459 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "214/215 [============================>.] - ETA: 0s - loss: 1.0683 - accuracy: 0.7084 - top_5_accuracy: 0.9096\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 26s 119ms/step - loss: 1.0686 - accuracy: 0.7081 - top_5_accuracy: 0.9097 - val_loss: 0.7001 - val_accuracy: 0.7557 - val_top_5_accuracy: 0.9510 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "215/215 [==============================] - 21s 96ms/step - loss: 1.1249 - accuracy: 0.6964 - top_5_accuracy: 0.9125 - val_loss: 0.6017 - val_accuracy: 0.7863 - val_top_5_accuracy: 0.9587 - lr: 5.0000e-04\n",
      "Epoch 32/50\n",
      "215/215 [==============================] - 22s 102ms/step - loss: 1.0316 - accuracy: 0.7114 - top_5_accuracy: 0.9138 - val_loss: 0.6303 - val_accuracy: 0.7751 - val_top_5_accuracy: 0.9612 - lr: 5.0000e-04\n",
      "Epoch 33/50\n",
      "215/215 [==============================] - 25s 115ms/step - loss: 1.0382 - accuracy: 0.7106 - top_5_accuracy: 0.9125 - val_loss: 0.6305 - val_accuracy: 0.7792 - val_top_5_accuracy: 0.9628 - lr: 5.0000e-04\n",
      "Epoch 34/50\n",
      "215/215 [==============================] - 19s 90ms/step - loss: 1.0162 - accuracy: 0.7162 - top_5_accuracy: 0.9167 - val_loss: 0.6129 - val_accuracy: 0.7777 - val_top_5_accuracy: 0.9567 - lr: 5.0000e-04\n",
      "Epoch 35/50\n",
      "215/215 [==============================] - 24s 109ms/step - loss: 0.9923 - accuracy: 0.7176 - top_5_accuracy: 0.9178 - val_loss: 0.6042 - val_accuracy: 0.7807 - val_top_5_accuracy: 0.9638 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 1.0152 - accuracy: 0.7130 - top_5_accuracy: 0.9205\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 25s 114ms/step - loss: 1.0152 - accuracy: 0.7130 - top_5_accuracy: 0.9205 - val_loss: 0.6277 - val_accuracy: 0.7787 - val_top_5_accuracy: 0.9674 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "215/215 [==============================] - 20s 94ms/step - loss: 0.9805 - accuracy: 0.7210 - top_5_accuracy: 0.9212 - val_loss: 0.5791 - val_accuracy: 0.7863 - val_top_5_accuracy: 0.9663 - lr: 2.5000e-04\n",
      "Epoch 38/50\n",
      "215/215 [==============================] - 25s 116ms/step - loss: 1.0259 - accuracy: 0.7107 - top_5_accuracy: 0.9130 - val_loss: 0.5846 - val_accuracy: 0.7838 - val_top_5_accuracy: 0.9638 - lr: 2.5000e-04\n",
      "Epoch 39/50\n",
      "215/215 [==============================] - 20s 94ms/step - loss: 1.0218 - accuracy: 0.7148 - top_5_accuracy: 0.9162 - val_loss: 0.5600 - val_accuracy: 0.8001 - val_top_5_accuracy: 0.9643 - lr: 2.5000e-04\n",
      "Epoch 40/50\n",
      "215/215 [==============================] - 22s 102ms/step - loss: 1.0212 - accuracy: 0.7110 - top_5_accuracy: 0.9145 - val_loss: 0.5779 - val_accuracy: 0.7848 - val_top_5_accuracy: 0.9628 - lr: 2.5000e-04\n",
      "Epoch 41/50\n",
      "215/215 [==============================] - 26s 121ms/step - loss: 1.0122 - accuracy: 0.7181 - top_5_accuracy: 0.9221 - val_loss: 0.5659 - val_accuracy: 0.7914 - val_top_5_accuracy: 0.9653 - lr: 2.5000e-04\n",
      "Epoch 42/50\n",
      "215/215 [==============================] - 22s 101ms/step - loss: 1.0308 - accuracy: 0.7200 - top_5_accuracy: 0.9149 - val_loss: 0.5946 - val_accuracy: 0.7868 - val_top_5_accuracy: 0.9582 - lr: 2.5000e-04\n",
      "Epoch 43/50\n",
      "215/215 [==============================] - 29s 133ms/step - loss: 0.9726 - accuracy: 0.7294 - top_5_accuracy: 0.9248 - val_loss: 0.5731 - val_accuracy: 0.7935 - val_top_5_accuracy: 0.9618 - lr: 2.5000e-04\n",
      "Epoch 44/50\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.9758 - accuracy: 0.7289 - top_5_accuracy: 0.9254\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "215/215 [==============================] - 27s 125ms/step - loss: 0.9758 - accuracy: 0.7289 - top_5_accuracy: 0.9254 - val_loss: 0.5838 - val_accuracy: 0.7894 - val_top_5_accuracy: 0.9623 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "215/215 [==============================] - 28s 131ms/step - loss: 0.9535 - accuracy: 0.7286 - top_5_accuracy: 0.9232 - val_loss: 0.5453 - val_accuracy: 0.8032 - val_top_5_accuracy: 0.9643 - lr: 1.2500e-04\n",
      "Epoch 46/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.9595 - accuracy: 0.7310 - top_5_accuracy: 0.9259 - val_loss: 0.5744 - val_accuracy: 0.7848 - val_top_5_accuracy: 0.9607 - lr: 1.2500e-04\n",
      "Epoch 47/50\n",
      "215/215 [==============================] - 24s 112ms/step - loss: 0.9711 - accuracy: 0.7283 - top_5_accuracy: 0.9190 - val_loss: 0.5343 - val_accuracy: 0.8001 - val_top_5_accuracy: 0.9720 - lr: 1.2500e-04\n",
      "Epoch 48/50\n",
      "215/215 [==============================] - 21s 98ms/step - loss: 0.9494 - accuracy: 0.7307 - top_5_accuracy: 0.9264 - val_loss: 0.5440 - val_accuracy: 0.8062 - val_top_5_accuracy: 0.9694 - lr: 1.2500e-04\n",
      "Epoch 49/50\n",
      "215/215 [==============================] - 18s 85ms/step - loss: 0.9452 - accuracy: 0.7264 - top_5_accuracy: 0.9294 - val_loss: 0.5583 - val_accuracy: 0.7991 - val_top_5_accuracy: 0.9618 - lr: 1.2500e-04\n",
      "Epoch 50/50\n",
      "215/215 [==============================] - 16s 76ms/step - loss: 1.0289 - accuracy: 0.7167 - top_5_accuracy: 0.9160 - val_loss: 0.5346 - val_accuracy: 0.8047 - val_top_5_accuracy: 0.9669 - lr: 1.2500e-04\n",
      "\n",
      "======================================================================\n",
      "EVALUATING EfficientNetB0\n",
      "======================================================================\n",
      "\n",
      "üìä Evaluating on test set...\n",
      "31/31 [==============================] - 2s 48ms/step - loss: 0.5208 - accuracy: 0.8187 - top_5_accuracy: 0.9664\n",
      "\n",
      "‚úì Test Results:\n",
      "  Loss: 0.5208\n",
      "  Accuracy: 0.8187\n",
      "  Top-5 Accuracy: 0.9664\n",
      "\n",
      "üìä Generating detailed metrics...\n",
      "‚úì Confusion matrix saved to: EfficientNetB0_output\\plots\\confusion_matrix.png\n",
      "‚úì Classification report saved to: EfficientNetB0_output\\logs\\classification_report.txt\n",
      "‚ö†Ô∏è  No training history available\n",
      "Test Accuracy: 81.87%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Your MixupCutmixAugmenter class (keep as is)\n",
    "class MixupCutmixAugmenter:\n",
    "    def __init__(self, mixup_alpha=0.2, cutmix_alpha=1.0, prob=0.5):\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.cutmix_alpha = cutmix_alpha\n",
    "        self.prob = prob\n",
    "\n",
    "    def sample_beta(self, alpha):\n",
    "        g1 = tf.random.gamma([], alpha, dtype=tf.float32)\n",
    "        g2 = tf.random.gamma([], alpha, dtype=tf.float32)\n",
    "        return g1 / (g1 + g2)\n",
    "\n",
    "    def mixup(self, images, labels):\n",
    "        lam = self.sample_beta(self.mixup_alpha)\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        index = tf.random.shuffle(tf.range(batch_size))\n",
    "        \n",
    "        images2 = tf.gather(images, index)\n",
    "        labels2 = tf.gather(labels, index)\n",
    "        \n",
    "        mixed_images = lam * images + (1 - lam) * images2\n",
    "        mixed_labels = lam * labels + (1 - lam) * labels2\n",
    "        \n",
    "        return mixed_images, mixed_labels\n",
    "\n",
    "    def cutmix(self, images, labels):\n",
    "        batch = tf.shape(images)[0]\n",
    "        H = tf.shape(images)[1]\n",
    "        W = tf.shape(images)[2]\n",
    "        \n",
    "        index = tf.random.shuffle(tf.range(batch))\n",
    "        images2 = tf.gather(images, index)\n",
    "        labels2 = tf.gather(labels, index)\n",
    "        \n",
    "        lam = self.sample_beta(self.cutmix_alpha)\n",
    "        cut_ratio = tf.sqrt(1 - lam)\n",
    "        \n",
    "        cut_w = tf.cast(tf.cast(W, tf.float32) * cut_ratio, tf.int32)\n",
    "        cut_h = tf.cast(tf.cast(H, tf.float32) * cut_ratio, tf.int32)\n",
    "        \n",
    "        cx = tf.random.uniform([batch], 0, W, tf.int32)\n",
    "        cy = tf.random.uniform([batch], 0, H, tf.int32)\n",
    "        \n",
    "        x1 = tf.clip_by_value(cx - cut_w // 2, 0, W)\n",
    "        y1 = tf.clip_by_value(cy - cut_h // 2, 0, H)\n",
    "        x2 = tf.clip_by_value(cx + cut_w // 2, 0, W)\n",
    "        y2 = tf.clip_by_value(cy + cut_h // 2, 0, H)\n",
    "        \n",
    "        x_range = tf.tile(tf.reshape(tf.range(W), [1, 1, W]), [batch, H, 1])\n",
    "        y_range = tf.tile(tf.reshape(tf.range(H), [1, H, 1]), [batch, 1, W])\n",
    "        \n",
    "        x_mask = tf.logical_and(\n",
    "            x_range >= tf.reshape(x1, [-1, 1, 1]),\n",
    "            x_range < tf.reshape(x2, [-1, 1, 1])\n",
    "        )\n",
    "        \n",
    "        y_mask = tf.logical_and(\n",
    "            y_range >= tf.reshape(y1, [-1, 1, 1]),\n",
    "            y_range < tf.reshape(y2, [-1, 1, 1])\n",
    "        )\n",
    "        \n",
    "        mask = tf.cast(tf.logical_and(x_mask, y_mask), tf.float32)\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "        \n",
    "        new_images = images * (1 - mask) + images2 * mask\n",
    "        \n",
    "        box_area = tf.cast((x2 - x1) * (y2 - y1), tf.float32)\n",
    "        lam_adj = 1 - (box_area / tf.cast(W * H, tf.float32))\n",
    "        \n",
    "        new_labels = lam_adj[:, None] * labels + (1 - lam_adj[:, None]) * labels2\n",
    "        \n",
    "        return new_images, new_labels\n",
    "\n",
    "    def __call__(self, images, labels):\n",
    "        if tf.random.uniform([]) > self.prob:\n",
    "            return images, labels\n",
    "        if tf.random.uniform([]) < 0.5:\n",
    "            return self.mixup(images, labels)\n",
    "        else:\n",
    "            return self.cutmix(images, labels)\n",
    "\n",
    "\n",
    "# CORRECTED TRAINING CODE\n",
    "# Remove the debug line: eager_tensor = tf.constant([2.0896919, 2.1128857, 2.1081853])\n",
    "\n",
    "# Step 1: Create model and prepare data\n",
    "efficientnet = EfficientNetDefectDetector(\"final_merged_dataset\")\n",
    "efficientnet.prepare_data(batch_size=32)\n",
    "efficientnet.build_model(learning_rate=0.001)\n",
    "\n",
    "# Step 2: Apply Mixup/CutMix to training data\n",
    "augmenter = MixupCutmixAugmenter(mixup_alpha=0.2, cutmix_alpha=1.0, prob=0.5)\n",
    "\n",
    "# Apply augmentation - THIS IS THE KEY FIX\n",
    "# Use .unbatch() then .batch() to ensure proper tensor handling\n",
    "efficientnet.train_dataset = (\n",
    "    efficientnet.train_dataset\n",
    "    .unbatch()  # Unbatch first\n",
    "    .batch(32)  # Rebatch\n",
    "    .map(lambda x, y: augmenter(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(\"‚úì Mixup/CutMix augmentation applied!\")\n",
    "\n",
    "# Step 3: Train with SIMPLIFIED callbacks to avoid serialization issues\n",
    "class SimpleCSVLogger(keras.callbacks.Callback):\n",
    "    \"\"\"Custom CSV logger that handles tensors properly\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        super().__init__()\n",
    "        self.filename = filename\n",
    "        self.file = None\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.file = open(self.filename, 'w')\n",
    "        self.file.write('epoch,loss,accuracy,val_loss,val_accuracy\\n')\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs:\n",
    "            # Convert any tensors to Python floats\n",
    "            loss = float(logs.get('loss', 0))\n",
    "            acc = float(logs.get('accuracy', 0))\n",
    "            val_loss = float(logs.get('val_loss', 0))\n",
    "            val_acc = float(logs.get('val_accuracy', 0))\n",
    "            \n",
    "            self.file.write(f'{epoch},{loss},{acc},{val_loss},{val_acc}\\n')\n",
    "            self.file.flush()\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.file:\n",
    "            self.file.close()\n",
    "\n",
    "# Use simplified callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    SimpleCSVLogger('training_log.csv')  # Use custom logger\n",
    "]\n",
    "\n",
    "# Train\n",
    "history = efficientnet.model.fit(\n",
    "    efficientnet.train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=efficientnet.val_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 4: Evaluate\n",
    "results = efficientnet.evaluate()\n",
    "efficientnet.plot_training_history()\n",
    "\n",
    "print(f\"Test Accuracy: {results['test_accuracy']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "870a1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_with_advanced_augmentation(\n",
    "    dataset_path,\n",
    "    class_names,\n",
    "    num_classes,\n",
    "    img_height=224,\n",
    "    img_width=224,\n",
    "    validation_split=0.2,\n",
    "    test_split=0.1,\n",
    "    batch_size=32,\n",
    "    use_mixup_cutmix=True,\n",
    "    use_autoaugment=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare data with advanced augmentation\n",
    "    \n",
    "    DROP-IN REPLACEMENT for your existing prepare_data() function!\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to dataset\n",
    "        class_names: List of class names\n",
    "        num_classes: Number of classes\n",
    "        img_height: Image height\n",
    "        img_width: Image width\n",
    "        validation_split: Validation split ratio\n",
    "        test_split: Test split ratio\n",
    "        batch_size: Batch size\n",
    "        use_mixup_cutmix: Enable Mixup/CutMix (recommended!)\n",
    "        use_autoaugment: Enable AutoAugment (recommended!)\n",
    "    \n",
    "    Returns:\n",
    "        train_dataset, val_dataset, test_dataset\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    # Get all image paths and labels\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    print(\"\\nüìÅ Scanning dataset...\")\n",
    "    for class_dir in dataset_path.rglob(\"*\"):\n",
    "        if class_dir.is_dir():\n",
    "            images = (list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\")) + \n",
    "                     list(class_dir.glob(\"*.jpeg\")) + list(class_dir.glob(\"*.bmp\")))\n",
    "            \n",
    "            if images:\n",
    "                class_name = class_dir.name\n",
    "                if class_name in class_names:\n",
    "                    class_idx = class_names.index(class_name)\n",
    "                    for img_path in images:\n",
    "                        image_paths.append(str(img_path))\n",
    "                        labels.append(class_idx)\n",
    "    \n",
    "    print(f\"‚úì Found {len(image_paths)} images\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.shuffle(len(image_paths), seed=42)\n",
    "    \n",
    "    # Calculate splits\n",
    "    train_size = int(len(image_paths) * (1 - validation_split - test_split))\n",
    "    val_size = int(len(image_paths) * validation_split)\n",
    "    \n",
    "    # Split datasets\n",
    "    train_ds = dataset.take(train_size)\n",
    "    val_ds = dataset.skip(train_size).take(val_size)\n",
    "    test_ds = dataset.skip(train_size + val_size)\n",
    "    \n",
    "    # Load and preprocess function\n",
    "    def load_and_preprocess(image_path, label, augment=False):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, [img_height, img_width])\n",
    "        \n",
    "        # Basic augmentation (only for training)\n",
    "        if augment and not use_autoaugment:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_flip_up_down(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.rot90(img, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "        \n",
    "        img = tf.clip_by_value(img, 0.0, 255.0)\n",
    "        img = img / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        # AutoAugment (only for training)\n",
    "        if augment and use_autoaugment:\n",
    "            img = autoaugment_policy(img)\n",
    "        \n",
    "        label_onehot = tf.one_hot(label, num_classes)\n",
    "        return img, label_onehot\n",
    "    \n",
    "    # Process datasets\n",
    "    train_dataset = train_ds.map(\n",
    "        lambda x, y: load_and_preprocess(x, y, augment=True),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).batch(batch_size)\n",
    "    \n",
    "    val_dataset = val_ds.map(\n",
    "        lambda x, y: load_and_preprocess(x, y, augment=False),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    test_dataset = test_ds.map(\n",
    "        lambda x, y: load_and_preprocess(x, y, augment=False),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Apply Mixup/CutMix to training data\n",
    "    if use_mixup_cutmix:\n",
    "        print(\"‚úì Applying Mixup/CutMix augmentation\")\n",
    "        augmenter = MixupCutmixAugmenter(mixup_alpha=0.2, cutmix_alpha=1.0, prob=0.5)\n",
    "        train_dataset = train_dataset.map(\n",
    "            lambda x, y: augmenter(x, y),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "    \n",
    "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    print(f\"‚úì Train samples: {train_size}\")\n",
    "    print(f\"‚úì Validation samples: {val_size}\")\n",
    "    print(f\"‚úì Test samples: {len(image_paths) - train_size - val_size}\")\n",
    "    print(f\"‚úì Advanced augmentation enabled!\")\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6373a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA PREPARATION - EfficientNetB0\n",
      "======================================================================\n",
      "\n",
      "üìÅ Scanning dataset...\n",
      "‚úì Found 9808 images\n",
      "‚úì Number of classes: 63\n",
      "‚úì Classes: ['Missing_hole', 'Missing_hole_rotation', 'Mouse_bite', 'Mouse_bite_rotation', 'Open_circuit', 'Open_circuit_rotation', 'PCB_USED', 'Short', 'Short_rotation', 'Spur']...\n",
      "\n",
      "‚úì Train samples: 6865\n",
      "‚úì Validation samples: 1961\n",
      "‚úì Test samples: 982\n",
      "\n",
      "======================================================================\n",
      "BUILDING EFFICIENTNET MODEL\n",
      "======================================================================\n",
      "\n",
      "‚úì Model built successfully!\n",
      "‚úì Total parameters: 4,858,082\n",
      "‚úì Base model frozen: True\n",
      "‚úì Mixup/CutMix augmentation applied!\n",
      "\n",
      "======================================================================\n",
      "TRAINING EfficientNetB0\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting training for 50 epochs...\n",
      "‚è±Ô∏è  Start time: 2025-12-07 14:03:51\n",
      "\n",
      "Epoch 1/50\n",
      "215/215 [==============================] - 40s 173ms/step - loss: 1.8065 - accuracy: 0.5959 - top_5_accuracy: 0.7633 - val_loss: 1.3921 - val_accuracy: 0.6400 - val_top_5_accuracy: 0.8587 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "210/215 [============================>.] - ETA: 0s - loss: 1.5920 - accuracy: 0.6210 - top_5_accuracy: 0.8210"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20024\\873372642.py\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Step 3: Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mefficientnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Step 4: Evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20024\\3635609630.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, early_stopping_patience)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         self.history = self.model.fit(\n\u001b[0m\u001b[0;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Simple augmentation without AutoAugment (works perfectly!)\n",
    "\n",
    "# Step 1: Create model and prepare data normally\n",
    "efficientnet = EfficientNetDefectDetector(\"final_merged_dataset\")\n",
    "efficientnet.prepare_data(batch_size=32)\n",
    "efficientnet.build_model(learning_rate=0.001)\n",
    "\n",
    "# Step 2: Apply Mixup/CutMix to training data\n",
    "from tensorflow import keras\n",
    "\n",
    "augmenter = MixupCutmixAugmenter(mixup_alpha=0.2, cutmix_alpha=1.0, prob=0.5)\n",
    "\n",
    "# Apply augmentation to training dataset\n",
    "efficientnet.train_dataset = efficientnet.train_dataset.map(\n",
    "    lambda x, y: augmenter(x, y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"‚úì Mixup/CutMix augmentation applied!\")\n",
    "\n",
    "# Step 3: Train\n",
    "efficientnet.train(epochs=50)\n",
    "\n",
    "# Step 4: Evaluate\n",
    "results = efficientnet.evaluate()\n",
    "efficientnet.plot_training_history()\n",
    "\n",
    "print(f\"Test Accuracy: {results['test_accuracy']*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
