{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30345a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DEFECT DETECTION DATASET MERGER\n",
      "============================================================\n",
      "\n",
      "Starting dataset extraction and merging process...\n",
      "‚úì Created directory structure\n",
      "\n",
      "============================================================\n",
      "STEP 1: EXTRACTING DATASETS\n",
      "============================================================\n",
      "\n",
      "üì¶ Extracting pcb-dataset...\n",
      "‚úì Extracted to: extracted_datasets\\pcb-dataset\n",
      "\n",
      "üì¶ Extracting neu-cls...\n",
      "‚úì Extracted to: extracted_datasets\\neu-cls\n",
      "\n",
      "üì¶ Extracting mvtec-ad...\n",
      "‚úì Extracted to: extracted_datasets\\mvtec-ad\n",
      "\n",
      "‚úì All datasets extracted successfully!\n",
      "\n",
      "============================================================\n",
      "STEP 2: GENERATING INDIVIDUAL DATASET METADATA\n",
      "============================================================\n",
      "\n",
      "üìä Analyzing pcb-dataset...\n",
      "  Total Images: 1396\n",
      "  Number of Classes: 13\n",
      "  Top 5 Classes: {'Open_circuit': 116, 'Short': 116, 'Spurious_copper': 116, 'Open_circuit_rotation': 116, 'Short_rotation': 116}\n",
      "\n",
      "üìä Analyzing neu-cls...\n",
      "  Total Images: 1800\n",
      "  Number of Classes: 1\n",
      "  Top 5 Classes: {'images': 1800}\n",
      "\n",
      "üìä Analyzing mvtec-ad...\n",
      "  Total Images: 6612\n",
      "  Number of Classes: 49\n",
      "  Top 5 Classes: {'good': 4096, 'color': 186, 'scratch': 182, 'crack': 168, 'combined': 110}\n",
      "\n",
      "‚úì Individual metadata saved to: metadata\\individual_datasets_metadata.json\n",
      "\n",
      "============================================================\n",
      "STEP 3: MERGING DATASETS\n",
      "============================================================\n",
      "\n",
      "üìÅ Merging pcb-dataset...\n",
      "  ‚úì Copied 1396 images\n",
      "\n",
      "üìÅ Merging neu-cls...\n",
      "  ‚úì Copied 1800 images\n",
      "\n",
      "üìÅ Merging mvtec-ad...\n",
      "  ‚úì Copied 6612 images\n",
      "\n",
      "‚úì Total images merged: 9808\n",
      "\n",
      "============================================================\n",
      "STEP 4: GENERATING MERGED DATASET METADATA\n",
      "============================================================\n",
      "\n",
      "üìä Merged Dataset Statistics:\n",
      "  Total Images: 9808\n",
      "  Number of Datasets: 3\n",
      "  Total Classes: 63\n",
      "\n",
      "‚úì Merged metadata saved to: metadata\\merged_dataset_metadata.json\n",
      "\n",
      "============================================================\n",
      "STEP 5: GENERATING SUMMARY REPORT\n",
      "============================================================\n",
      "‚úì Summary report saved to: metadata\\merge_summary_report.txt\n",
      "\n",
      "============================================================\n",
      "FINAL SUMMARY\n",
      "============================================================\n",
      "\n",
      "‚úì Individual datasets analyzed: 3\n",
      "‚úì Total images merged: 9808\n",
      "‚úì Merged dataset location: final_merged_dataset\n",
      "‚úì Metadata location: metadata\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚úì Process completed successfully!\n",
      "\n",
      "Your merged dataset is ready at: final_merged_dataset\n",
      "Check the metadata folder for detailed reports: metadata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "\n",
    "class DefectDatasetMerger:\n",
    "    def __init__(self, base_dir=\".\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.extract_dir = self.base_dir / \"extracted_datasets\"\n",
    "        self.merged_dir = self.base_dir / \"final_merged_dataset\"\n",
    "        self.metadata_dir = self.base_dir / \"metadata\"\n",
    "        \n",
    "        # Dataset configurations\n",
    "        self.datasets = {\n",
    "            \"pcb-dataset\": {\"zip\": \"pcb-dataset.zip\", \"extracted\": None},\n",
    "            \"neu-cls\": {\"zip\": \"neu-cls.zip\", \"extracted\": None},\n",
    "            \"mvtec-ad\": {\"zip\": \"mvtec-ad.zip\", \"extracted\": None}\n",
    "        }\n",
    "        \n",
    "        # Supported image extensions\n",
    "        self.image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}\n",
    "        \n",
    "    def create_directories(self):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        self.extract_dir.mkdir(exist_ok=True)\n",
    "        self.merged_dir.mkdir(exist_ok=True)\n",
    "        self.metadata_dir.mkdir(exist_ok=True)\n",
    "        print(\"‚úì Created directory structure\")\n",
    "    \n",
    "    def extract_datasets(self):\n",
    "        \"\"\"Extract all dataset zip files\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 1: EXTRACTING DATASETS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for dataset_name, info in self.datasets.items():\n",
    "            zip_path = self.base_dir / info[\"zip\"]\n",
    "            extract_path = self.extract_dir / dataset_name\n",
    "            \n",
    "            if not zip_path.exists():\n",
    "                print(f\"‚ö† Warning: {info['zip']} not found. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nüì¶ Extracting {dataset_name}...\")\n",
    "            \n",
    "            # Remove existing extraction if present\n",
    "            if extract_path.exists():\n",
    "                shutil.rmtree(extract_path)\n",
    "            \n",
    "            extract_path.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Extract zip file\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "            \n",
    "            self.datasets[dataset_name][\"extracted\"] = extract_path\n",
    "            print(f\"‚úì Extracted to: {extract_path}\")\n",
    "        \n",
    "        print(\"\\n‚úì All datasets extracted successfully!\")\n",
    "    \n",
    "    def count_images(self, directory):\n",
    "        \"\"\"Recursively count images in a directory\"\"\"\n",
    "        image_count = 0\n",
    "        class_counts = defaultdict(int)\n",
    "        \n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if Path(file).suffix.lower() in self.image_extensions:\n",
    "                    image_count += 1\n",
    "                    # Try to determine class from parent directory\n",
    "                    parent = Path(root).name\n",
    "                    class_counts[parent] += 1\n",
    "        \n",
    "        return image_count, dict(class_counts)\n",
    "    \n",
    "    def generate_individual_metadata(self):\n",
    "        \"\"\"Generate metadata for individual datasets before merging\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 2: GENERATING INDIVIDUAL DATASET METADATA\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        individual_metadata = {}\n",
    "        \n",
    "        for dataset_name, info in self.datasets.items():\n",
    "            if info[\"extracted\"] is None:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nüìä Analyzing {dataset_name}...\")\n",
    "            \n",
    "            total_images, class_counts = self.count_images(info[\"extracted\"])\n",
    "            \n",
    "            metadata = {\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"extraction_path\": str(info[\"extracted\"]),\n",
    "                \"total_images\": total_images,\n",
    "                \"num_classes\": len(class_counts),\n",
    "                \"classes\": class_counts,\n",
    "                \"analysis_timestamp\": datetime.datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            individual_metadata[dataset_name] = metadata\n",
    "            \n",
    "            print(f\"  Total Images: {total_images}\")\n",
    "            print(f\"  Number of Classes: {len(class_counts)}\")\n",
    "            print(f\"  Top 5 Classes: {dict(list(sorted(class_counts.items(), key=lambda x: x[1], reverse=True))[:5])}\")\n",
    "        \n",
    "        # Save individual metadata\n",
    "        metadata_file = self.metadata_dir / \"individual_datasets_metadata.json\"\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(individual_metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n‚úì Individual metadata saved to: {metadata_file}\")\n",
    "        \n",
    "        return individual_metadata\n",
    "    \n",
    "    def merge_datasets(self):\n",
    "        \"\"\"Merge all datasets into a unified structure\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 3: MERGING DATASETS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Create merged directory structure\n",
    "        if self.merged_dir.exists():\n",
    "            shutil.rmtree(self.merged_dir)\n",
    "        self.merged_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        merge_stats = {\n",
    "            \"total_images_copied\": 0,\n",
    "            \"datasets_merged\": [],\n",
    "            \"file_mapping\": []\n",
    "        }\n",
    "        \n",
    "        for dataset_name, info in self.datasets.items():\n",
    "            if info[\"extracted\"] is None:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nüìÅ Merging {dataset_name}...\")\n",
    "            dataset_dir = self.merged_dir / dataset_name\n",
    "            dataset_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            images_copied = 0\n",
    "            \n",
    "            # Walk through extracted directory and copy all images\n",
    "            for root, dirs, files in os.walk(info[\"extracted\"]):\n",
    "                for file in files:\n",
    "                    if Path(file).suffix.lower() in self.image_extensions:\n",
    "                        src_path = Path(root) / file\n",
    "                        \n",
    "                        # Create relative path structure\n",
    "                        rel_path = src_path.relative_to(info[\"extracted\"])\n",
    "                        dst_path = dataset_dir / rel_path\n",
    "                        \n",
    "                        # Create destination directory if needed\n",
    "                        dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        \n",
    "                        # Copy file\n",
    "                        shutil.copy2(src_path, dst_path)\n",
    "                        images_copied += 1\n",
    "                        \n",
    "                        # Record mapping\n",
    "                        merge_stats[\"file_mapping\"].append({\n",
    "                            \"source\": str(src_path),\n",
    "                            \"destination\": str(dst_path),\n",
    "                            \"dataset\": dataset_name\n",
    "                        })\n",
    "            \n",
    "            merge_stats[\"total_images_copied\"] += images_copied\n",
    "            merge_stats[\"datasets_merged\"].append({\n",
    "                \"name\": dataset_name,\n",
    "                \"images_copied\": images_copied\n",
    "            })\n",
    "            \n",
    "            print(f\"  ‚úì Copied {images_copied} images\")\n",
    "        \n",
    "        print(f\"\\n‚úì Total images merged: {merge_stats['total_images_copied']}\")\n",
    "        \n",
    "        return merge_stats\n",
    "    \n",
    "    def generate_merged_metadata(self, merge_stats, individual_metadata):\n",
    "        \"\"\"Generate metadata for merged dataset\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 4: GENERATING MERGED DATASET METADATA\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        total_images, class_counts = self.count_images(self.merged_dir)\n",
    "        \n",
    "        merged_metadata = {\n",
    "            \"merged_dataset_path\": str(self.merged_dir),\n",
    "            \"merge_timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"total_images\": total_images,\n",
    "            \"num_datasets\": len(merge_stats[\"datasets_merged\"]),\n",
    "            \"datasets_included\": merge_stats[\"datasets_merged\"],\n",
    "            \"total_classes\": len(class_counts),\n",
    "            \"class_distribution\": class_counts,\n",
    "            \"individual_datasets_summary\": {}\n",
    "        }\n",
    "        \n",
    "        # Add individual dataset summaries\n",
    "        for dataset_name, metadata in individual_metadata.items():\n",
    "            merged_metadata[\"individual_datasets_summary\"][dataset_name] = {\n",
    "                \"original_images\": metadata[\"total_images\"],\n",
    "                \"original_classes\": metadata[\"num_classes\"]\n",
    "            }\n",
    "        \n",
    "        # Save merged metadata\n",
    "        metadata_file = self.metadata_dir / \"merged_dataset_metadata.json\"\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(merged_metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüìä Merged Dataset Statistics:\")\n",
    "        print(f\"  Total Images: {total_images}\")\n",
    "        print(f\"  Number of Datasets: {len(merge_stats['datasets_merged'])}\")\n",
    "        print(f\"  Total Classes: {len(class_counts)}\")\n",
    "        \n",
    "        print(f\"\\n‚úì Merged metadata saved to: {metadata_file}\")\n",
    "        \n",
    "        return merged_metadata\n",
    "    \n",
    "    def generate_summary_report(self, individual_metadata, merged_metadata):\n",
    "        \"\"\"Generate a human-readable summary report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 5: GENERATING SUMMARY REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        report_file = self.metadata_dir / \"merge_summary_report.txt\"\n",
    "        \n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "            f.write(\"DEFECT DETECTION DATASET MERGE REPORT\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            f.write(f\"Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            # Individual datasets section\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            f.write(\"INDIVIDUAL DATASETS (BEFORE MERGE)\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\\n\")\n",
    "            \n",
    "            total_before = 0\n",
    "            for dataset_name, metadata in individual_metadata.items():\n",
    "                f.write(f\"Dataset: {dataset_name}\\n\")\n",
    "                f.write(f\"  Location: {metadata['extraction_path']}\\n\")\n",
    "                f.write(f\"  Total Images: {metadata['total_images']}\\n\")\n",
    "                f.write(f\"  Number of Classes: {metadata['num_classes']}\\n\")\n",
    "                f.write(f\"  Classes: {', '.join(metadata['classes'].keys())}\\n\\n\")\n",
    "                total_before += metadata['total_images']\n",
    "            \n",
    "            f.write(f\"TOTAL IMAGES (ALL DATASETS): {total_before}\\n\\n\")\n",
    "            \n",
    "            # Merged dataset section\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            f.write(\"MERGED DATASET (AFTER MERGE)\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\\n\")\n",
    "            f.write(f\"Location: {merged_metadata['merged_dataset_path']}\\n\")\n",
    "            f.write(f\"Total Images: {merged_metadata['total_images']}\\n\")\n",
    "            f.write(f\"Number of Datasets Merged: {merged_metadata['num_datasets']}\\n\")\n",
    "            f.write(f\"Total Classes: {merged_metadata['total_classes']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"Images per Dataset:\\n\")\n",
    "            for dataset in merged_metadata['datasets_included']:\n",
    "                f.write(f\"  - {dataset['name']}: {dataset['images_copied']} images\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "            f.write(\"VERIFICATION\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\\n\")\n",
    "            f.write(f\"Images before merge: {total_before}\\n\")\n",
    "            f.write(f\"Images after merge:  {merged_metadata['total_images']}\\n\")\n",
    "            \n",
    "            if total_before == merged_metadata['total_images']:\n",
    "                f.write(\"‚úì SUCCESS: All images preserved during merge!\\n\")\n",
    "            else:\n",
    "                f.write(f\"‚ö† WARNING: Image count mismatch! Difference: {abs(total_before - merged_metadata['total_images'])}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "        \n",
    "        print(f\"‚úì Summary report saved to: {report_file}\")\n",
    "        \n",
    "        # Print summary to console\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FINAL SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\n‚úì Individual datasets analyzed: {len(individual_metadata)}\")\n",
    "        print(f\"‚úì Total images merged: {merged_metadata['total_images']}\")\n",
    "        print(f\"‚úì Merged dataset location: {self.merged_dir}\")\n",
    "        print(f\"‚úì Metadata location: {self.metadata_dir}\")\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Execute the complete extraction and merging process\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DEFECT DETECTION DATASET MERGER\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nStarting dataset extraction and merging process...\")\n",
    "        \n",
    "        # Step 1: Create directories\n",
    "        self.create_directories()\n",
    "        \n",
    "        # Step 2: Extract datasets\n",
    "        self.extract_datasets()\n",
    "        \n",
    "        # Step 3: Generate individual metadata\n",
    "        individual_metadata = self.generate_individual_metadata()\n",
    "        \n",
    "        # Step 4: Merge datasets\n",
    "        merge_stats = self.merge_datasets()\n",
    "        \n",
    "        # Step 5: Generate merged metadata\n",
    "        merged_metadata = self.generate_merged_metadata(merge_stats, individual_metadata)\n",
    "        \n",
    "        # Step 6: Generate summary report\n",
    "        self.generate_summary_report(individual_metadata, merged_metadata)\n",
    "        \n",
    "        print(\"\\n‚úì Process completed successfully!\")\n",
    "        print(f\"\\nYour merged dataset is ready at: {self.merged_dir}\")\n",
    "        print(f\"Check the metadata folder for detailed reports: {self.metadata_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize and run the merger\n",
    "    merger = DefectDatasetMerger(base_dir=\".\")\n",
    "    merger.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
